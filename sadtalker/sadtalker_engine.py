#!/usr/bin/env python3
"""
SadTalker Engine - ãƒ¢ãƒ‡ãƒ«äº‹å‰ãƒ­ãƒ¼ãƒ‰å‹é«˜é€Ÿå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³
FastAPIç”¨ã®ã‚³ã‚¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚¯ãƒ©ã‚¹
"""

import os
import sys
import torch
import warnings
import tempfile
import shutil
from pathlib import Path
from typing import Optional, Dict, Any
from dataclasses import dataclass
import time
from time import strftime
import subprocess

# SadTalker imports
sys.path.append('/home/SadTalker')
sys.path.append('/home/SadTalker/src')
from src.utils.preprocess import CropAndExtract
from src.test_audio2coeff import Audio2Coeff  
from src.facerender.animate_onnx import AnimateFromCoeff
from src.generate_batch import get_data
from src.generate_facerender_batch import get_facerender_data
from src.utils.init_path import init_path

@dataclass
class SadTalkerConfig:
    """SadTalkerè¨­å®šç®¡ç†"""
    quality: str = "fast"  # fast, high
    fp16: bool = False
    expression_scale: float = 1.0
    still_mode: bool = True
    yaw: Optional[float] = None
    pitch: Optional[float] = None
    roll: Optional[float] = None
    batch_size: int = 1  # RTX 3050æœ€é©åŒ–
    
    def __post_init__(self):
        """è¨­å®šæ¤œè¨¼"""
        if self.quality not in ["fast", "high"]:
            raise ValueError("quality must be 'fast' or 'high'")
        if not 0.0 <= self.expression_scale <= 2.0:
            raise ValueError("expression_scale must be between 0.0 and 2.0")

class SadTalkerEngine:
    """
    SadTalkerå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ - ãƒ¢ãƒ‡ãƒ«äº‹å‰ãƒ­ãƒ¼ãƒ‰å‹
    
    ç‰¹å¾´:
    - èµ·å‹•æ™‚1å›ã®ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    - GPU VRAMåŠ¹ç‡çš„åˆ©ç”¨
    - FP16æœ€é©åŒ–å¯¾å¿œ  
    - GFPGANçµ±åˆ
    - å®Œå…¨ãƒã‚¤ã‚ºãƒ•ãƒªãƒ¼éŸ³å£°
    """
    
    def __init__(self, 
                 checkpoints_dir: str = "/app/checkpoints",
                 gfpgan_dir: str = "/app/gfpgan",
                 device: str = "cuda"):
        """
        SadTalkerã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        
        Args:
            checkpoints_dir: ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            gfpgan_dir: GFPGANãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª  
            device: å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹ (cuda/cpu)
        """
        self.checkpoints_dir = Path(checkpoints_dir)
        self.gfpgan_dir = Path(gfpgan_dir)
        self.device = device
        self.models_loaded = False
        
        
        # ğŸš€ GPUæœ€é©åŒ–è¨­å®š
        self._setup_gpu_optimization()
        
        # ğŸ“¦ ãƒ¢ãƒ‡ãƒ«äº‹å‰ãƒ­ãƒ¼ãƒ‰
        self._preload_models()
        
        print("âœ… SadTalkerEngineåˆæœŸåŒ–å®Œäº† - ãƒ¢ãƒ‡ãƒ«å¸¸é§æº–å‚™å®Œäº†")
    
    def _setup_gpu_optimization(self):
        """GPUæœ€é©åŒ–è¨­å®š"""
        if not torch.cuda.is_available():
            print("âš ï¸ CUDAæœªæ¤œå‡º - CPUãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œ")
            self.device = "cpu"
            return
        
        print(f"ğŸ” GPUæ¤œå‡º: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ” VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        
        # CUDAæœ€é©åŒ–
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        torch.cuda.empty_cache()
        torch.cuda.set_per_process_memory_fraction(0.85)  # RTX 3050ç”¨
        
        print("ğŸš€ GPUæœ€é©åŒ–è¨­å®šå®Œäº†")
    
    def _perform_warmup(self):
        """GPUã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ— - ãƒ€ãƒŸãƒ¼æ¨è«–ã§åˆå›æœ€é©åŒ–ã‚’å®Œäº†"""
        try:
            print("ğŸ”¥ GPUã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é–‹å§‹ - åˆå›æœ€é©åŒ–å®Ÿè¡Œä¸­...")
            
            import numpy as np
            import cv2
            import librosa
            import soundfile as sf
            
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)
                
                # ğŸ–¼ï¸ ãƒ€ãƒŸãƒ¼ç”»åƒä½œæˆï¼ˆ64x64ã€é¡”ã‚‰ã—ãå½¢çŠ¶ï¼‰
                dummy_img = np.zeros((64, 64, 3), dtype=np.uint8)
                # ç°¡å˜ãªé¡”å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ¥•å††ï¼‰
                cv2.ellipse(dummy_img, (32, 32), (20, 25), 0, 0, 360, (128, 128, 128), -1)
                # ç›®ã®ä½ç½®
                cv2.circle(dummy_img, (24, 26), 2, (255, 255, 255), -1)
                cv2.circle(dummy_img, (40, 26), 2, (255, 255, 255), -1)
                # å£ã®ä½ç½®
                cv2.ellipse(dummy_img, (32, 40), (4, 2), 0, 0, 180, (255, 255, 255), 1)
                
                dummy_image_path = temp_path / "dummy.jpg"
                cv2.imwrite(str(dummy_image_path), dummy_img)
                
                # ğŸµ ãƒ€ãƒŸãƒ¼éŸ³å£°ä½œæˆï¼ˆ1ç§’ã€ãƒ¢ãƒï¼‰
                sample_rate = 16000
                duration = 1.0
                t = np.linspace(0, duration, int(sample_rate * duration), False)
                # è¤‡æ•°å‘¨æ³¢æ•°ã®åˆæˆéŸ³ï¼ˆéŸ³å£°ã‚‰ã—ãï¼‰
                dummy_audio = 0.1 * (np.sin(2 * np.pi * 300 * t) + 
                                   0.5 * np.sin(2 * np.pi * 600 * t) + 
                                   0.3 * np.sin(2 * np.pi * 900 * t))
                
                dummy_audio_path = temp_path / "dummy.wav"
                sf.write(str(dummy_audio_path), dummy_audio, sample_rate)
                
                print("ğŸ“¦ ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº† - ãƒ•ãƒ«æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œä¸­...")
                
                # ğŸš€ ãƒ•ãƒ«æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œï¼ˆçµæœã¯ç ´æ£„ï¼‰
                save_dir = temp_path / "warmup"
                save_dir.mkdir()
                first_frame_dir = save_dir / "first_frame_dir" 
                first_frame_dir.mkdir()
                
                # 1. å‰å‡¦ç†ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
                first_coeff_path, crop_pic_path, crop_info = self.preprocess_model.generate(
                    str(dummy_image_path), str(first_frame_dir), 'crop', source_image_flag=True
                )
                
                # 2. éŸ³å£°è§£æã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—  
                from src.generate_batch import get_data
                batch = get_data(first_coeff_path, str(dummy_audio_path), self.device, None, still=True)
                coeff_path = self.audio_to_coeff.generate(batch, str(save_dir), 0, None)
                
                # 3. å‹•ç”»ç”Ÿæˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆGFPGANå«ã‚€ï¼‰
                from src.generate_facerender_batch import get_facerender_data
                data = get_facerender_data(
                    coeff_path, crop_pic_path, first_coeff_path, str(dummy_audio_path), 
                    1, None, None, None, expression_scale=1.0, still_mode=True, preprocess='crop'
                )
                
                # GFPGANä»˜ãã§å®Ÿè¡Œï¼ˆé«˜è² è·ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼‰
                video_path = self.animate_from_coeff.generate_deploy(
                    data, str(save_dir), str(dummy_image_path), crop_info, 
                    enhancer='gfpgan', background_enhancer=None, preprocess='crop'
                )
                
                # 4. CUDAã‚«ãƒ¼ãƒãƒ«å®Œå…¨åˆæœŸåŒ–
                self._complete_cuda_warmup()
                
                # 5. PyTorchJITæœ€é©åŒ–ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
                self._pytorch_jit_warmup()
                
                # 6. CUDAãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                torch.cuda.empty_cache()
                torch.cuda.synchronize()  # GPUåŒæœŸå¾…ã¡
                
                print("âœ… GPUã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº† - åˆå›ã‹ã‚‰é«˜é€Ÿå®Ÿè¡Œæº–å‚™å®Œäº†ï¼")
                
        except BaseException as e:
            print(f"âš ï¸ GPUã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼ï¼ˆç¶šè¡Œå¯èƒ½ï¼‰: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦ã‚‚ç¶šè¡Œï¼ˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã¯ä»»æ„ï¼‰
    
    def _complete_cuda_warmup(self):
        """å®Œå…¨CUDAã‚«ãƒ¼ãƒãƒ«åˆæœŸåŒ– - å…¨ã‚«ãƒ¼ãƒãƒ«ã‚¿ã‚¤ãƒ—ã‚’äº‹å‰å®Ÿè¡Œ"""
        print("ğŸ”¥ CUDAã‚«ãƒ¼ãƒãƒ«å®Œå…¨åˆæœŸåŒ–ä¸­...")
        
        # ç•°ãªã‚‹ã‚µã‚¤ã‚ºã®ãƒ†ãƒ³ã‚½ãƒ«æ“ä½œã§ã‚«ãƒ¼ãƒãƒ«åˆæœŸåŒ–
        sizes = [(1, 3, 64, 64), (1, 3, 256, 256), (1, 3, 512, 512)]
        
        for size in sizes:
            # åŸºæœ¬æ¼”ç®—ã‚«ãƒ¼ãƒãƒ«
            x = torch.randn(size, device=self.device)
            y = torch.randn(size, device=self.device)
            
            # ç•³ã¿è¾¼ã¿ã‚«ãƒ¼ãƒãƒ«
            z = torch.conv2d(x, torch.randn(16, 3, 3, 3, device=self.device), padding=1)
            
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³é–¢æ•°ã‚«ãƒ¼ãƒãƒ«
            torch.relu(z, inplace=True)
            torch.sigmoid(z)
            torch.tanh(z)
            
            # ãƒãƒƒãƒæ­£è¦åŒ–ã‚«ãƒ¼ãƒãƒ«
            bn = torch.nn.BatchNorm2d(16).to(self.device)
            z = bn(z)
            
            # ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚«ãƒ¼ãƒãƒ«
            z = torch.nn.functional.interpolate(z, scale_factor=2, mode='bilinear')
            
            # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚«ãƒ¼ãƒãƒ«
            attn = torch.nn.MultiheadAttention(16, 4).to(self.device)
            seq = torch.randn(10, 1, 16, device=self.device)
            attn(seq, seq, seq)
        
        # CUDAã‚¤ãƒ™ãƒ³ãƒˆåŒæœŸ
        torch.cuda.synchronize()
        print("âœ… CUDAã‚«ãƒ¼ãƒãƒ«å®Œå…¨åˆæœŸåŒ–å®Œäº†")
    
    def _pytorch_jit_warmup(self):
        """PyTorchJITå®Œå…¨æœ€é©åŒ–ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—"""
        print("ğŸ”¥ PyTorchJITæœ€é©åŒ–ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­...")
        
        # JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«
        @torch.jit.script
        def dummy_conv_model(x):
            conv1 = torch.nn.functional.conv2d(x, torch.randn(32, 3, 3, 3, device=x.device), padding=1)
            relu1 = torch.relu(conv1)
            conv2 = torch.nn.functional.conv2d(relu1, torch.randn(64, 32, 3, 3, device=x.device), padding=1)
            relu2 = torch.relu(conv2)
            return torch.nn.functional.adaptive_avg_pool2d(relu2, (1, 1))
        
        @torch.jit.script  
        def dummy_attention_model(x):
            # Self-attention simulation
            q = torch.nn.functional.linear(x, torch.randn(512, 512, device=x.device))
            k = torch.nn.functional.linear(x, torch.randn(512, 512, device=x.device))
            v = torch.nn.functional.linear(x, torch.randn(512, 512, device=x.device))
            
            scores = torch.matmul(q, k.transpose(-2, -1)) / (512 ** 0.5)
            attn = torch.softmax(scores, dim=-1)
            return torch.matmul(attn, v)
        
        # ç•°ãªã‚‹ã‚µã‚¤ã‚ºã§JITæœ€é©åŒ–å®Ÿè¡Œ
        test_sizes = [
            (1, 3, 64, 64),    # å°ã‚µã‚¤ã‚º
            (1, 3, 256, 256),  # ä¸­ã‚µã‚¤ã‚º
            (4, 3, 128, 128),  # ãƒãƒƒãƒã‚µã‚¤ã‚ºå¤‰æ›´
        ]
        
        with torch.no_grad():
            for size in test_sizes:
                x = torch.randn(size, device=self.device)
                
                # ç•³ã¿è¾¼ã¿ãƒ¢ãƒ‡ãƒ«JITæœ€é©åŒ–
                for _ in range(3):  # è¤‡æ•°å›å®Ÿè¡Œã§JITæœ€é©åŒ–
                    _ = dummy_conv_model(x)
                
                # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«JITæœ€é©åŒ–
                seq_x = torch.randn(10, size[0], 512, device=self.device)
                for _ in range(3):
                    _ = dummy_attention_model(seq_x)
        
        # CUDAã‚°ãƒ©ãƒ•æœ€é©åŒ–ï¼ˆPyTorch 1.10+ï¼‰
        if hasattr(torch.cuda, 'CUDAGraph'):
            try:
                x = torch.randn(1, 3, 256, 256, device=self.device)
                graph = torch.cuda.CUDAGraph()
                
                with torch.cuda.graph(graph):
                    y = dummy_conv_model(x)
                
                # ã‚°ãƒ©ãƒ•å®Ÿè¡Œã§æœ€é©åŒ–
                graph.replay()
                print("âœ… CUDAGraphæœ€é©åŒ–å®Œäº†")
            except:
                print("âš ï¸ CUDAGraphæœ€é©åŒ–ã‚¹ã‚­ãƒƒãƒ—")
        
        torch.cuda.synchronize()
        print("âœ… PyTorchJITæœ€é©åŒ–ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")
    
    def _preload_models(self):
        """ãƒ¢ãƒ‡ãƒ«äº‹å‰ãƒ­ãƒ¼ãƒ‰ - èµ·å‹•æ™‚1å›ã®ã¿å®Ÿè¡Œ"""
        try:
            print("ğŸ“¦ SadTalkerãƒ¢ãƒ‡ãƒ«äº‹å‰ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
            
            # ãƒ‘ã‚¹åˆæœŸåŒ–
            sadtalker_paths = init_path(
                str(self.checkpoints_dir), 
                '/home/SadTalker/src/config', 
                '256', 
                True, 
                'crop'
            )
            
            # è­¦å‘Šéè¡¨ç¤º
            warnings.filterwarnings('ignore')
            
            # ğŸ­ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆäº‹å‰ãƒ­ãƒ¼ãƒ‰ï¼‰
            self.preprocess_model = CropAndExtract(sadtalker_paths, self.device)
            self.audio_to_coeff = Audio2Coeff(sadtalker_paths, self.device)
            self.animate_from_coeff = AnimateFromCoeff(sadtalker_paths, self.device)
            
            self.models_loaded = True
            print("âœ… ãƒ¢ãƒ‡ãƒ«äº‹å‰ãƒ­ãƒ¼ãƒ‰å®Œäº† - å¸¸é§æº–å‚™å®Œäº†")
            
            # ğŸš€ GPUã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆãƒ€ãƒŸãƒ¼æ¨è«–ã§åˆå›æœ€é©åŒ–ï¼‰
            try:
                self._perform_warmup()
            except Exception as e:
                print(f"âš ï¸ GPUã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ã‚­ãƒƒãƒ—: {e}")
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _apply_fp16_optimization(self, config: SadTalkerConfig):
        """FP16æœ€é©åŒ–é©ç”¨"""
        if config.fp16:
            torch.backends.cudnn.allow_tf32 = True
            torch.cuda.empty_cache()
            print("ğŸš€ FP16æœ€é©åŒ–æœ‰åŠ¹")
        else:
            print("ğŸ”„ æ¨™æº–ç²¾åº¦ãƒ¢ãƒ¼ãƒ‰")
    
    
    def generate_video(self, 
                      image_path: str, 
                      audio_path: str,
                      config: SadTalkerConfig = None) -> Dict[str, Any]:
        """
        å‹•ç”»ç”Ÿæˆãƒ¡ã‚¤ãƒ³å‡¦ç†
        
        Args:
            image_path: å…¥åŠ›ç”»åƒãƒ‘ã‚¹
            audio_path: å…¥åŠ›éŸ³å£°ãƒ‘ã‚¹  
            config: SadTalkerè¨­å®š
            
        Returns:
            å‡¦ç†çµæœè¾æ›¸
        """
        if not self.models_loaded:
            raise RuntimeError("ãƒ¢ãƒ‡ãƒ«ãŒäº‹å‰ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        if config is None:
            config = SadTalkerConfig()
        
        # FP16æœ€é©åŒ–é©ç”¨
        self._apply_fp16_optimization(config)
        
        print(f"ğŸ¬ å‹•ç”»ç”Ÿæˆé–‹å§‹ - {config.quality}å“è³ª")
        
        try:
            # ä¸€æ™‚ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)
                save_dir = temp_path / f"sadtalker_{strftime('%Y%m%d_%H%M%S')}"
                save_dir.mkdir()
                
                first_frame_dir = save_dir / "first_frame_dir"
                first_frame_dir.mkdir()
                
                # ğŸ­ å‰å‡¦ç†ï¼ˆäº‹å‰ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼‰
                first_coeff_path, crop_pic_path, crop_info = self.preprocess_model.generate(
                    image_path, str(first_frame_dir), 'crop', source_image_flag=True
                )
                print("âœ… å‰å‡¦ç†å®Œäº†ï¼ˆcropæœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰ï¼‰")
                
                # ğŸµ éŸ³å£°è§£æï¼ˆäº‹å‰ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼‰
                batch = get_data(first_coeff_path, audio_path, self.device, None, still=config.still_mode)
                coeff_path = self.audio_to_coeff.generate(batch, str(save_dir), 0, None)
                print("âœ… éŸ³å£°è§£æå®Œäº†")
                
                # ğŸ¬ å‹•ç”»ç”Ÿæˆï¼ˆäº‹å‰ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼‰
                yaw_list = [config.yaw] if config.yaw is not None else None
                pitch_list = [config.pitch] if config.pitch is not None else None  
                roll_list = [config.roll] if config.roll is not None else None
                
                data = get_facerender_data(
                    coeff_path, crop_pic_path, first_coeff_path, audio_path, 
                    config.batch_size, yaw_list, pitch_list, roll_list, 
                    expression_scale=config.expression_scale, 
                    still_mode=config.still_mode, 
                    preprocess='crop'
                )
                
                # ã‚¨ãƒ³ãƒãƒ³ã‚µãƒ¼è¨­å®š
                enhancer = 'gfpgan' if config.quality == 'high' else None
                
                video_path = self.animate_from_coeff.generate_deploy(
                    data, str(save_dir), image_path, crop_info, 
                    enhancer=enhancer, background_enhancer=None, preprocess='crop'
                )
                
                if config.quality == 'high':
                    print("ğŸ”¥ GFPGANé¡”ã‚¨ãƒ³ãƒãƒ³ã‚µãƒ¼é©ç”¨å®Œäº†")
                
                print(f"âœ… å‹•ç”»ç”Ÿæˆå®Œäº†: {video_path}")
                
                # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
                output_dir = Path("output")
                output_dir.mkdir(exist_ok=True)
                
                # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ•ã‚¡ã‚¤ãƒ«åã§æ°¸ç¶šçš„ã«ä¿å­˜
                import uuid
                persistent_filename = f"sadtalker_{strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}.mp4"
                persistent_path = output_dir / persistent_filename
                
                # ğŸ”‡ éŸ³å£°ãƒãƒ¼ã‚¸ï¼ˆREADMEã®ã‚¹ãƒˆãƒªãƒ¼ãƒ åˆ†é›¢æŠ€è¡“ï¼‰ - ç›´æ¥æ°¸ç¶šãƒ‘ã‚¹ã«ä¿å­˜
                final_video_path = self._merge_audio_with_stream_separation(
                    video_path, audio_path, temp_path, persistent_path
                )
                
                print(f"âœ… æ°¸ç¶šä¿å­˜: {persistent_path}")
                
                return {
                    "success": True,
                    "video_path": str(persistent_path),
                    "config": config,
                    "message": "å‹•ç”»ç”ŸæˆæˆåŠŸ"
                }
                
        except Exception as e:
            print(f"âŒ å‹•ç”»ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "success": False,
                "error": str(e),
                "config": config,
                "message": "å‹•ç”»ç”Ÿæˆå¤±æ•—"
            }
    
    def _merge_audio_with_stream_separation(self, video_path: str, audio_path: str, temp_dir: Path, output_path: Path) -> str:
        """
        READMEã®ã‚¹ãƒˆãƒªãƒ¼ãƒ åˆ†é›¢æŠ€è¡“ã«ã‚ˆã‚‹å®Œå…¨ãƒã‚¤ã‚ºãƒ•ãƒªãƒ¼éŸ³å£°ãƒãƒ¼ã‚¸
        """
        print("ğŸ”‡ ã‚¹ãƒˆãƒªãƒ¼ãƒ åˆ†é›¢æŠ€è¡“é©ç”¨ä¸­...")
        
        # Step 1: å…ƒéŸ³å£°ã‚’MP3ã«å¤‰æ›ï¼ˆå“è³ªä¿æŒï¼‰
        temp_mp3 = temp_dir / "original_audio.mp3"
        convert_cmd = [
            "ffmpeg", "-i", audio_path,
            "-c:a", "libmp3lame", "-b:a", "192k", "-ar", "44100",
            "-y", str(temp_mp3)
        ]
        
        result = subprocess.run(convert_cmd, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"âš ï¸ éŸ³å£°å¤‰æ›å¤±æ•—: {result.stderr}")
            # å…ƒå‹•ç”»ã‚’å‡ºåŠ›ãƒ‘ã‚¹ã«ã‚³ãƒ”ãƒ¼
            import shutil
            shutil.copy2(video_path, output_path)
            return str(output_path)
        
        # Step 2: ã‚¹ãƒˆãƒªãƒ¼ãƒ åˆ†é›¢ - ç›´æ¥å‡ºåŠ›ãƒ‘ã‚¹ã«æ›¸ãè¾¼ã¿
        merge_cmd = [
            "ffmpeg", 
            "-i", video_path,
            "-i", str(temp_mp3),
            "-c:v", "copy",
            "-c:a", "copy",  # éŸ³å£°ã‚’ä¸€åˆ‡åŠ å·¥ã›ãšã‚³ãƒ”ãƒ¼
            "-map", "0:v:0",  # SadTalkerå‹•ç”»ã®æ˜ åƒã®ã¿
            "-map", "1:a:0",  # å…ƒéŸ³å£°MP3ã®éŸ³å£°ã®ã¿
            "-shortest", "-y", str(output_path)
        ]
        
        result = subprocess.run(merge_cmd, capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… å®Œå…¨ãƒã‚¤ã‚ºãƒ•ãƒªãƒ¼éŸ³å£°ãƒãƒ¼ã‚¸æˆåŠŸ")
            return str(output_path)
        else:
            print(f"âš ï¸ éŸ³å£°ãƒãƒ¼ã‚¸å¤±æ•—: {result.stderr}")
            # å…ƒå‹•ç”»ã‚’å‡ºåŠ›ãƒ‘ã‚¹ã«ã‚³ãƒ”ãƒ¼
            import shutil
            shutil.copy2(video_path, output_path)
            return str(output_path)
    
    def get_status(self) -> Dict[str, Any]:
        """ã‚¨ãƒ³ã‚¸ãƒ³çŠ¶æ…‹å–å¾—"""
        return {
            "models_loaded": self.models_loaded,
            "device": self.device,
            "gpu_available": torch.cuda.is_available(),
            "gpu_memory_allocated": torch.cuda.memory_allocated() if torch.cuda.is_available() else 0,
            "gpu_memory_cached": torch.cuda.memory_reserved() if torch.cuda.is_available() else 0
        }