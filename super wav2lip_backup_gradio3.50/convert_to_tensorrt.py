#!/usr/bin/env python3
"""
ONNX to TensorRT Engine Converter
ONNXãƒ¢ãƒ‡ãƒ«ã‚’TensorRTã‚¨ãƒ³ã‚¸ãƒ³ã«å¤‰æ›ã—ã¦é«˜é€ŸåŒ–
"""

import os
import sys
import time
from pathlib import Path

def convert_with_onnxruntime():
    """ONNXRuntimeã®TensorRTãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã‚¨ãƒ³ã‚¸ãƒ³ç”Ÿæˆ"""
    try:
        import onnxruntime as ort
        import numpy as np
        
        print("ğŸ”§ ONNXRuntime TensorRT Provider ã«ã‚ˆã‚‹å¤‰æ›é–‹å§‹...")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
        onnx_path = "/app/models/onnx/wav2lip_gan.onnx"
        
        if not Path(onnx_path).exists():
            print(f"âŒ ONNXãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {onnx_path}")
            return False
        
        print(f"ğŸ“ å…¥åŠ›: {onnx_path}")
        print(f"ğŸ“ ã‚µã‚¤ã‚º: {Path(onnx_path).stat().st_size / 1024 / 1024:.2f} MB")
        
        # TensorRTãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
        providers = [
            ('TensorrtExecutionProvider', {
                'device_id': 0,
                'trt_max_workspace_size': 4 * 1024 * 1024 * 1024,  # 4GB
                'trt_fp16_enable': True,
                'trt_engine_cache_enable': True,
                'trt_engine_cache_path': '/app/models/onnx/trt_cache',
                'trt_int8_enable': False,
                'trt_max_partition_iterations': 1000,
                'trt_min_subgraph_size': 1
            }),
            'CUDAExecutionProvider',
            'CPUExecutionProvider'
        ]
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs('/app/models/onnx/trt_cache', exist_ok=True)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆï¼ˆã“ã®æ™‚ç‚¹ã§TensorRTã‚¨ãƒ³ã‚¸ãƒ³ãŒç”Ÿæˆã•ã‚Œã‚‹ï¼‰
        print("ğŸ”¨ TensorRTã‚¨ãƒ³ã‚¸ãƒ³ç”Ÿæˆä¸­...")
        start_time = time.time()
        
        session = ort.InferenceSession(onnx_path, providers=providers)
        
        # å…¥åŠ›æƒ…å ±å–å¾—
        input_name = session.get_inputs()[0].name
        input_shape = session.get_inputs()[0].shape
        print(f"ğŸ“‹ å…¥åŠ›: {input_name}, shape: {input_shape}")
        
        # ãƒ€ãƒŸãƒ¼å…¥åŠ›ã§ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆã‚¨ãƒ³ã‚¸ãƒ³æœ€é©åŒ–ï¼‰
        print("ğŸ”¥ ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œä¸­...")
        if input_shape[0] == 'batch_size' or input_shape[0] is None:
            dummy_shape = [1] + list(input_shape[1:])
        else:
            dummy_shape = input_shape
            
        # Noneã‚’é©åˆ‡ãªå€¤ã«ç½®æ›
        for i, dim in enumerate(dummy_shape):
            if dim is None or (isinstance(dim, str)):
                if i == 0:
                    dummy_shape[i] = 1  # ãƒãƒƒãƒã‚µã‚¤ã‚º
                elif i == 1:
                    dummy_shape[i] = 80  # éŸ³å£°ç‰¹å¾´é‡æ¬¡å…ƒï¼ˆæ¨å®šï¼‰
                else:
                    dummy_shape[i] = 16  # ãã®ä»–ã®æ¬¡å…ƒ
        
        dummy_input = np.random.randn(*dummy_shape).astype(np.float32)
        
        # è¤‡æ•°å›å®Ÿè¡Œã—ã¦ã‚¨ãƒ³ã‚¸ãƒ³æœ€é©åŒ–
        for i in range(3):
            _ = session.run(None, {input_name: dummy_input})
            print(f"  ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ— {i+1}/3 å®Œäº†")
        
        elapsed = time.time() - start_time
        print(f"âœ… TensorRTã‚¨ãƒ³ã‚¸ãƒ³ç”Ÿæˆå®Œäº†: {elapsed:.2f}ç§’")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        cache_files = list(Path('/app/models/onnx/trt_cache').glob('*'))
        if cache_files:
            total_size = sum(f.stat().st_size for f in cache_files)
            print(f"ğŸ“ TensorRTã‚­ãƒ£ãƒƒã‚·ãƒ¥: {len(cache_files)}ãƒ•ã‚¡ã‚¤ãƒ«, {total_size/1024/1024:.2f}MB")
            for f in cache_files:
                print(f"  - {f.name}: {f.stat().st_size/1024/1024:.2f}MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    print("ğŸš€ TensorRTå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹")
    print("=" * 50)
    
    # ç’°å¢ƒç¢ºèª
    try:
        import onnxruntime as ort
        providers = ort.get_available_providers()
        print(f"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {providers}")
        
        if 'TensorrtExecutionProvider' not in providers:
            print("âŒ TensorrtExecutionProvider ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return
            
    except ImportError:
        print("âŒ ONNXRuntime ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    # å¤‰æ›å®Ÿè¡Œ
    success = convert_with_onnxruntime()
    
    if success:
        print("\nğŸ‰ TensorRTå¤‰æ›å®Œäº†ï¼")
        print("ğŸ“ˆ æ¬¡å›ã®æ¨è«–ã‹ã‚‰é«˜é€ŸåŒ–ã•ã‚Œã¾ã™")
    else:
        print("\nâŒ TensorRTå¤‰æ›å¤±æ•—")

if __name__ == "__main__":
    main()