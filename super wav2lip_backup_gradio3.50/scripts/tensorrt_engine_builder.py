#!/usr/bin/env python3
"""
TensorRT Engine Builder for Super Wav2Lip
å‹•çš„è§£åƒåº¦å¯¾å¿œ ONNX â†’ TensorRT å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨ä¾‹:
python scripts/tensorrt_engine_builder.py --model wav2lip --optimize
python scripts/tensorrt_engine_builder.py --model gfpgan --dynamic --precision fp16
"""

import os
import sys
import argparse
import logging
from pathlib import Path
import tensorrt as trt
import onnx
import numpy as np

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TensorRTEngineBuilder:
    def __init__(self, verbose=False):
        """TensorRT ã‚¨ãƒ³ã‚¸ãƒ³ãƒ“ãƒ«ãƒ€ãƒ¼åˆæœŸåŒ–"""
        self.logger = trt.Logger(trt.Logger.VERBOSE if verbose else trt.Logger.INFO)
        self.builder = trt.Builder(self.logger)
        self.config = self.builder.create_builder_config()
        
        # GPU ãƒ¡ãƒ¢ãƒªè¨­å®šï¼ˆRTX 3050ç”¨æœ€é©åŒ–ï¼‰
        self.config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 2 << 30)  # 2GB
        
    def build_wav2lip_engine(self, onnx_path: str, engine_path: str, 
                           dynamic_batch=True, fp16=True, int8=False):
        """
        Wav2Lip ONNX â†’ TensorRT ã‚¨ãƒ³ã‚¸ãƒ³å¤‰æ›
        
        Args:
            onnx_path: å…¥åŠ›ONNXãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
            engine_path: å‡ºåŠ›TensorRTã‚¨ãƒ³ã‚¸ãƒ³ãƒ‘ã‚¹  
            dynamic_batch: å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºå¯¾å¿œ
            fp16: FP16ç²¾åº¦ä½¿ç”¨
            int8: INT8ç²¾åº¦ä½¿ç”¨ï¼ˆè¦ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        """
        logger.info(f"ğŸš€ Wav2Lip TensorRT ã‚¨ãƒ³ã‚¸ãƒ³æ§‹ç¯‰é–‹å§‹: {onnx_path}")
        
        # ç²¾åº¦è¨­å®š
        if fp16 and self.builder.platform_has_fast_fp16:
            self.config.set_flag(trt.BuilderFlag.FP16)
            logger.info("âœ… FP16ç²¾åº¦æœ‰åŠ¹")
        if int8 and self.builder.platform_has_fast_int8:
            self.config.set_flag(trt.BuilderFlag.INT8)
            logger.info("âœ… INT8ç²¾åº¦æœ‰åŠ¹")
            
        # ONNXèª­ã¿è¾¼ã¿
        network = self.builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
        parser = trt.OnnxParser(network, self.logger)
        
        with open(onnx_path, 'rb') as model:
            if not parser.parse(model.read()):
                logger.error("âŒ ONNXè§£æå¤±æ•—")
                for error in range(parser.num_errors):
                    logger.error(parser.get_error(error))
                return False
                
        # å‹•çš„å½¢çŠ¶è¨­å®šï¼ˆWav2Lipç”¨ï¼‰
        if dynamic_batch:
            # 1ã¤ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«å…¨ã¦ã®å…¥åŠ›ã‚’è¨­å®š
            profile = self.builder.create_optimization_profile()
            
            # ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ å…¥åŠ›: [batch, 1, 80, 16] å›ºå®šã‚µã‚¤ã‚º
            mel_input = network.get_input(0)
            profile.set_shape(mel_input.name, 
                            (1, 1, 80, 16),     # min
                            (1, 1, 80, 16),     # opt  
                            (1, 1, 80, 16))     # max
            
            # å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ å…¥åŠ›: [batch, 6, 96, 96] å›ºå®š
            video_input = network.get_input(1)
            profile.set_shape(video_input.name,
                            (1, 6, 96, 96),   # min
                            (1, 6, 96, 96),   # opt
                            (1, 6, 96, 96))   # max
            
            self.config.add_optimization_profile(profile)
            logger.info("âœ… å‹•çš„å½¢çŠ¶ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šå®Œäº†")
            
        # ã‚¨ãƒ³ã‚¸ãƒ³æ§‹ç¯‰
        logger.info("ğŸ”§ TensorRT ã‚¨ãƒ³ã‚¸ãƒ³æ§‹ç¯‰ä¸­...")
        serialized_engine = self.builder.build_serialized_network(network, self.config)
        
        if serialized_engine is None:
            logger.error("âŒ ã‚¨ãƒ³ã‚¸ãƒ³æ§‹ç¯‰å¤±æ•—")
            return False
            
        # ã‚¨ãƒ³ã‚¸ãƒ³ä¿å­˜
        with open(engine_path, 'wb') as f:
            f.write(serialized_engine)
            
        engine_size = os.path.getsize(engine_path) / (1024 * 1024)
        logger.info(f"âœ… Wav2Lip TensorRT ã‚¨ãƒ³ã‚¸ãƒ³å®Œæˆ: {engine_path} ({engine_size:.1f}MB)")
        return True
        
    def build_gfpgan_engine(self, onnx_path: str, engine_path: str,
                          dynamic_resolution=True, fp16=True):
        """
        GFPGAN ONNX â†’ TensorRT ã‚¨ãƒ³ã‚¸ãƒ³å¤‰æ›ï¼ˆå‹•çš„è§£åƒåº¦å¯¾å¿œï¼‰
        
        Args:
            onnx_path: å…¥åŠ›ONNXãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
            engine_path: å‡ºåŠ›TensorRTã‚¨ãƒ³ã‚¸ãƒ³ãƒ‘ã‚¹
            dynamic_resolution: å‹•çš„è§£åƒåº¦å¯¾å¿œ
            fp16: FP16ç²¾åº¦ä½¿ç”¨
        """
        logger.info(f"ğŸ¨ GFPGAN TensorRT ã‚¨ãƒ³ã‚¸ãƒ³æ§‹ç¯‰é–‹å§‹: {onnx_path}")
        
        # ç²¾åº¦è¨­å®š
        if fp16 and self.builder.platform_has_fast_fp16:
            self.config.set_flag(trt.BuilderFlag.FP16)
            logger.info("âœ… FP16ç²¾åº¦æœ‰åŠ¹")
            
        # ONNXèª­ã¿è¾¼ã¿
        network = self.builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
        parser = trt.OnnxParser(network, self.logger)
        
        with open(onnx_path, 'rb') as model:
            if not parser.parse(model.read()):
                logger.error("âŒ ONNXè§£æå¤±æ•—")
                for error in range(parser.num_errors):
                    logger.error(parser.get_error(error))
                return False
                
        # å‹•çš„è§£åƒåº¦è¨­å®šï¼ˆGFPGANç”¨ï¼‰
        if dynamic_resolution:
            input_tensor = network.get_input(0)
            profile = self.builder.create_optimization_profile()
            
            # å‹•çš„è§£åƒåº¦: 256x256 â†’ 512x512 â†’ 1024x1024
            profile.set_shape(input_tensor.name,
                            (1, 3, 256, 256),    # min: è»½é‡å‡¦ç†
                            (1, 3, 512, 512),    # opt: æ¨™æº–å“è³ª  
                            (1, 3, 1024, 1024))  # max: æœ€é«˜å“è³ª
            
            self.config.add_optimization_profile(profile)
            logger.info("âœ… å‹•çš„è§£åƒåº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šå®Œäº† (256-1024px)")
            
        # æœ€é©åŒ–è¨­å®š
        self.config.set_flag(trt.BuilderFlag.PREFER_PRECISION_CONSTRAINTS)
        
        # ã‚¨ãƒ³ã‚¸ãƒ³æ§‹ç¯‰
        logger.info("ğŸ”§ GFPGAN TensorRT ã‚¨ãƒ³ã‚¸ãƒ³æ§‹ç¯‰ä¸­...")
        serialized_engine = self.builder.build_serialized_network(network, self.config)
        
        if serialized_engine is None:
            logger.error("âŒ ã‚¨ãƒ³ã‚¸ãƒ³æ§‹ç¯‰å¤±æ•—")
            return False
            
        # ã‚¨ãƒ³ã‚¸ãƒ³ä¿å­˜
        with open(engine_path, 'wb') as f:
            f.write(serialized_engine)
            
        engine_size = os.path.getsize(engine_path) / (1024 * 1024)
        logger.info(f"âœ… GFPGAN TensorRT ã‚¨ãƒ³ã‚¸ãƒ³å®Œæˆ: {engine_path} ({engine_size:.1f}MB)")
        return True
        
    def benchmark_engine(self, engine_path: str, input_shape: tuple, iterations=100):
        """
        TensorRT ã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        
        Args:
            engine_path: TensorRTã‚¨ãƒ³ã‚¸ãƒ³ãƒ‘ã‚¹
            input_shape: å…¥åŠ›å½¢çŠ¶
            iterations: å®Ÿè¡Œå›æ•°
        """
        logger.info(f"âš¡ TensorRT ã‚¨ãƒ³ã‚¸ãƒ³ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯: {engine_path}")
        
        # ã‚¨ãƒ³ã‚¸ãƒ³èª­ã¿è¾¼ã¿
        runtime = trt.Runtime(self.logger)
        with open(engine_path, 'rb') as f:
            engine = runtime.deserialize_cuda_engine(f.read())
            
        context = engine.create_execution_context()
        
        # GPU ãƒ¡ãƒ¢ãƒªç¢ºä¿
        import pycuda.driver as cuda
        import pycuda.autoinit
        
        # å…¥åŠ›ãƒ»å‡ºåŠ›ãƒãƒƒãƒ•ã‚¡ç¢ºä¿
        input_buffer = cuda.mem_alloc(np.prod(input_shape) * np.dtype(np.float32).itemsize)
        output_shape = (1, 3, input_shape[2], input_shape[3])  # åŒã˜ã‚µã‚¤ã‚ºå‡ºåŠ›æƒ³å®š
        output_buffer = cuda.mem_alloc(np.prod(output_shape) * np.dtype(np.float32).itemsize)
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        import time
        cuda.Context.synchronize()
        start_time = time.time()
        
        for _ in range(iterations):
            context.execute_v2([int(input_buffer), int(output_buffer)])
            cuda.Context.synchronize()
            
        end_time = time.time()
        
        avg_time = (end_time - start_time) / iterations * 1000  # ms
        fps = 1000 / avg_time
        
        logger.info(f"âœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
        logger.info(f"   å¹³å‡å®Ÿè¡Œæ™‚é–“: {avg_time:.2f}ms")
        logger.info(f"   FPS: {fps:.1f}")
        logger.info(f"   å…¥åŠ›å½¢çŠ¶: {input_shape}")
        
        return avg_time, fps

def main():
    parser = argparse.ArgumentParser(description='TensorRT ã‚¨ãƒ³ã‚¸ãƒ³ãƒ“ãƒ«ãƒ€ãƒ¼')
    parser.add_argument('--model', choices=['wav2lip', 'gfpgan', 'both'], 
                       default='both', help='å¤‰æ›å¯¾è±¡ãƒ¢ãƒ‡ãƒ«')
    parser.add_argument('--onnx-dir', default='/app/models/onnx', 
                       help='ONNXãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--engine-dir', default='/app/models/tensorrt',
                       help='TensorRTã‚¨ãƒ³ã‚¸ãƒ³å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--dynamic', action='store_true',
                       help='å‹•çš„å½¢çŠ¶å¯¾å¿œ')
    parser.add_argument('--precision', choices=['fp32', 'fp16', 'int8'],
                       default='fp16', help='æ¨è«–ç²¾åº¦')
    parser.add_argument('--optimize', action='store_true',
                       help='æœ€é©åŒ–æœ‰åŠ¹')
    parser.add_argument('--benchmark', action='store_true',
                       help='ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ')
    parser.add_argument('--verbose', action='store_true',
                       help='è©³ç´°ãƒ­ã‚°')
    
    args = parser.parse_args()
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    Path(args.engine_dir).mkdir(parents=True, exist_ok=True)
    
    # ãƒ“ãƒ«ãƒ€ãƒ¼åˆæœŸåŒ–
    builder = TensorRTEngineBuilder(verbose=args.verbose)
    
    # Wav2Lip ã‚¨ãƒ³ã‚¸ãƒ³æ§‹ç¯‰
    if args.model in ['wav2lip', 'both']:
        wav2lip_onnx = f"{args.onnx_dir}/wav2lip_gan.onnx"
        wav2lip_engine = f"{args.engine_dir}/wav2lip_gan.trt"
        
        if os.path.exists(wav2lip_onnx):
            success = builder.build_wav2lip_engine(
                wav2lip_onnx, wav2lip_engine,
                dynamic_batch=args.dynamic,
                fp16=(args.precision == 'fp16'),
                int8=(args.precision == 'int8')
            )
            
            if success and args.benchmark:
                builder.benchmark_engine(wav2lip_engine, (1, 1, 80, 16))
        else:
            logger.warning(f"âš ï¸ Wav2Lip ONNX not found: {wav2lip_onnx}")
    
    # GFPGAN ã‚¨ãƒ³ã‚¸ãƒ³æ§‹ç¯‰  
    if args.model in ['gfpgan', 'both']:
        gfpgan_onnx = f"{args.onnx_dir}/../enhancers/GFPGAN/GFPGANv1.4.onnx"
        gfpgan_engine = f"{args.engine_dir}/gfpgan_v1.4.trt"
        
        if os.path.exists(gfpgan_onnx):
            success = builder.build_gfpgan_engine(
                gfpgan_onnx, gfpgan_engine,
                dynamic_resolution=args.dynamic,
                fp16=(args.precision == 'fp16')
            )
            
            if success and args.benchmark:
                builder.benchmark_engine(gfpgan_engine, (1, 3, 512, 512))
        else:
            logger.warning(f"âš ï¸ GFPGAN ONNX not found: {gfpgan_onnx}")
    
    logger.info("ğŸ‰ TensorRT ã‚¨ãƒ³ã‚¸ãƒ³æ§‹ç¯‰å®Œäº†!")

if __name__ == "__main__":
    main()