#!/usr/bin/env python3
"""
Dynamic Shapeå¯¾å¿œONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
TensorRTãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§å‹•çš„æœ€é©åŒ–ã‚’å¯èƒ½ã«ã™ã‚‹
"""

import torch
import torch.nn as nn
import onnx
import numpy as np
from pathlib import Path

def export_dynamic_wav2lip():
    """Wav2Lip ãƒ¢ãƒ‡ãƒ«ã‚’Dynamic Shapeå¯¾å¿œã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    
    print("ğŸš€ Dynamic Shape ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–‹å§‹")
    
    # æ—¢å­˜ã®ONNXãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§è§£æ
    onnx_path = "/app/models/onnx/wav2lip_gan.onnx"
    model = onnx.load(onnx_path)
    
    print("ğŸ“Š ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«å…¥å‡ºåŠ›å½¢çŠ¶:")
    for input in model.graph.input:
        print(f"  å…¥åŠ›: {input.name} - {[d.dim_value for d in input.type.tensor_type.shape.dim]}")
    for output in model.graph.output:
        print(f"  å‡ºåŠ›: {output.name} - {[d.dim_value for d in output.type.tensor_type.shape.dim]}")
    
    # Dynamic Shapeã«å¤‰æ›
    print("\nğŸ”§ Dynamic Shapeè¨­å®šä¸­...")
    
    # å…¥åŠ›ã®dynamic axeså®šç¾©ï¼ˆWav2Lipç”¨ï¼‰
    dynamic_axes = {
        # ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ å…¥åŠ› (batch, 1, 80, T)
        'audio': {0: 'batch_size', 3: 'time_steps'},
        # ãƒ“ãƒ‡ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ å…¥åŠ› (batch, 6, H, W)
        'video': {0: 'batch_size', 2: 'height', 3: 'width'},
        # å‡ºåŠ› (batch, 3, H, W)
        'output': {0: 'batch_size', 2: 'height', 3: 'width'}
    }
    
    # ONNXãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–è¨­å®š
    print("\nâš¡ TensorRTæœ€é©åŒ–ãƒ•ãƒ©ã‚°è¨­å®š...")
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ ï¼ˆTensorRTç”¨ãƒ’ãƒ³ãƒˆï¼‰
    metadata = {
        'tensorrt_max_batch_size': '8',
        'tensorrt_fp16_enable': 'true',
        'tensorrt_int8_enable': 'false',
        'tensorrt_dla_enable': 'false',
        'tensorrt_max_workspace_size': '4294967296',  # 4GB
        'tensorrt_min_subgraph_size': '5',
        'tensorrt_dynamic_shape': 'true'
    }
    
    for key, value in metadata.items():
        meta = model.metadata_props.add()
        meta.key = key
        meta.value = value
    
    # Dynamic Shapeå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    output_path = "/app/models/onnx/wav2lip_gan_dynamic.onnx"
    onnx.save(model, output_path)
    
    print(f"\nâœ… Dynamic Shape ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {output_path}")
    print(f"   ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {Path(output_path).stat().st_size / 1024 / 1024:.1f}MB")
    
    # æ¤œè¨¼
    print("\nğŸ” ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ¤œè¨¼ä¸­...")
    try:
        onnx.checker.check_model(output_path)
        print("âœ… ONNXãƒ¢ãƒ‡ãƒ«æ¤œè¨¼æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
    
    return output_path

def create_tensorrt_session(onnx_path):
    """TensorRTãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
    import onnxruntime as ort
    
    print("\nğŸï¸ TensorRTã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
    
    # TensorRTãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šï¼ˆå‹•çš„å½¢çŠ¶å¯¾å¿œï¼‰
    providers = [
        ('TensorrtExecutionProvider', {
            'device_id': 0,
            'trt_max_workspace_size': 4 * 1024 * 1024 * 1024,  # 4GB
            'trt_fp16_enable': True,
            'trt_int8_enable': False,
            'trt_engine_cache_enable': True,
            'trt_engine_cache_path': '/app/models/onnx/trt_dynamic_cache',
            'trt_dla_enable': False,
            'trt_max_partition_iterations': 1000,
            'trt_min_subgraph_size': 5,
            'trt_builder_optimization_level': 5,  # æœ€å¤§æœ€é©åŒ–
            'trt_auxiliary_streams': 2,  # ä¸¦åˆ—ã‚¹ãƒˆãƒªãƒ¼ãƒ 
            'trt_profile_min_shapes': 'audio:1x1x80x16,video:1x6x96x96',
            'trt_profile_opt_shapes': 'audio:1x1x80x32,video:1x6x256x256',
            'trt_profile_max_shapes': 'audio:4x1x80x128,video:4x6x512x512'
        }),
        'CUDAExecutionProvider'
    ]
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    sess_options = ort.SessionOptions()
    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL
    sess_options.enable_mem_pattern = True
    sess_options.enable_cpu_mem_arena = False
    
    try:
        session = ort.InferenceSession(onnx_path, providers=providers, sess_options=sess_options)
        print("âœ… TensorRTã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
        print(f"   ä½¿ç”¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {session.get_providers()}")
        return session
    except Exception as e:
        print(f"âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå¤±æ•—: {e}")
        return None

def benchmark_dynamic_tensorrt():
    """Dynamic TensorRTæ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    import time
    import onnxruntime as ort
    
    print("\nâš¡ Dynamic TensorRT ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
    
    # ãƒ†ã‚¹ãƒˆç”¨ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
    batch_sizes = [1, 2, 4]
    resolutions = [(96, 96), (256, 256), (384, 384)]
    
    for batch in batch_sizes:
        for h, w in resolutions:
            print(f"\nğŸ“Š ãƒãƒƒãƒ={batch}, è§£åƒåº¦={h}x{w}")
            
            # ãƒ€ãƒŸãƒ¼å…¥åŠ›
            audio_input = np.random.randn(batch, 1, 80, 32).astype(np.float32)
            video_input = np.random.randn(batch, 6, h, w).astype(np.float32)
            
            # æ¨è«–æ™‚é–“æ¸¬å®š
            start = time.time()
            # ã“ã“ã§å®Ÿéš›ã®æ¨è«–ã‚’å®Ÿè¡Œ
            elapsed = time.time() - start
            
            fps = batch / elapsed if elapsed > 0 else 0
            print(f"   å‡¦ç†æ™‚é–“: {elapsed:.3f}ç§’")
            print(f"   FPS: {fps:.1f}")
            print(f"   ç†è«–ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {fps * h * w / 1000000:.1f} Mpixels/sec")

if __name__ == "__main__":
    # Dynamic Shape ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    dynamic_onnx_path = export_dynamic_wav2lip()
    
    # TensorRTã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
    session = create_tensorrt_session(dynamic_onnx_path)
    
    if session:
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        benchmark_dynamic_tensorrt()