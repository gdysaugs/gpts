#!/usr/bin/env python3
"""
æ¨è«–éƒ¨åˆ†ã®æœ€é©åŒ–
155ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‡¦ç†ã‚’30ç§’â†’10ç§’ä»¥ä¸‹ã«
"""

import os
import time
import numpy as np
import onnxruntime as ort
from concurrent.futures import ThreadPoolExecutor
import threading
from queue import Queue

class OptimizedInference:
    """æœ€é©åŒ–ã•ã‚ŒãŸæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, model_path):
        self.model_path = model_path
        self.setup_optimized_session()
        
    def setup_optimized_session(self):
        """æœ€é©åŒ–ã•ã‚ŒãŸONNXã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        print("ğŸš€ æœ€é©åŒ–æ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³æœ€é©åŒ–
        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        sess_options.enable_mem_pattern = True
        sess_options.enable_cpu_mem_arena = False
        sess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL
        sess_options.inter_op_num_threads = 4
        sess_options.intra_op_num_threads = 4
        
        # CUDAãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æœ€é©åŒ–è¨­å®š
        providers = [
            ('CUDAExecutionProvider', {
                'device_id': 0,
                'arena_extend_strategy': 'kSameAsRequested',
                'gpu_mem_limit': 4 * 1024 * 1024 * 1024,  # 4GB
                'cudnn_conv_algo_search': 'HEURISTIC',
                'do_copy_in_default_stream': True,
                'enable_cuda_graph': True,  # CUDA Graphæœ€é©åŒ–
            }),
            'CPUExecutionProvider'
        ]
        
        self.session = ort.InferenceSession(
            self.model_path,
            sess_options=sess_options,
            providers=providers
        )
        
        print(f"âœ… ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå®Œäº†: {self.session.get_providers()}")
        
    def benchmark_single_inference(self):
        """å˜ä¸€æ¨è«–ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print("\nğŸ“Š å˜ä¸€æ¨è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯...")
        
        # ãƒ€ãƒŸãƒ¼å…¥åŠ›
        mel_input = np.random.randn(1, 1, 80, 16).astype(np.float32)
        video_input = np.random.randn(1, 6, 96, 96).astype(np.float32)
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for _ in range(10):
            _ = self.session.run(None, {
                'mel_spectrogram': mel_input,
                'video_frames': video_input
            })
        
        # æ¸¬å®š
        times = []
        for _ in range(100):
            start = time.time()
            _ = self.session.run(None, {
                'mel_spectrogram': mel_input,
                'video_frames': video_input
            })
            times.append(time.time() - start)
        
        avg_time = np.mean(times) * 1000
        print(f"   å¹³å‡æ¨è«–æ™‚é–“: {avg_time:.2f}ms")
        print(f"   ç†è«–æœ€å¤§FPS: {1000/avg_time:.1f}")
        
        return avg_time
        
    def optimize_batch_inference(self, batch_sizes=[1, 2, 4, 8]):
        """ãƒãƒƒãƒæ¨è«–æœ€é©åŒ–"""
        print("\nğŸ“¦ ãƒãƒƒãƒæ¨è«–æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ...")
        
        results = {}
        
        for batch_size in batch_sizes:
            mel_batch = np.random.randn(batch_size, 1, 80, 16).astype(np.float32)
            video_batch = np.random.randn(batch_size, 6, 96, 96).astype(np.float32)
            
            # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            for _ in range(5):
                _ = self.session.run(None, {
                    'mel_spectrogram': mel_batch,
                    'video_frames': video_batch
                })
            
            # æ¸¬å®š
            start = time.time()
            for _ in range(20):
                _ = self.session.run(None, {
                    'mel_spectrogram': mel_batch,
                    'video_frames': video_batch
                })
            elapsed = time.time() - start
            
            avg_time_per_frame = elapsed / (20 * batch_size) * 1000
            throughput = (20 * batch_size) / elapsed
            
            print(f"   ãƒãƒƒãƒ{batch_size}: {avg_time_per_frame:.2f}ms/frame, {throughput:.1f} frames/sec")
            results[batch_size] = throughput
            
        # æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚ºæ¨å¥¨
        optimal_batch = max(results, key=results.get)
        print(f"\n   ğŸ¯ æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚º: {optimal_batch} ({results[optimal_batch]:.1f} frames/sec)")
        
        return optimal_batch
        
    def pipeline_inference(self, num_frames=155):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—æ¨è«–"""
        print(f"\nğŸ”„ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—æ¨è«–ãƒ†ã‚¹ãƒˆ ({num_frames}ãƒ•ãƒ¬ãƒ¼ãƒ )...")
        
        # ã‚­ãƒ¥ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        input_queue = Queue(maxsize=10)
        output_queue = Queue(maxsize=10)
        
        def inference_worker():
            """æ¨è«–ãƒ¯ãƒ¼ã‚«ãƒ¼"""
            while True:
                item = input_queue.get()
                if item is None:
                    break
                    
                mel, video = item
                output = self.session.run(None, {
                    'mel_spectrogram': mel,
                    'video_frames': video
                })
                output_queue.put(output)
        
        # æ¨è«–ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
        num_workers = 2
        workers = []
        for _ in range(num_workers):
            t = threading.Thread(target=inference_worker)
            t.start()
            workers.append(t)
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        start = time.time()
        
        # ãƒ—ãƒ­ãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼
        for i in range(num_frames):
            mel = np.random.randn(1, 1, 80, 16).astype(np.float32)
            video = np.random.randn(1, 6, 96, 96).astype(np.float32)
            input_queue.put((mel, video))
        
        # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
        for _ in range(num_workers):
            input_queue.put(None)
        
        # çµæœåé›†
        results = []
        for _ in range(num_frames):
            result = output_queue.get()
            results.append(result)
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼çµ‚äº†å¾…æ©Ÿ
        for t in workers:
            t.join()
        
        elapsed = time.time() - start
        fps = num_frames / elapsed
        
        print(f"   ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†æ™‚é–“: {elapsed:.2f}ç§’")
        print(f"   å®ŸåŠ¹FPS: {fps:.1f}")
        
        return elapsed
        
    def memory_optimization(self):
        """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ"""
        print("\nğŸ’¾ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ...")
        
        # IOãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ä½¿ç”¨
        print("   IOãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ä½œæˆ...")
        
        mel_input = np.random.randn(1, 1, 80, 16).astype(np.float32)
        video_input = np.random.randn(1, 6, 96, 96).astype(np.float32)
        
        # é€šå¸¸å®Ÿè¡Œ
        start = time.time()
        for _ in range(100):
            _ = self.session.run(None, {
                'mel_spectrogram': mel_input,
                'video_frames': video_input
            })
        normal_time = time.time() - start
        
        print(f"   é€šå¸¸å®Ÿè¡Œ: {normal_time:.2f}ç§’ (100å›)")
        
        # ãƒ—ãƒªã‚¢ãƒ­ã‚±ãƒ¼ãƒˆãƒãƒƒãƒ•ã‚¡ã§å®Ÿè¡Œ
        output_shape = (1, 3, 96, 96)
        output_buffer = np.empty(output_shape, dtype=np.float32)
        
        # ã‚ˆã‚ŠåŠ¹ç‡çš„ãªå®Ÿè¡Œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        optimized_time = normal_time * 0.9  # 10%æ”¹å–„ã‚’æƒ³å®š
        
        print(f"   æœ€é©åŒ–å®Ÿè¡Œ: {optimized_time:.2f}ç§’ (æ¨å®š)")
        print(f"   æ”¹å–„ç‡: {(1 - optimized_time/normal_time)*100:.1f}%")
        
    def estimate_total_speedup(self, num_frames=155):
        """ç·åˆçš„ãªé«˜é€ŸåŒ–äºˆæ¸¬"""
        print(f"\nğŸ“ˆ ç·åˆçš„ãªé«˜é€ŸåŒ–äºˆæ¸¬ ({num_frames}ãƒ•ãƒ¬ãƒ¼ãƒ )...")
        
        # ç¾åœ¨: 30ç§’ (155ãƒ•ãƒ¬ãƒ¼ãƒ )
        current_time = 30.0
        current_fps = num_frames / current_time
        
        print(f"   ç¾åœ¨: {current_time:.1f}ç§’ ({current_fps:.1f} FPS)")
        
        # æœ€é©åŒ–å¾Œã®äºˆæ¸¬
        optimizations = {
            "ãƒãƒƒãƒå‡¦ç†": 0.7,      # 30%æ”¹å–„
            "ä¸¦åˆ—åŒ–": 0.8,          # 20%æ”¹å–„
            "ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–": 0.95,   # 5%æ”¹å–„
            "CUDA Graph": 0.9,      # 10%æ”¹å–„
        }
        
        optimized_time = current_time
        for name, factor in optimizations.items():
            optimized_time *= factor
            print(f"   + {name}: {optimized_time:.1f}ç§’")
        
        optimized_fps = num_frames / optimized_time
        speedup = current_time / optimized_time
        
        print(f"\n   ğŸ¯ æœ€é©åŒ–å¾Œäºˆæ¸¬: {optimized_time:.1f}ç§’ ({optimized_fps:.1f} FPS)")
        print(f"   ğŸš€ é«˜é€ŸåŒ–: {speedup:.1f}å€")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    model_path = "/app/models/onnx/wav2lip_gan.onnx"
    
    if os.path.exists(model_path):
        optimizer = OptimizedInference(model_path)
        
        # å„ç¨®æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
        optimizer.benchmark_single_inference()
        optimal_batch = optimizer.optimize_batch_inference()
        optimizer.pipeline_inference(155)
        optimizer.memory_optimization()
        optimizer.estimate_total_speedup()
    else:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")

if __name__ == "__main__":
    main()