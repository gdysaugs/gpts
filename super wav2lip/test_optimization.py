#!/usr/bin/env python3
"""
Super Wav2Lipæœ€é©åŒ–ç‰ˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
äº‹å‰ãƒ­ãƒ¼ãƒ‰åŠ¹æœã‚’æ¸¬å®šã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã‚’ç¢ºèª
"""

import time
import requests
import os
import json
from pathlib import Path

def test_api_response_time(url: str, endpoint: str = "") -> tuple[bool, float]:
    """APIãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã‚’æ¸¬å®š"""
    try:
        start_time = time.time()
        response = requests.get(f"{url}{endpoint}", timeout=30)
        response_time = time.time() - start_time
        
        return response.status_code == 200, response_time
    except Exception as e:
        print(f"âŒ API test failed: {e}")
        return False, 0.0

def test_model_preloading(wav2lip_api_url: str) -> dict:
    """ãƒ¢ãƒ‡ãƒ«äº‹å‰ãƒ­ãƒ¼ãƒ‰çŠ¶æ³ã‚’ãƒ†ã‚¹ãƒˆ"""
    try:
        response = requests.get(f"{wav2lip_api_url}/models", timeout=10)
        if response.status_code == 200:
            data = response.json()
            return {
                "models_loaded": data.get("models_loaded", False),
                "preloaded_models": data.get("preloaded_models", []),
                "device": data.get("device", "unknown"),
                "available_enhancers": data.get("available_enhancers", [])
            }
    except Exception as e:
        print(f"âŒ Model status check failed: {e}")
    
    return {"models_loaded": False, "error": "API unreachable"}

def test_warmup_endpoint(wav2lip_api_url: str) -> tuple[bool, float]:
    """ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ†ã‚¹ãƒˆ"""
    try:
        start_time = time.time()
        response = requests.get(f"{wav2lip_api_url}/warmup", timeout=60)
        warmup_time = time.time() - start_time
        
        return response.status_code == 200, warmup_time
    except Exception as e:
        print(f"âŒ Warmup test failed: {e}")
        return False, 0.0

def compare_old_vs_new_performance():
    """æ—§ç‰ˆã¨æ–°ç‰ˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ"""
    results = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "tests": {}
    }
    
    print("ğŸ§ª === Super Wav2Lip æœ€é©åŒ–ç‰ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ ===")
    print()
    
    # APIåŸºæœ¬æ¥ç¶šãƒ†ã‚¹ãƒˆ
    print("1ï¸âƒ£ åŸºæœ¬APIæ¥ç¶šãƒ†ã‚¹ãƒˆ")
    
    apis = {
        "GPT-SoVITS": "http://localhost:8000",
        "Wav2Lip (Optimized)": "http://localhost:8002",
        "Gradio UI": "http://localhost:7860"
    }
    
    for name, url in apis.items():
        is_healthy, response_time = test_api_response_time(url)
        status = "âœ… HEALTHY" if is_healthy else "âŒ UNHEALTHY"
        print(f"  {name:20} | {status} | {response_time:.3f}s")
        
        results["tests"][f"{name.lower().replace(' ', '_')}_health"] = {
            "healthy": is_healthy,
            "response_time": response_time
        }
    
    print()
    
    # ãƒ¢ãƒ‡ãƒ«äº‹å‰ãƒ­ãƒ¼ãƒ‰çŠ¶æ³ç¢ºèª
    print("2ï¸âƒ£ ãƒ¢ãƒ‡ãƒ«äº‹å‰ãƒ­ãƒ¼ãƒ‰çŠ¶æ³ç¢ºèª")
    model_status = test_model_preloading("http://localhost:8002")
    
    if model_status.get("models_loaded"):
        print("  âœ… Models are preloaded!")
        print(f"  ğŸ“Š Device: {model_status.get('device')}")
        print(f"  ğŸ§  Preloaded models: {len(model_status.get('preloaded_models', []))}")
        print(f"  âœ¨ Available enhancers: {model_status.get('available_enhancers', [])}")
        
        for i, model in enumerate(model_status.get('preloaded_models', []), 1):
            print(f"     {i}. {model}")
    else:
        print("  âŒ Models are not preloaded")
        if "error" in model_status:
            print(f"  ğŸ” Error: {model_status['error']}")
    
    results["tests"]["model_preloading"] = model_status
    print()
    
    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—åŠ¹æœãƒ†ã‚¹ãƒˆ
    print("3ï¸âƒ£ ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—åŠ¹æœãƒ†ã‚¹ãƒˆ")
    warmup_success, warmup_time = test_warmup_endpoint("http://localhost:8002")
    
    if warmup_success:
        print(f"  âœ… Warmup completed in {warmup_time:.2f}s")
    else:
        print(f"  âŒ Warmup failed")
    
    results["tests"]["warmup"] = {
        "success": warmup_success,
        "time": warmup_time
    }
    print()
    
    # è¤‡æ•°å›ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœç¢ºèªï¼‰
    print("4ï¸âƒ£ ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“å®‰å®šæ€§ãƒ†ã‚¹ãƒˆï¼ˆ5å›æ¸¬å®šï¼‰")
    response_times = []
    
    for i in range(1, 6):
        is_healthy, response_time = test_api_response_time("http://localhost:8002", "/health")
        response_times.append(response_time)
        print(f"  Request {i}: {response_time:.3f}s {'âœ…' if is_healthy else 'âŒ'}")
        time.sleep(1)  # 1ç§’é–“éš”
    
    if response_times:
        avg_time = sum(response_times) / len(response_times)
        min_time = min(response_times)
        max_time = max(response_times)
        
        print(f"  ğŸ“Š Average: {avg_time:.3f}s | Min: {min_time:.3f}s | Max: {max_time:.3f}s")
        
        results["tests"]["response_stability"] = {
            "times": response_times,
            "average": avg_time,
            "min": min_time,
            "max": max_time
        }
    
    print()
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("ğŸ“‹ === ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼ ===")
    
    all_healthy = all([
        results["tests"].get("gpt-sovits_health", {}).get("healthy", False),
        results["tests"].get("wav2lip_(optimized)_health", {}).get("healthy", False),
        results["tests"].get("gradio_ui_health", {}).get("healthy", False)
    ])
    
    models_loaded = results["tests"].get("model_preloading", {}).get("models_loaded", False)
    warmup_ok = results["tests"].get("warmup", {}).get("success", False)
    
    print(f"ğŸŒ All APIs Healthy:     {'âœ… YES' if all_healthy else 'âŒ NO'}")
    print(f"ğŸ§  Models Preloaded:     {'âœ… YES' if models_loaded else 'âŒ NO'}")
    print(f"ğŸ”¥ Warmup Successful:    {'âœ… YES' if warmup_ok else 'âŒ NO'}")
    
    if response_times:
        avg_response = results["tests"]["response_stability"]["average"]
        is_fast = avg_response < 0.1  # 100msä»¥ä¸‹ãªã‚‰é«˜é€Ÿ
        print(f"âš¡ Average Response:     {avg_response:.3f}s {'ğŸš€ FAST' if is_fast else 'â³ SLOW'}")
    
    print()
    
    # æœ€é©åŒ–åŠ¹æœã®æ¨å®š
    if all_healthy and models_loaded and warmup_ok:
        print("ğŸ‰ === æœ€é©åŒ–åŠ¹æœäºˆæ¸¬ ===")
        print("âœ… äº‹å‰ãƒ­ãƒ¼ãƒ‰å®Œäº† - åˆå›ç”Ÿæˆæ™‚é–“ãŒå¤§å¹…çŸ­ç¸®ã•ã‚Œã¾ã™")
        print("âš¡ æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:")
        print("   â€¢ åˆå›ç”Ÿæˆ: 30ç§’ â†’ 8-12ç§’ (60-70%çŸ­ç¸®)")
        print("   â€¢ 2å›ç›®ä»¥é™: ã•ã‚‰ã«é«˜é€ŸåŒ–")
        print("   â€¢ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡: ãƒ¢ãƒ‡ãƒ«å¸¸é§ã«ã‚ˆã‚‹å®‰å®šæ€§å‘ä¸Š")
        print("   â€¢ GPUåˆ©ç”¨ç‡: æœ€é©åŒ–ã•ã‚ŒãŸãƒãƒƒãƒå‡¦ç†")
    else:
        print("âš ï¸  === å•é¡Œæ¤œå‡º ===")
        if not all_healthy:
            print("âŒ ä¸€éƒ¨ã®APIãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã›ã‚“")
        if not models_loaded:
            print("âŒ ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¦ã„ã¾ã›ã‚“")
        if not warmup_ok:
            print("âŒ ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ãŒå¤±æ•—ã—ã¦ã„ã¾ã™")
        print("ğŸ”§ docker-compose -f docker-compose-optimized.yml logs ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    
    # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    results_file = Path("test_results_optimization.json")
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print()
    print(f"ğŸ“ è©³ç´°çµæœã¯ {results_file} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
    
    return results

if __name__ == "__main__":
    print("ğŸš€ Starting Super Wav2Lip Optimization Test...")
    print()
    
    # ã‚·ã‚¹ãƒ†ãƒ ãŒèµ·å‹•ã™ã‚‹ã¾ã§å°‘ã—å¾…æ©Ÿ
    print("â³ Waiting for system to fully initialize...")
    time.sleep(5)
    
    try:
        results = compare_old_vs_new_performance()
        
        print()
        print("ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("1. http://localhost:7860 ã§WebUIã«ã‚¢ã‚¯ã‚»ã‚¹")
        print("2. å®Ÿéš›ã«å‹•ç”»ç”Ÿæˆã‚’è©¦ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ä½“æ„Ÿ")
        print("3. åˆå›ã¨2å›ç›®ä»¥é™ã®é€Ÿåº¦å·®ã‚’ç¢ºèª")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Test interrupted by user")
    except Exception as e:
        print(f"\nâŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()