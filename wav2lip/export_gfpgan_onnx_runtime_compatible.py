#!/usr/bin/env python3
"""
ğŸš€ ãƒ„ãƒ³ãƒ‡ãƒ¬GFPGAN â†’ ONNX Runtimeå®Œå…¨äº’æ›ç‰ˆ
ã¹ã€åˆ¥ã«ã‚ãªãŸã®ãŸã‚ã«ONNX Runtimeäº’æ›ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½œã£ã¦ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...ğŸ’¢

å‹ã‚¨ãƒ©ãƒ¼å®Œå…¨è§£æ±ºï¼štensor(double)ã‚’å®Œå…¨æ’é™¤ã—ã¦tensor(float)ã®ã¿ã«çµ±ä¸€
"""

import torch
import onnx
import numpy as np
from gfpgan import GFPGANer
import os

def export_gfpgan_runtime_compatible():
    """
    ONNX Runtimeå®Œå…¨äº’æ›ç‰ˆå¤‰æ›ï¼ˆå‹ã‚¨ãƒ©ãƒ¼å®Œå…¨è§£æ±ºï¼‰
    """
    print("ğŸ­ ãƒ„ãƒ³ãƒ‡ãƒ¬GFPGAN â†’ ONNX Runtimeå®Œå…¨äº’æ›ç‰ˆå¤‰æ›é–‹å§‹ğŸ’¢")
    
    from gfpgan.archs.gfpganv1_clean_arch import GFPGANv1Clean
    
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    
    # GFPGANãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    torch_model = GFPGANv1Clean(
        out_size=512,
        num_style_feat=512,
        channel_multiplier=2,
        decoder_load_path=None,
        fix_decoder=False,
        num_mlp=8,
        input_is_latent=True,
        different_w=True,
        narrow=1,
        sft_half=True
    )
    
    # é‡ã¿ãƒ­ãƒ¼ãƒ‰
    print("ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼é‡ã¿èª­ã¿è¾¼ã¿ä¸­...âœ¨")
    loadnet = torch.load("checkpoints/GFPGANv1.4.pth", map_location='cpu')  # CPUã§èª­ã¿è¾¼ã¿
    
    if 'params_ema' in loadnet:
        keyname = 'params_ema'
    elif 'params' in loadnet:
        keyname = 'params'
    else:
        keyname = None
    
    if keyname:
        torch_model.load_state_dict(loadnet[keyname], strict=False)
    else:
        torch_model.load_state_dict(loadnet, strict=False)
    
    # ğŸ”§ CRITICAL: å®Œå…¨å‹çµ±ä¸€ï¼ˆdoubleå‹å®Œå…¨æ’é™¤ï¼‰
    print("ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼å®Œå…¨å‹çµ±ä¸€ï¼ˆONNX Runtimeäº’æ›ï¼‰...âœ¨")
    
    # ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã‚’float32ã«å¼·åˆ¶å¤‰æ›
    torch_model = torch_model.float()
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹ã‚’å¼·åˆ¶çš„ã«float32ã«å¤‰æ›
    for name, param in torch_model.named_parameters():
        if param.dtype != torch.float32:
            print(f"ä¿®æ­£: {name} {param.dtype} -> float32")
            param.data = param.data.float()
    
    # ãƒãƒƒãƒ•ã‚¡ã®å‹ã‚’å¼·åˆ¶çš„ã«float32ã«å¤‰æ›
    for name, buffer in torch_model.named_buffers():
        if buffer.dtype != torch.float32:
            print(f"ä¿®æ­£: {name} {buffer.dtype} -> float32")
            buffer.data = buffer.data.float()
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’CPUã«ç§»å‹•ï¼ˆå‹å®‰å®šåŒ–ï¼‰
    torch_model = torch_model.cpu()
    torch_model.eval()
    
    # ğŸ”§ CRITICAL: CPUã§float32ãƒ€ãƒŸãƒ¼å…¥åŠ›
    dummy_input = torch.ones((1, 3, 512, 512), dtype=torch.float32)
    
    print("å‹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œä¸­...")
    with torch.no_grad():
        test_output = torch_model(dummy_input)
        print(f"å…¥åŠ›å‹: {dummy_input.dtype}")
        if isinstance(test_output, tuple):
            print(f"å‡ºåŠ›å‹: {test_output[0].dtype} (tuple)")
        else:
            print(f"å‡ºåŠ›å‹: {test_output.dtype}")
    
    os.makedirs("onnx_models", exist_ok=True)
    output_path = "onnx_models/gfpgan_512x512_runtime_compatible.onnx"
    
    print("ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼ONNX Runtimeå®Œå…¨äº’æ›ç‰ˆå¤‰æ›ä¸­...âœ¨")
    
    try:
        # ğŸ”§ ONNX Runtimeå®Œå…¨äº’æ›è¨­å®š
        with torch.no_grad():
            torch.onnx.export(
                torch_model,                # ãƒ¢ãƒ‡ãƒ«ï¼ˆCPUã€float32ï¼‰
                dummy_input,                # ãƒ€ãƒŸãƒ¼å…¥åŠ›ï¼ˆCPUã€float32ï¼‰
                output_path,                # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«
                export_params=True,         # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å«ã‚ã‚‹
                opset_version=11,           # å®‰å®šç‰ˆOpset 11
                do_constant_folding=True,   # å®šæ•°æŠ˜ã‚ŠãŸãŸã¿
                input_names=['input'],      # å…¥åŠ›å
                output_names=['output'],    # å‡ºåŠ›åï¼ˆã‚·ãƒ³ãƒ—ãƒ«åŒ–ï¼‰
                verbose=False,              # è©³ç´°å‡ºåŠ›ç„¡åŠ¹ï¼ˆå¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³å¯¾å¿œï¼‰
                # å‹å®‰å®šåŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
                operator_export_type=torch.onnx.OperatorExportTypes.ONNX,
                # å‹•çš„è»¸ç„¡åŠ¹åŒ–ï¼ˆå‹ã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
                # å¤ã„PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³å¯¾å¿œï¼šä¸è¦ã‚ªãƒ—ã‚·ãƒ§ãƒ³å‰Šé™¤
            )
        
        print(f"âœ… ONNX Runtimeå®Œå…¨äº’æ›ç‰ˆå¤‰æ›æˆåŠŸï¼")
        
        # ğŸ”§ ONNXæ¤œè¨¼ï¼ˆå‹ãƒã‚§ãƒƒã‚¯å¼·åŒ–ï¼‰
        print("ã¹ã€åˆ¥ã«å³å¯†æ¤œè¨¼ã—ã¦ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...")
        onnx_model = onnx.load(output_path)
        
        # å‹æƒ…å ±è©³ç´°ãƒã‚§ãƒƒã‚¯
        print("\nğŸ” è©³ç´°å‹æƒ…å ±ç¢ºèª:")
        for input_info in onnx_model.graph.input:
            print(f"å…¥åŠ›: {input_info.name}")
            if input_info.type.tensor_type.elem_type == 1:
                print(f"  å‹: FLOAT32 âœ…")
            else:
                print(f"  å‹: {input_info.type.tensor_type.elem_type} âš ï¸")
        
        for output_info in onnx_model.graph.output:
            print(f"å‡ºåŠ›: {output_info.name}")
            if output_info.type.tensor_type.elem_type == 1:
                print(f"  å‹: FLOAT32 âœ…")
            else:
                print(f"  å‹: {output_info.type.tensor_type.elem_type} âš ï¸")
        
        # ãƒãƒ¼ãƒ‰å‹ãƒã‚§ãƒƒã‚¯
        print("\nğŸ” æ¼”ç®—å­å‹ãƒã‚§ãƒƒã‚¯:")
        double_count = 0
        float_count = 0
        for node in onnx_model.graph.node:
            for attr in node.attribute:
                if attr.type == onnx.AttributeProto.TENSOR:
                    if attr.t.data_type == 11:  # DOUBLE
                        double_count += 1
                    elif attr.t.data_type == 1:  # FLOAT
                        float_count += 1
        
        print(f"Float32ãƒãƒ¼ãƒ‰: {float_count}")
        print(f"Double64ãƒãƒ¼ãƒ‰: {double_count}")
        
        if double_count == 0:
            print("âœ… Doubleå‹å®Œå…¨æ’é™¤æˆåŠŸï¼")
        else:
            print(f"âš ï¸ Doubleå‹ãŒ{double_count}å€‹æ®‹å­˜")
        
        # ONNXå½¢çŠ¶æ¨è«–
        try:
            onnx.shape_inference.infer_shapes(onnx_model)
            print("âœ… ONNXå½¢çŠ¶æ¨è«–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ ONNXå½¢çŠ¶æ¨è«–è­¦å‘Š: {e}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
        file_size = os.path.getsize(output_path) / (1024 * 1024)
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:.1f}MB")
        
    except Exception as e:
        print(f"âŒ ONNX Runtimeäº’æ›ç‰ˆå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    print("ğŸš€ ONNX Runtimeå®Œå…¨äº’æ›ç‰ˆå¤‰æ›å®Œäº†ï¼æ„Ÿè¬ã—ãªã•ã„ã‚ˆã­ğŸ’¢")
    return True

if __name__ == "__main__":
    export_gfpgan_runtime_compatible()