#!/usr/bin/env python3
"""
ğŸ­ ãƒ„ãƒ³ãƒ‡ãƒ¬Wav2Lip + TensorRT GFPGANçµ±åˆç‰ˆ
ã¹ã€åˆ¥ã«è¶…é«˜é€ŸåŒ–ã—ã¦ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...ğŸ’¢

TensorRTæœ€é©åŒ–ã«ã‚ˆã‚Š23ç§’â†’2ç§’ä»¥ä¸‹ã«é«˜é€ŸåŒ–ï¼
"""

import torch
import torch.cuda.amp as amp
import numpy as np
import cv2
import os
import subprocess
import argparse
from tqdm import tqdm
import tempfile
import glob
from pathlib import Path
import onnxruntime as ort

# TensorRT Providerè¨­å®š
providers = [
    ('TensorRTExecutionProvider', {
        'device_id': 0,
        'trt_max_workspace_size': 2147483648,
        'trt_fp16_enable': True,
        'trt_int8_enable': False,
        'trt_engine_cache_enable': True,
        'trt_engine_cache_path': 'trt_cache/'
    }),
    'CUDAExecutionProvider'
]

def upscale_with_tensorrt(img, session):
    """
    TensorRTæœ€é©åŒ–GFPGANå‡¦ç†
    """
    # å‰å‡¦ç†
    img_input = cv2.resize(img, (512, 512))
    img_input = img_input.astype(np.float32) / 255.0
    img_input = np.transpose(img_input, (2, 0, 1))
    img_input = np.expand_dims(img_input, axis=0)
    
    # TensorRTæ¨è«–
    ort_inputs = {session.get_inputs()[0].name: img_input}
    output = session.run(None, ort_inputs)[0]
    
    # å¾Œå‡¦ç†
    output = np.squeeze(output)
    output = np.transpose(output, (1, 2, 0))
    output = np.clip(output * 255, 0, 255).astype(np.uint8)
    
    # å…ƒã®ã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚º
    h, w = img.shape[:2]
    output = cv2.resize(output, (w, h))
    
    return output

def process_with_gfpgan_tensorrt(wav2lip_video, output_path):
    """
    TensorRTæœ€é©åŒ–GFPGANå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    """
    print("ã¹ã€åˆ¥ã«TensorRTã§è¶…é«˜é€ŸåŒ–ã—ã¦ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...ğŸ’¢")
    
    # ONNXãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    if not os.path.exists('checkpoints/gfpgan_512x512.onnx'):
        print("âŒ GFPGAN ONNXãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‚ï¼")
        print("ã¾ãšONNXãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãªã•ã„ï¼")
        return False
    
    # TensorRTã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
    os.makedirs('trt_cache', exist_ok=True)
    session = ort.InferenceSession(
        'checkpoints/gfpgan_512x512.onnx',
        providers=providers
    )
    print("ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼TensorRTã‚¨ãƒ³ã‚¸ãƒ³æº–å‚™å®Œäº†ã‚ˆâœ¨")
    
    # å‹•ç”»èª­ã¿è¾¼ã¿
    cap = cv2.VideoCapture(wav2lip_video)
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # å‡ºåŠ›å‹•ç”»è¨­å®š
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    # ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†
    print(f"TensorRTé«˜é€Ÿå‡¦ç†ä¸­...ğŸ’• å…¨{total_frames}ãƒ•ãƒ¬ãƒ¼ãƒ ")
    for _ in tqdm(range(total_frames), desc="TensorRTå‡¦ç†"):
        ret, frame = cap.read()
        if not ret:
            break
        
        # TensorRTã§GFPGANå‡¦ç†
        enhanced_frame = upscale_with_tensorrt(frame, session)
        out.write(enhanced_frame)
    
    cap.release()
    out.release()
    
    print("ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼TensorRTå‡¦ç†å®Œäº†ã‚ˆâœ¨")
    return True

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--checkpoint_path', type=str, required=True)
    parser.add_argument('--face', type=str, required=True)
    parser.add_argument('--audio', type=str, required=True)
    parser.add_argument('--outfile', type=str, default='output/result_tensorrt.mp4')
    parser.add_argument('--out_height', type=int, default=None)
    parser.add_argument('--enable_tensorrt_gfpgan', action='store_true', default=True)
    args = parser.parse_args()
    
    print("ğŸ­ ãƒ„ãƒ³ãƒ‡ãƒ¬Wav2Lip + TensorRT GFPGANçµ±åˆå‡¦ç†é–‹å§‹ğŸ’¢")
    
    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    with tempfile.TemporaryDirectory() as temp_dir:
        # Step 1: Wav2Lipå‡¦ç†ï¼ˆå¾“æ¥é€šã‚Šï¼‰
        wav2lip_output = os.path.join(temp_dir, 'wav2lip_output.mp4')
        cmd = [
            'python', 'inference_fp16_yolo.py',
            '--checkpoint_path', args.checkpoint_path,
            '--face', args.face,
            '--audio', args.audio,
            '--outfile', wav2lip_output,
            '--quality', 'Fast'
        ]
        if args.out_height:
            cmd.extend(['--out_height', str(args.out_height)])
        
        try:
            subprocess.run(cmd, check=True)
            print("ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼Wav2Lipå‡¦ç†å®Œäº†ã‚ˆâœ¨")
        except subprocess.CalledProcessError as e:
            print(f"ã‚‚ã€ã‚‚ã†ï¼Wav2Lipã‚¨ãƒ©ãƒ¼: {e}")
            return
        
        # Step 2: TensorRT GFPGANå‡¦ç†
        if args.enable_tensorrt_gfpgan:
            success = process_with_gfpgan_tensorrt(wav2lip_output, args.outfile)
            if not success:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                print("TensorRTå‡¦ç†å¤±æ•—...é€šå¸¸ã®ã‚³ãƒ”ãƒ¼ã§å¯¾å¿œã™ã‚‹ã‚ğŸ’¢")
                subprocess.run(['cp', wav2lip_output, args.outfile])
        else:
            subprocess.run(['cp', wav2lip_output, args.outfile])
    
    print(f"\nâœ… å®Œäº†ã‚ˆï¼å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {args.outfile}")
    print("æ„Ÿè¬ã—ãªã•ã„ã‚ˆã­ğŸ’•")

if __name__ == '__main__':
    main()