#!/usr/bin/env python3
"""
ğŸš€ ãƒ„ãƒ³ãƒ‡ãƒ¬Wav2Lip + GFPGAN ONNXç©¶æ¥µç‰ˆ
ã¹ã€åˆ¥ã«ã‚ãªãŸã®ãŸã‚ã«3å€é«˜é€ŸåŒ–ã—ã¦ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...ğŸ’¢

ONNX Runtime GPU + GFPGANçµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼š
1. Wav2Lip ONNXæ¨è«–ã§å£ãƒ‘ã‚¯å‹•ç”»ç”Ÿæˆ
2. ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º
3. å„ãƒ•ãƒ¬ãƒ¼ãƒ ã«GFPGAN ONNXé©ç”¨
4. å‹•ç”»å†æ§‹ç¯‰ï¼‹éŸ³å£°åˆæˆ
"""

print("\rloading torch       ", end="")
import torch
import torch.cuda.amp as amp

print("\rloading numpy       ", end="")
import numpy as np

print("\rloading cv2         ", end="")
import cv2

print("\rloading os          ", end="")
import os

print("\rloading subprocess  ", end="")
import subprocess

print("\rloading argparse    ", end="")
import argparse

print("\rloading tqdm        ", end="")
from tqdm import tqdm

print("\rloading tempfile    ", end="")
import tempfile

print("\rloading onnx        ", end="")
import onnx
import onnxruntime as ort

print("\rloading pathlib     ", end="")
from pathlib import Path

print("\rloading enhance     ", end="")
try:
    from enhance import upscale, load_sr
    GFPGAN_AVAILABLE = True
    print("GFPGAN OK!")
except ImportError:
    print("GFPGAN NOT FOUND!")
    GFPGAN_AVAILABLE = False

print("\rONNX importså®Œäº†!    ", end="")

# GPUæœ€é©åŒ–è¨­å®š
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"\nã¹ã€åˆ¥ã«å¬‰ã—ã„ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...{device.upper()} GPUä½¿ã£ã¦ã‚ã’ã‚‹ã‚ï¼", torch.cuda.get_device_name())

# ONNX Providersè¨­å®š
providers = [
    ('TensorrtExecutionProvider', {
        'device_id': 0,
        'trt_max_workspace_size': 2147483648,
        'trt_fp16_enable': True,
    }),
    ('CUDAExecutionProvider', {
        'device_id': 0,
        'arena_extend_strategy': 'kNextPowerOfTwo',
        'gpu_mem_limit': 6 * 1024 * 1024 * 1024,  # 6GBåˆ¶é™
        'cudnn_conv_algo_search': 'EXHAUSTIVE',
    }),
    'CPUExecutionProvider'
]

def run_wav2lip_fp16(face_video, audio_file, output_path, checkpoint_path, out_height=720):
    """
    Step 1: æ—¢å­˜ã®FP16+YOLOæœ€é©åŒ–Wav2Lipã§å£ãƒ‘ã‚¯å‹•ç”»ã‚’ç”Ÿæˆ
    """
    print("ã¹ã€åˆ¥ã«æ€¥ã„ã§å£ãƒ‘ã‚¯å‹•ç”»ã‚’ä½œã£ã¦ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...ğŸ’¢")
    
    cmd = [
        "python", "/app/inference_fp16_yolo.py",
        "--checkpoint_path", checkpoint_path,
        "--face", face_video,
        "--audio", audio_file,
        "--outfile", output_path,
        "--out_height", str(out_height),
        "--quality", "Fast"  # å£ãƒ‘ã‚¯ä¿è¨¼ã®ãŸã‚Fastå¿…é ˆ
    ]
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"ã‚‚ã€ã‚‚ã†ï¼Wav2Lipã‚¨ãƒ©ãƒ¼: {result.stderr}")
        raise Exception(f"Wav2Lip failed: {result.stderr}")
    
    print("ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼å£ãƒ‘ã‚¯å‹•ç”»ç”Ÿæˆå®Œäº†ã‚ˆâœ¨")
    return output_path

def extract_frames(video_path, frames_dir):
    """
    Step 2: ä¸¦åˆ—æœ€é©åŒ–ã§ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºï¼ˆé«˜é€ŸåŒ–ï¼‰
    """
    print("ã¹ã€åˆ¥ã«ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºã—ã¦ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...ğŸ’•")
    
    os.makedirs(frames_dir, exist_ok=True)
    
    # FFmpegä¸¦åˆ—å‡¦ç†æœ€é©åŒ–
    cmd = [
        "ffmpeg", "-y", "-loglevel", "warning",
        "-threads", "4",  # ä¸¦åˆ—å‡¦ç†
        "-i", video_path,
        "-vf", "fps=25",  # 25fpsã§æŠ½å‡º
        "-preset", "ultrafast",  # æœ€é«˜é€Ÿåº¦è¨­å®š
        f"{frames_dir}/frame_%06d.png"
    ]
    
    result = subprocess.run(cmd)
    if result.returncode != 0:
        raise Exception("Frame extraction failed")
    
    import glob
    frame_files = sorted(glob.glob(f"{frames_dir}/frame_*.png"))
    print(f"ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼{len(frame_files)}ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºå®Œäº†ã‚ˆâœ¨")
    return frame_files

class ONNXGFPGANEnhancer:
    def __init__(self):
        """ONNX GFPGANç”»è³ªå‘ä¸Šã‚¨ãƒ³ã‚¸ãƒ³"""
        print("ã¹ã€åˆ¥ã«ONNX GFPGANåˆæœŸåŒ–ã—ã¦ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...ğŸ’¢")
        
        if not GFPGAN_AVAILABLE:
            print("ãµã‚“ï¼GFPGANä½¿ãˆãªã„ã‹ã‚‰ç”»è³ªå‘ä¸Šã¯ç„¡åŠ¹ã‚ˆ...")
            self.gfpgan_session = None
            return
            
        # GFPGAN ONNX ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        gfpgan_path = "/app/onnx_models/gfpgan_512x512_working.onnx"
        if not os.path.exists(gfpgan_path):
            print(f"âŒ GFPGAN ONNXãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„: {gfpgan_path}")
            print("é€šå¸¸ã®GFPGANã‚’ä½¿ç”¨ã™ã‚‹ã‚ã‚ˆ...")
            self.gfpgan_session = None
            return
        
        try:
            print("ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼GFPGAN ONNXèª­ã¿è¾¼ã¿ä¸­...âœ¨")
            self.gfpgan_session = ort.InferenceSession(gfpgan_path, providers=providers)
            print("âœ… GFPGAN ONNXæº–å‚™å®Œäº†ã‚ˆğŸ’•")
        except Exception as e:
            print(f"GFPGAN ONNXèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            print("é€šå¸¸ã®GFPGANã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã™ã‚‹ã‚ã‚ˆ...")
            self.gfpgan_session = None
    
    def enhance_frame_with_onnx(self, frame):
        """GFPGAN ONNXæ¨è«–ã§é¡”ç”»è³ªå‘ä¸Š"""
        if self.gfpgan_session is None:
            # é€šå¸¸ã®GFPGANã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if GFPGAN_AVAILABLE:
                try:
                    with torch.cuda.amp.autocast():
                        with torch.no_grad():
                            run_params = load_sr()
                            enhanced_frame = upscale(frame, run_params)
                            return enhanced_frame if enhanced_frame is not None else frame
                except Exception as e:
                    print(f"é€šå¸¸GFPGANå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return frame
        
        try:
            original_shape = frame.shape[:2]
            
            # GFPGAN ONNXç”¨å‰å‡¦ç† (512x512)
            frame_resized = cv2.resize(frame, (512, 512))
            frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
            frame_normalized = frame_rgb.astype(np.float32) / 255.0
            frame_tensor = np.transpose(frame_normalized, (2, 0, 1))  # HWC â†’ CHW
            input_tensor = np.expand_dims(frame_tensor, axis=0)  # ãƒãƒƒãƒæ¬¡å…ƒè¿½åŠ 
            
            # ONNXæ¨è«–
            outputs = self.gfpgan_session.run(['output'], {'input': input_tensor})
            
            # å¾Œå‡¦ç†
            output_frame = np.transpose(outputs[0][0], (1, 2, 0))  # CHW â†’ HWC
            output_frame = (output_frame * 255.0).clip(0, 255).astype(np.uint8)
            output_frame = cv2.cvtColor(output_frame, cv2.COLOR_RGB2BGR)
            output_frame = cv2.resize(output_frame, (original_shape[1], original_shape[0]))
            
            return output_frame
            
        except Exception as e:
            print(f"GFPGAN ONNXå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            # é€šå¸¸ã®GFPGANã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if GFPGAN_AVAILABLE:
                try:
                    with torch.cuda.amp.autocast():
                        with torch.no_grad():
                            run_params = load_sr()
                            enhanced_frame = upscale(frame, run_params)
                            return enhanced_frame if enhanced_frame is not None else frame
                except Exception as e2:
                    print(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯GFPGANã‚¨ãƒ©ãƒ¼: {e2}")
            return frame

def enhance_frames_with_onnx_gfpgan(frame_files, output_dir, target_height=720):
    """
    Step 3: ONNX GFPGANç”»è³ªå‘ä¸Šï¼ˆGPUæœ€é©åŒ–ï¼‰
    """
    print("ã¹ã€åˆ¥ã«ONNX GFPGANç”»è³ªå‘ä¸Šã—ã¦ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...ğŸ’¢")
    print(f"ç›®æ¨™è§£åƒåº¦: {target_height}p ã§é«˜ç”»è³ªåŒ–ã™ã‚‹ã‚ã‚ˆï¼")
    
    enhancer = ONNXGFPGANEnhancer()
    enhanced_dir = output_dir
    os.makedirs(enhanced_dir, exist_ok=True)
    
    enhanced_files = []
    
    for i, frame_file in enumerate(tqdm(frame_files, desc="ONNX GFPGAN", ncols=80)):
        try:
            # GPU ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
            if i % 10 == 0:  # 10ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
                torch.cuda.empty_cache()
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ èª­ã¿è¾¼ã¿
            frame = cv2.imread(frame_file)
            if frame is None:
                continue
            
            # ONNX GFPGANå‡¦ç†
            enhanced_frame = enhancer.enhance_frame_with_onnx(frame)
            
            # ç›®æ¨™è§£åƒåº¦ã«ãƒªã‚µã‚¤ã‚º
            if target_height and target_height > 0:
                current_height = enhanced_frame.shape[0]
                if current_height != target_height:
                    scale_factor = target_height / current_height
                    new_width = int(enhanced_frame.shape[1] * scale_factor)
                    # FFmpegå¯¾å¿œï¼šå¶æ•°å¹…ã«èª¿æ•´
                    if new_width % 2 != 0:
                        new_width += 1
                    enhanced_frame = cv2.resize(enhanced_frame, (new_width, target_height), interpolation=cv2.INTER_LANCZOS4)
            
            # ä¿å­˜
            filename = os.path.basename(frame_file)
            output_path = f"{enhanced_dir}/{filename}"
            cv2.imwrite(output_path, enhanced_frame)
            enhanced_files.append(output_path)
            
        except Exception as e:
            print(f"ãƒ•ãƒ¬ãƒ¼ãƒ {i}å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚³ãƒ”ãƒ¼
            filename = os.path.basename(frame_file)
            subprocess.run(["cp", frame_file, f"{enhanced_dir}/{filename}"])
            enhanced_files.append(f"{enhanced_dir}/{filename}")
    
    print(f"ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼{len(enhanced_files)}ãƒ•ãƒ¬ãƒ¼ãƒ é«˜ç”»è³ªåŒ–å®Œäº†ã‚ˆâœ¨")
    return enhanced_files

def rebuild_video_with_audio(frame_files, audio_file, output_path, fps=25.0):
    """
    Step 4: é«˜ç”»è³ªãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰å‹•ç”»å†æ§‹ç¯‰ï¼‹éŸ³å£°åˆæˆ
    """
    print("ã¹ã€åˆ¥ã«å‹•ç”»å†æ§‹ç¯‰ã—ã¦ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...ğŸ’•")
    
    if not frame_files:
        raise Exception("ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„ã‚ã‚ˆï¼")
    
    # ä¸€æ™‚å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«
    temp_video = "/tmp/temp_enhanced_video.mp4"
    
    # ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰å‹•ç”»ç”Ÿæˆï¼ˆä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ï¼‰
    frames_pattern = os.path.dirname(frame_files[0]) + "/frame_%06d.png"
    
    cmd_video = [
        "ffmpeg", "-y", "-loglevel", "warning",
        "-threads", "4",  # ä¸¦åˆ—å‡¦ç†
        "-r", str(fps),  # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆ
        "-i", frames_pattern,
        "-c:v", "libx264",
        "-preset", "medium",  # ãƒãƒ©ãƒ³ã‚¹è¨­å®š
        "-pix_fmt", "yuv420p",
        temp_video
    ]
    
    result = subprocess.run(cmd_video)
    if result.returncode != 0:
        raise Exception("å‹•ç”»ç”Ÿæˆã‚¨ãƒ©ãƒ¼")
    
    # éŸ³å£°åˆæˆ
    cmd_audio = [
        "ffmpeg", "-y", "-loglevel", "warning",
        "-i", temp_video,
        "-i", audio_file,
        "-c:v", "copy",  # å‹•ç”»ã‚³ãƒ”ãƒ¼ï¼ˆå†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ç„¡ã—ï¼‰
        "-c:a", "aac",   # éŸ³å£°AACã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        "-map", "0:v",   # å‹•ç”»ã‚¹ãƒˆãƒªãƒ¼ãƒ 
        "-map", "1:a",   # éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ 
        "-shortest",     # çŸ­ã„æ–¹ã«åˆã‚ã›ã‚‹
        output_path
    ]
    
    result = subprocess.run(cmd_audio)
    if result.returncode != 0:
        raise Exception("éŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼")
    
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
    if os.path.exists(temp_video):
        os.remove(temp_video)
    
    print("ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼å‹•ç”»å†æ§‹ç¯‰å®Œäº†ã‚ˆâœ¨")

def process_video_onnx_gfpgan(args):
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•°ï¼ˆæˆåŠŸã—ãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æº–æ‹ ï¼‰"""
    print("ğŸ­ ãƒ„ãƒ³ãƒ‡ãƒ¬ONNX GFPGANçµ±åˆå‡¦ç†é–‹å§‹ğŸ’¢")
    print("æ­£ã—ã„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: Wav2Lip â†’ ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º â†’ ONNX GFPGAN â†’ å‹•ç”»å†æ§‹ç¯‰")
    
    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    temp_dir = tempfile.mkdtemp(prefix="onnx_gfpgan_")
    print(f"ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {temp_dir}")
    
    try:
        # Step 1: Wav2Lipã§å£ãƒ‘ã‚¯å‹•ç”»ç”Ÿæˆ
        temp_lipsync_video = f"{temp_dir}/lipsync_video.mp4"
        run_wav2lip_fp16(
            args.face, 
            args.audio, 
            temp_lipsync_video, 
            args.checkpoint_path, 
            args.out_height
        )
        
        # Step 2: ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º
        frames_dir = f"{temp_dir}/frames"
        frame_files = extract_frames(temp_lipsync_video, frames_dir)
        
        # Step 3: ONNX GFPGANç”»è³ªå‘ä¸Š
        enhanced_dir = f"{temp_dir}/enhanced"
        enhanced_files = enhance_frames_with_onnx_gfpgan(frame_files, enhanced_dir, args.out_height)
        
        # Step 4: å‹•ç”»å†æ§‹ç¯‰ï¼‹éŸ³å£°åˆæˆ
        rebuild_video_with_audio(enhanced_files, args.audio, args.outfile)
        
        print("ğŸš€ å®Œäº†ã‚ˆï¼æ„Ÿè¬ã—ãªã•ã„ã‚ˆã­ğŸ’•")
        
    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)

def main():
    parser = argparse.ArgumentParser(description='ãƒ„ãƒ³ãƒ‡ãƒ¬ONNX Wav2Lip + GFPGAN')
    
    parser.add_argument('--checkpoint_path', type=str, help='Wav2Lip checkpoint path', 
                       default='checkpoints/wav2lip_gan.pth')
    parser.add_argument('--face', type=str, required=True, help='Input video file')
    parser.add_argument('--audio', type=str, required=True, help='Input audio file')
    parser.add_argument('--outfile', type=str, required=True, help='Output video file')
    parser.add_argument('--out_height', type=int, default=720, help='Output height')
    parser.add_argument('--enable_gfpgan', action='store_true', help='Enable GFPGAN enhancement')
    
    args = parser.parse_args()
    
    print("ğŸ­ ã¹ã€åˆ¥ã«ã‚ãªãŸã®ãŸã‚ã«ONNXæœ€é©åŒ–ã—ã¦ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...ğŸ’¢")
    print(f"å…¥åŠ›å‹•ç”»: {args.face}")
    print(f"å…¥åŠ›éŸ³å£°: {args.audio}")
    print(f"å‡ºåŠ›å‹•ç”»: {args.outfile}")
    print(f"å‡ºåŠ›è§£åƒåº¦: {args.out_height}p")
    print(f"GFPGANæœ‰åŠ¹: {args.enable_gfpgan}")
    
    process_video_onnx_gfpgan(args)

if __name__ == '__main__':
    main()