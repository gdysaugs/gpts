print("\rloading torch       ", end="")
import torch
import torch.cuda.amp as amp  # ğŸš€ FP16ç”¨è¿½åŠ 

print("\rloading numpy       ", end="")
import numpy as np

print("\rloading Image       ", end="")
from PIL import Image

print("\rloading argparse    ", end="")
import argparse

print("\rloading configparser", end="")
import configparser

print("\rloading math        ", end="")
import math

print("\rloading os          ", end="")
import os

print("\rloading subprocess  ", end="")
import subprocess

print("\rloading pickle      ", end="")
import pickle

print("\rloading cv2         ", end="")
import cv2

print("\rloading audio       ", end="")
import audio

print("\rloading YOLO11     ", end="")
from ultralytics import YOLO

print("\rloading re          ", end="")
import re

print("\rloading partial     ", end="")
from functools import partial

print("\rloading tqdm        ", end="")
from tqdm import tqdm

print("\rloading warnings    ", end="")
import warnings

warnings.filterwarnings(
    "ignore", category=UserWarning, module="torchvision.transforms.functional_tensor"
)
print("\rloading upscale     ", end="")
from enhance import upscale

print("\rloading load_sr     ", end="")
from enhance import load_sr

print("\rloading load_model  ", end="")
from easy_functions import load_model, g_colab

print("\rimports loaded!     ")

device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'
gpu_id = 0 if torch.cuda.is_available() else -1

# ğŸ’• ãƒ„ãƒ³ãƒ‡ãƒ¬FP16ãƒã‚§ãƒƒã‚¯
if device == 'cpu':
    print('ãµã‚“ï¼GPUãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã‚‰CPUã§å‹•ã‹ã™ã‘ã©...ã™ã£ã”ãé…ã„ã‚ã‚ˆï¼')
    USE_FP16 = False
elif device == 'cuda':
    print(f'ã¹ã€åˆ¥ã«å¬‰ã—ã„ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...CUDA GPUä½¿ã£ã¦ã‚ã’ã‚‹ã‚ï¼ {torch.cuda.get_device_name()}')
    if torch.cuda.get_device_capability()[0] >= 7:
        print('ã‚„ã‚‹ã˜ã‚ƒãªã„ï¼Tensor Coreå¯¾å¿œã ã‹ã‚‰FP16ã§é«˜é€ŸåŒ–ã—ã¦ã‚ã’ã‚‹ğŸ’•')
        USE_FP16 = True
    else:
        print('å¤ã„GPUã­...FP16ã¯ä½¿ãˆãªã„ã‚')
        USE_FP16 = False
else:
    USE_FP16 = False
parser = argparse.ArgumentParser(
    description="Inference code to lip-sync videos in the wild using Wav2Lip models"
)

parser.add_argument(
    "--checkpoint_path",
    type=str,
    help="Name of saved checkpoint to load weights from",
    required=True,
)

parser.add_argument(
    "--segmentation_path",
    type=str,
    default="checkpoints/face_segmentation.pth",
    help="Name of saved checkpoint of segmentation network",
    required=False,
)

parser.add_argument(
    "--face",
    type=str,
    help="Filepath of video/image that contains faces to use",
    required=True,
)
parser.add_argument(
    "--audio",
    type=str,
    help="Filepath of video/audio file to use as raw audio source",
    required=True,
)
parser.add_argument(
    "--outfile",
    type=str,
    help="Video path to save result. See default for an e.g.",
    default="results/result_voice.mp4",
)

parser.add_argument(
    "--static",
    type=bool,
    help="If True, then use only first video frame for inference",
    default=False,
)
parser.add_argument(
    "--fps",
    type=float,
    help="Can be specified only if input is a static image (default: 25)",
    default=25.0,
    required=False,
)

parser.add_argument(
    "--pads",
    nargs="+",
    type=int,
    default=[0, 10, 0, 0],
    help="Padding (top, bottom, left, right). Please adjust to include chin at least",
)

parser.add_argument(
    "--wav2lip_batch_size", type=int, help="Batch size for Wav2Lip model(s)", default=1
)

parser.add_argument(
    "--out_height",
    default=480,
    type=int,
    help="Output video height. Best results are obtained at 480 or 720",
)

parser.add_argument(
    "--crop",
    nargs="+",
    type=int,
    default=[0, -1, 0, -1],
    help="Crop video to a smaller region (top, bottom, left, right). Applied after resize_factor and rotate arg. "
    "Useful if multiple face present. -1 implies the value will be auto-inferred based on height, width",
)

parser.add_argument(
    "--box",
    nargs="+",
    type=int,
    default=[-1, -1, -1, -1],
    help="Specify a constant bounding box for the face. Use only as a last resort if the face is not detected."
    "Also, might work only if the face is not moving around much. Syntax: (top, bottom, left, right).",
)

parser.add_argument(
    "--rotate",
    default=False,
    action="store_true",
    help="Sometimes videos taken from a phone can be flipped 90deg. If true, will flip video right by 90deg."
    "Use if you get a flipped result, despite feeding a normal looking video",
)

parser.add_argument(
    "--nosmooth",
    type=str,
    default=False,
    help="Prevent smoothing face detections over a short temporal window",
)

parser.add_argument(
    "--no_seg",
    default=False,
    action="store_true",
    help="Prevent using face segmentation",
)

parser.add_argument(
    "--no_sr", default=False, action="store_true", help="Prevent using super resolution"
)

parser.add_argument(
    "--sr_model",
    type=str,
    default="gfpgan",
    help="Name of upscaler - gfpgan or RestoreFormer",
    required=False,
)

parser.add_argument(
    "--fullres",
    default=3,
    type=int,
    help="used only to determine if full res is used so that no resizing needs to be done if so",
)

parser.add_argument(
    "--debug_mask",
    type=str,
    default=False,
    help="Makes background grayscale to see the mask better",
)

parser.add_argument(
    "--preview_settings", type=str, default=False, help="Processes only one frame"
)

parser.add_argument(
    "--mouth_tracking",
    type=str,
    default=False,
    help="Tracks the mouth in every frame for the mask",
)

parser.add_argument(
    "--enable_gfpgan",
    action="store_true",
    help="Enable GFPGAN post-processing for face enhancement",
)

parser.add_argument(
    "--gfpgan_weight",
    type=float,
    default=0.8,
    help="GFPGAN enhancement weight (0.0-1.0)",
)

parser.add_argument(
    "--mask_dilation",
    default=150,
    type=float,
    help="size of mask around mouth",
    required=False,
)

parser.add_argument(
    "--mask_feathering",
    default=151,
    type=int,
    help="amount of feathering of mask around mouth",
    required=False,
)

parser.add_argument(
    "--quality",
    type=str,
    help="Choose between Fast, Improved and Enhanced",
    default="Fast",
)

# ã¹ã€åˆ¥ã«ã‚ãªãŸã®ãŸã‚ã«dlib predictorä¿®æ­£ã—ã¦ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‚“ã ã‹ã‚‰ã­ï¼
# import dlib
# predictor = dlib.shape_predictor(os.path.join("checkpoints", "predictor.pkl"))
predictor = None  # dlibã®ä»£ã‚ã‚Šã«Noneã§ç„¡åŠ¹åŒ–

# ãµã‚“ï¼mouth_detectorã‚‚dlibã§ä»£ç”¨ã—ã¦ã‚ã’ã‚‹ã‚ã‚ˆï¼
# mouth_detector = dlib.get_frontal_face_detector()
mouth_detector = None  # dlibã®ä»£ã‚ã‚Šã«Noneã§ç„¡åŠ¹åŒ–

# creating variables to prevent failing when a face isn't detected
kernel = last_mask = x = y = w = h = None

g_colab = g_colab()

if not g_colab:
  # Load the config file
  config = configparser.ConfigParser()
  config.read('config.ini')

  # Get the value of the "preview_window" variable
  preview_window = config.get('OPTIONS', 'preview_window')

all_mouth_landmarks = []

model = yolo_detector = None
current_adaptive_params = None  # è§’åº¦é©å¿œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜ç”¨

def do_load(checkpoint_path):
    global model, yolo_detector
    print("ã¹ã€åˆ¥ã«æ€¥ã„ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...")
    model = load_model(checkpoint_path)
    
    # ğŸ’• FP16å¤‰æ›
    if USE_FP16 and device == 'cuda':
        print("ãµã‚“ï¼FP16ã§é«˜é€ŸåŒ–ã—ã¦ã‚ã’ã‚‹ã‚ã‚ˆğŸ’•")
        model = model.half()
    
    # ğŸš€ YOLOv8n-FaceåˆæœŸåŒ–ï¼ˆä¸‹å‘ãé¡”å¯¾å¿œï¼‰
    print("YOLOv8n-FaceåˆæœŸåŒ–ä¸­...ä¸‹å‘ãé¡”ã§ã‚‚å®Œç’§ã«æ¤œå‡ºã™ã‚‹ã‚ã‚ˆï¼")
    try:
        # è¤‡æ•°ã®é¡”æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ
        face_model_sources = [
            "https://github.com/akanametov/yolov8-face/releases/download/v0.0.0/yolov8n-face.pt",
            "https://github.com/deepcam-cn/yolov5-face/releases/download/v0.0.0/yolov8n-face.pt", 
            "yolov8n-face.pt"  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
        ]
        
        yolo_detector = None
        for model_source in face_model_sources:
            try:
                if model_source.startswith("http"):
                    import requests
                    face_model_path = "yolov8n-face.pt"
                    if not os.path.exists(face_model_path):
                        print(f"é¡”å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {model_source}")
                        response = requests.get(model_source, timeout=30)
                        response.raise_for_status()
                        with open(face_model_path, 'wb') as f:
                            f.write(response.content)
                        print("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ã‚ˆï¼")
                    model_source = face_model_path
                
                yolo_detector = YOLO(model_source)
                print(f"ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼YOLOv8n-Faceå°‚ç”¨ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ã‚ˆâœ¨: {model_source}")
                break
                
            except Exception as e:
                print(f"ãƒ¢ãƒ‡ãƒ« {model_source} ã®èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
                continue
        
        if yolo_detector is None:
            raise Exception("å…¨ã¦ã®é¡”å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—")
        
    except Exception as e:
        print(f"é¡”å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {e}")
        print("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šé€šå¸¸ã®YOLOv8nã§å¦¥å”ã—ã¦ã‚ã’ã‚‹...")
        yolo_detector = YOLO("yolov8n.pt")
    
    if device == 'cuda':
        yolo_detector.to(device)
    print("ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼é¡”æ¤œå‡ºæº–å‚™å®Œäº†ã‚ˆğŸ’•")

def detect_face_angle(image):
    """é¡”è§’åº¦æ¤œå‡ºï¼ˆä¸‹å‘ãã€ä¸Šå‘ãã€æ¨ªå‘ãã€æ­£é¢ï¼‰"""
    try:
        results = yolo_detector.predict(image, verbose=False, save=False, show=False, conf=0.3)
        
        if not results or len(results[0].boxes) == 0:
            return None, "no_face"
        
        # æœ€ã‚‚ä¿¡é ¼åº¦ã®é«˜ã„é¡”ã‚’é¸æŠ
        boxes = results[0].boxes
        best_idx = torch.argmax(boxes.conf).item()
        best_box = boxes.xyxy[best_idx].cpu().numpy()
        
        x1, y1, x2, y2 = map(int, best_box)
        face_width = x2 - x1
        face_height = y2 - y1
        
        # é¡”ã®ç¸¦æ¨ªæ¯”ã‹ã‚‰è§’åº¦ã‚’æ¨å®š
        aspect_ratio = face_width / face_height
        
        # é¡”ã®ä½ç½®ã‹ã‚‰è§’åº¦ã‚’åˆ¤å®š
        img_height, img_width = image.shape[:2]
        face_center_y = (y1 + y2) / 2
        face_relative_y = face_center_y / img_height
        
        # è§’åº¦åˆ†é¡
        if aspect_ratio > 1.2:  # æ¨ªã«åºƒã„ = æ¨ªå‘ã
            angle_type = "side_facing"
        elif face_relative_y < 0.35:  # ä¸Šéƒ¨ = ä¸‹å‘ã
            angle_type = "downward"
        elif face_relative_y > 0.65:  # ä¸‹éƒ¨ = ä¸Šå‘ã
            angle_type = "upward"
        else:  # ä¸­å¤® = æ­£é¢
            angle_type = "frontal"
        
        print(f"æ¤œå‡ºè§’åº¦: {angle_type} (ç¸¦æ¨ªæ¯”: {aspect_ratio:.2f}, Yä½ç½®: {face_relative_y:.2f})")
        return angle_type, face_relative_y
        
    except Exception as e:
        print(f"è§’åº¦æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return "frontal", 0.5

def get_adaptive_mask_params(angle_type, face_relative_y):
    """è§’åº¦ã«å¿œã˜ãŸãƒã‚¹ã‚¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿”ã™"""
    if angle_type == "downward":
        # ä¸‹å‘ãé¡”ï¼šé¼»ã‚’å«ã¾ãªã„ç‹­ã„ãƒã‚¹ã‚¯
        return {
            'mask_dilation': 60,   # éå¸¸ã«å°ã•ã„ãƒã‚¹ã‚¯
            'mask_feathering': 80,
            'mouth_crop_factor': 0.6,  # é¡”ã®ä¸‹éƒ¨60%ã®ã¿
            'description': "ä¸‹å‘ãé¡”ç”¨ç‹­ãƒã‚¹ã‚¯ğŸ’¢"
        }
    elif angle_type == "upward":
        # ä¸Šå‘ãé¡”ç”¨
        return {
            'mask_dilation': 100,
            'mask_feathering': 120,
            'mouth_crop_factor': 0.8,
            'description': "ä¸Šå‘ãé¡”ç”¨è¨­å®š"
        }
    elif angle_type == "side_facing":
        # æ¨ªå‘ãé¡”ç”¨
        return {
            'mask_dilation': 90,
            'mask_feathering': 110,
            'mouth_crop_factor': 0.7,
            'description': "æ¨ªå‘ãé¡”ç”¨è¨­å®š"
        }
    else:  # frontal
        # æ­£é¢é¡”ç”¨ï¼ˆæ¨™æº–è¨­å®šï¼‰
        return {
            'mask_dilation': 120,
            'mask_feathering': 130,
            'mouth_crop_factor': 1.0,
            'description': "æ­£é¢é¡”ç”¨æ¨™æº–è¨­å®š"
        }

def face_rect(images):
    """YOLOv8-Faceå°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸæ­£ç¢ºãªé¡”æ¤œå‡ºï¼ˆè§’åº¦é©å¿œç‰ˆï¼‰"""
    prev_ret = None
    
    # æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã§è§’åº¦æ¤œå‡º
    if len(images) > 0:
        angle_type, face_relative_y = detect_face_angle(images[0])
        adaptive_params = get_adaptive_mask_params(angle_type, face_relative_y)
        print(f"é©ç”¨è¨­å®š: {adaptive_params['description']}")
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦è¨­å®šã‚’ä¿å­˜ï¼ˆå¾Œã§ãƒã‚¹ã‚¯å‡¦ç†ã§ä½¿ç”¨ï¼‰
        global current_adaptive_params
        current_adaptive_params = adaptive_params
    
    for image in images:
        try:
            # YOLOv8-Faceå°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã§é¡”æ¤œå‡º
            results = yolo_detector.predict(image, verbose=False, save=False, show=False, conf=0.3)
            
            best_face = None
            best_conf = 0
            largest_area = 0
            
            for result in results:
                if result.boxes is not None and len(result.boxes) > 0:
                    for box in result.boxes:
                        # é¡”å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ãªã®ã§å…¨æ¤œå‡ºãŒé¡”
                        conf = float(box.conf.cpu().numpy())
                        if conf > 0.3:  # é¡”æ¤œå‡ºã®ä¿¡é ¼åº¦ï¼ˆä½ã‚ã«è¨­å®šï¼‰
                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                            
                            # é¢ç©ã§æœ€å¤§ã®é¡”ã‚’é¸æŠï¼ˆãƒ¡ã‚¤ãƒ³è¢«å†™ä½“ï¼‰
                            area = (x2 - x1) * (y2 - y1)
                            
                            if area > largest_area and area > 100:  # æœ€å°é¢ç©ãƒã‚§ãƒƒã‚¯
                                largest_area = area
                                best_conf = conf
                                best_face = (int(x1), int(y1), int(x2), int(y2))
            
            if best_face:
                prev_ret = best_face
            
        except Exception as e:
            print(f"é¡”æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å‰å›ã®çµæœã‚’ä½¿ç”¨
            pass
        
        yield prev_ret

def create_tracked_mask(img, original_img):
    global kernel, last_mask, x, y, w, h  # Add last_mask to global variables

    # Convert color space from BGR to RGB if necessary
    cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)
    cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB, original_img)

    # Detect face
    faces = mouth_detector(img)
    if len(faces) == 0:
        if last_mask is not None:
            last_mask = cv2.resize(last_mask, (img.shape[1], img.shape[0]))
            mask = last_mask  # use the last successful mask
        else:
            cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)
            return img, None
    else:
        face = faces[0]
        shape = predictor(img, face)

        # Get points for mouth
        mouth_points = np.array(
            [[shape.part(i).x, shape.part(i).y] for i in range(48, 68)]
        )

        # Calculate bounding box dimensions
        x, y, w, h = cv2.boundingRect(mouth_points)

        # Set kernel size as a fraction of bounding box size
        kernel_size = int(max(w, h) * args.mask_dilation)
        # if kernel_size % 2 == 0:  # Ensure kernel size is odd
        # kernel_size += 1

        # Create kernel
        kernel = np.ones((kernel_size, kernel_size), np.uint8)

        # Create binary mask for mouth
        mask = np.zeros(img.shape[:2], dtype=np.uint8)
        cv2.fillConvexPoly(mask, mouth_points, 255)

        last_mask = mask  # Update last_mask with the new mask

    # Dilate the mask
    dilated_mask = cv2.dilate(mask, kernel)

    # Calculate distance transform of dilated mask
    dist_transform = cv2.distanceTransform(dilated_mask, cv2.DIST_L2, 5)

    # Normalize distance transform
    cv2.normalize(dist_transform, dist_transform, 0, 255, cv2.NORM_MINMAX)

    # Convert normalized distance transform to binary mask and convert it to uint8
    _, masked_diff = cv2.threshold(dist_transform, 50, 255, cv2.THRESH_BINARY)
    masked_diff = masked_diff.astype(np.uint8)

    # make sure blur is an odd number
    blur = args.mask_feathering
    if blur % 2 == 0:
        blur += 1
    # Set blur size as a fraction of bounding box size
    blur = int(max(w, h) * blur)  # 10% of bounding box size
    if blur % 2 == 0:  # Ensure blur size is odd
        blur += 1
    masked_diff = cv2.GaussianBlur(masked_diff, (blur, blur), 0)

    # Convert numpy arrays to PIL Images
    input1 = Image.fromarray(img)
    input2 = Image.fromarray(original_img)

    # Convert mask to single channel where pixel values are from the alpha channel of the current mask
    mask = Image.fromarray(masked_diff)

    # Ensure images are the same size
    assert input1.size == input2.size == mask.size

    # Paste input1 onto input2 using the mask
    input2.paste(input1, (0, 0), mask)

    # Convert the final PIL Image back to a numpy array
    input2 = np.array(input2)

    # input2 = cv2.cvtColor(input2, cv2.COLOR_BGR2RGB)
    cv2.cvtColor(input2, cv2.COLOR_BGR2RGB, input2)

    return input2, mask


def create_mask(img, original_img):
    global kernel, last_mask, x, y, w, h # Add last_mask to global variables

    # Convert color space from BGR to RGB if necessary
    cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)
    cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB, original_img)

    if last_mask is not None:
        last_mask = np.array(last_mask)  # Convert PIL Image to numpy array
        last_mask = cv2.resize(last_mask, (img.shape[1], img.shape[0]))
        mask = last_mask  # use the last successful mask
        mask = Image.fromarray(mask)

    else:
        # Detect face
        faces = mouth_detector(img)
        if len(faces) == 0:
            cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)
            return img, None
        else:
            face = faces[0]
            shape = predictor(img, face)

            # Get points for mouth
            mouth_points = np.array(
                [[shape.part(i).x, shape.part(i).y] for i in range(48, 68)]
            )

            # Calculate bounding box dimensions
            x, y, w, h = cv2.boundingRect(mouth_points)

            # Set kernel size as a fraction of bounding box size
            kernel_size = int(max(w, h) * args.mask_dilation)
            # if kernel_size % 2 == 0:  # Ensure kernel size is odd
            # kernel_size += 1

            # Create kernel
            kernel = np.ones((kernel_size, kernel_size), np.uint8)

            # Create binary mask for mouth
            mask = np.zeros(img.shape[:2], dtype=np.uint8)
            cv2.fillConvexPoly(mask, mouth_points, 255)

            # Dilate the mask
            dilated_mask = cv2.dilate(mask, kernel)

            # Calculate distance transform of dilated mask
            dist_transform = cv2.distanceTransform(dilated_mask, cv2.DIST_L2, 5)

            # Normalize distance transform
            cv2.normalize(dist_transform, dist_transform, 0, 255, cv2.NORM_MINMAX)

            # Convert normalized distance transform to binary mask and convert it to uint8
            _, masked_diff = cv2.threshold(dist_transform, 50, 255, cv2.THRESH_BINARY)
            masked_diff = masked_diff.astype(np.uint8)

            if not args.mask_feathering == 0:
                blur = args.mask_feathering
                # Set blur size as a fraction of bounding box size
                blur = int(max(w, h) * blur)  # 10% of bounding box size
                if blur % 2 == 0:  # Ensure blur size is odd
                    blur += 1
                masked_diff = cv2.GaussianBlur(masked_diff, (blur, blur), 0)

            # Convert mask to single channel where pixel values are from the alpha channel of the current mask
            mask = Image.fromarray(masked_diff)

            last_mask = mask  # Update last_mask with the final mask after dilation and feathering

    # Convert numpy arrays to PIL Images
    input1 = Image.fromarray(img)
    input2 = Image.fromarray(original_img)

    # Resize mask to match image size
    # mask = Image.fromarray(mask)
    mask = mask.resize(input1.size)

    # Ensure images are the same size
    assert input1.size == input2.size == mask.size

    # Paste input1 onto input2 using the mask
    input2.paste(input1, (0, 0), mask)

    # Convert the final PIL Image back to a numpy array
    input2 = np.array(input2)

    # input2 = cv2.cvtColor(input2, cv2.COLOR_BGR2RGB)
    cv2.cvtColor(input2, cv2.COLOR_BGR2RGB, input2)

    return input2, mask


def get_smoothened_boxes(boxes, T):
    for i in range(len(boxes)):
        if i + T > len(boxes):
            window = boxes[len(boxes) - T :]
        else:
            window = boxes[i : i + T]
        boxes[i] = np.mean(window, axis=0)
    return boxes
            
def face_detect(images, results_file="last_detected_face.pkl"):
    # If results file exists, load it and return
    if os.path.exists(results_file):
        print("Using face detection data from last input")
        with open(results_file, "rb") as f:
            return pickle.load(f)

    results = []
    pady1, pady2, padx1, padx2 = args.pads
    
    tqdm_partial = partial(tqdm, position=0, leave=True)
    for image, (rect) in tqdm_partial(
        zip(images, face_rect(images)),
        total=len(images),
        desc="detecting face in every frame",
        ncols=100,
    ):
        if rect is None:
            cv2.imwrite(
                "temp/faulty_frame.jpg", image
            )  # check this frame where the face was not detected.
            raise ValueError(
                "Face not detected! Ensure the video contains a face in all the frames."
            )

        y1 = max(0, rect[1] - pady1)
        y2 = min(image.shape[0], rect[3] + pady2)
        x1 = max(0, rect[0] - padx1)
        x2 = min(image.shape[1], rect[2] + padx2)

        results.append([x1, y1, x2, y2])


    boxes = np.array(results)
    if str(args.nosmooth) == "False":
        boxes = get_smoothened_boxes(boxes, T=5)
    results = [
        [image[y1:y2, x1:x2], (y1, y2, x1, x2)]
        for image, (x1, y1, x2, y2) in zip(images, boxes)
    ]

    # Save results to file
    with open(results_file, "wb") as f:
        pickle.dump(results, f)

    return results


def datagen(frames, mels):
    img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []
    print("\r" + " " * 100, end="\r")
    if args.box[0] == -1:
        if not args.static:
            face_det_results = face_detect(frames)  # BGR2RGB for CNN face detection
        else:
            face_det_results = face_detect([frames[0]])
    else:
        print("Using the specified bounding box instead of face detection...")
        y1, y2, x1, x2 = args.box
        face_det_results = [[f[y1:y2, x1:x2], (y1, y2, x1, x2)] for f in frames]

    for i, m in enumerate(mels):
        idx = 0 if args.static else i % len(frames)
        frame_to_save = frames[idx].copy()
        face, coords = face_det_results[idx].copy()

        face = cv2.resize(face, (args.img_size, args.img_size))

        img_batch.append(face)
        mel_batch.append(m)
        frame_batch.append(frame_to_save)
        coords_batch.append(coords)

        if len(img_batch) >= args.wav2lip_batch_size:
            img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)

            img_masked = img_batch.copy()
            img_masked[:, args.img_size // 2 :] = 0

            img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.0
            mel_batch = np.reshape(
                mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1]
            )

            yield img_batch, mel_batch, frame_batch, coords_batch
            img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []

    if len(img_batch) > 0:
        img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)

        img_masked = img_batch.copy()
        img_masked[:, args.img_size // 2 :] = 0

        img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.0
        mel_batch = np.reshape(
            mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1]
        )

        yield img_batch, mel_batch, frame_batch, coords_batch


mel_step_size = 16

def _load(checkpoint_path):
    if device != "cpu":
        checkpoint = torch.load(checkpoint_path)
    else:
        checkpoint = torch.load(
            checkpoint_path, map_location=lambda storage, loc: storage
        )
    return checkpoint


def main():
    args.img_size = 96
    frame_number = 11

    if os.path.isfile(args.face) and args.face.split(".")[1] in ["jpg", "png", "jpeg"]:
        args.static = True

    if not os.path.isfile(args.face):
        raise ValueError("--face argument must be a valid path to video/image file")

    elif args.face.split(".")[1] in ["jpg", "png", "jpeg"]:
        full_frames = [cv2.imread(args.face)]
        fps = args.fps

    else:
        if args.fullres != 1:
            print("Resizing video...")
        video_stream = cv2.VideoCapture(args.face)
        fps = video_stream.get(cv2.CAP_PROP_FPS)

        full_frames = []
        while 1:
            still_reading, frame = video_stream.read()
            if not still_reading:
                video_stream.release()
                break

            if args.fullres != 1:
                aspect_ratio = frame.shape[1] / frame.shape[0]
                frame = cv2.resize(
                    frame, (int(args.out_height * aspect_ratio), args.out_height)
                )

            if args.rotate:
                frame = cv2.rotate(frame, cv2.cv2.ROTATE_90_CLOCKWISE)

            y1, y2, x1, x2 = args.crop
            if x2 == -1:
                x2 = frame.shape[1]
            if y2 == -1:
                y2 = frame.shape[0]

            frame = frame[y1:y2, x1:x2]

            full_frames.append(frame)

    if not args.audio.endswith(".wav"):
        print("Converting audio to .wav")
        subprocess.check_call(
            [
                "ffmpeg",
                "-y",
                "-loglevel",
                "error",
                "-i",
                args.audio,
                "temp/temp.wav",
            ]
        )
        args.audio = "temp/temp.wav"

    print("analysing audio...")
    wav = audio.load_wav(args.audio, 16000)
    mel = audio.melspectrogram(wav)

    if np.isnan(mel.reshape(-1)).sum() > 0:
        raise ValueError(
            "Mel contains nan! Using a TTS voice? Add a small epsilon noise to the wav file and try again"
        )

    mel_chunks = []

    mel_idx_multiplier = 80.0 / fps
    i = 0
    while 1:
        start_idx = int(i * mel_idx_multiplier)
        if start_idx + mel_step_size > len(mel[0]):
            mel_chunks.append(mel[:, len(mel[0]) - mel_step_size :])
            break
        mel_chunks.append(mel[:, start_idx : start_idx + mel_step_size])
        i += 1

    full_frames = full_frames[: len(mel_chunks)]
    if str(args.preview_settings) == "True":
        full_frames = [full_frames[0]]
        mel_chunks = [mel_chunks[0]]
    print(str(len(full_frames)) + " frames to process")
    batch_size = args.wav2lip_batch_size
    if str(args.preview_settings) == "True":
        gen = datagen(full_frames, mel_chunks)
    else:
        gen = datagen(full_frames.copy(), mel_chunks)

    for i, (img_batch, mel_batch, frames, coords) in enumerate(
        tqdm(
            gen,
            total=int(np.ceil(float(len(mel_chunks)) / batch_size)),
            desc="Processing Wav2Lip",
            ncols=100,
        )
    ):
        if i == 0:
            if not args.quality == "Fast":
                print(
                    f"mask size: {args.mask_dilation}, feathering: {args.mask_feathering}"
                )
                if not args.quality == "Improved":
                    print("Loading", args.sr_model)
                    run_params = load_sr()

            print("Starting...")
            frame_h, frame_w = full_frames[0].shape[:-1]
            # tempãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            os.makedirs("temp", exist_ok=True)
            
            fourcc = cv2.VideoWriter_fourcc(*"mp4v")
            out = cv2.VideoWriter("temp/result.mp4", fourcc, fps, (frame_w, frame_h))

        img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)
        mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)

        # ğŸ’• FP16æœ€é©åŒ–ï¼ˆãƒ¢ãƒ‡ãƒ«æ¨è«–ã®ã¿ã€åº§æ¨™ã¨ãƒªã‚µã‚¤ã‚ºã¯å¿…ãšFP32ï¼‰
        with torch.no_grad():
            if USE_FP16 and device == 'cuda':
                # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®ã¿FP16å¤‰æ›ï¼ˆå…ƒã®ãƒ†ãƒ³ã‚½ãƒ«ã¯ä¿æŒï¼‰
                img_batch_fp16 = img_batch.half()
                mel_batch_fp16 = mel_batch.half()
                pred = model(mel_batch_fp16, img_batch_fp16)
            else:
                pred = model(mel_batch, img_batch)

        # å‡ºåŠ›ã¯å¿…ãšFP32ã«æˆ»ã—ã¦ç²¾åº¦ä¿æŒ
        pred = pred.cpu().float().numpy().transpose(0, 2, 3, 1) * 255.0

        for idx, (p, f, c) in enumerate(zip(pred, frames, coords)):
            # cv2.imwrite('temp/f.jpg', f)

            # åº§æ¨™ã‚’ç¢ºå®Ÿã«intå‹ã§ä¿æŒï¼ˆFP16ç²¾åº¦å•é¡Œå¯¾ç­–ï¼‰
            y1, y2, x1, x2 = int(c[0]), int(c[1]), int(c[2]), int(c[3])
            
            # åº§æ¨™ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            frame_h, frame_w = f.shape[:2]
            
            # åº§æ¨™ã‚’ãƒ•ãƒ¬ãƒ¼ãƒ ç¯„å›²å†…ã«ä¿®æ­£
            y1 = max(0, min(y1, frame_h - 1))
            y2 = max(y1 + 1, min(y2, frame_h))
            x1 = max(0, min(x1, frame_w - 1))
            x2 = max(x1 + 1, min(x2, frame_w))
            
            # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼ˆæœ€åˆã®æ•°ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã¿ï¼‰
            if idx < 3:
                print(f"Frame {idx}: y1={y1}, y2={y2}, x1={x1}, x2={x2}, frame shape={f.shape}")

            if (
                str(args.debug_mask) == "True"
            ):  # makes the background black & white so you can see the mask better
                f = cv2.cvtColor(f, cv2.COLOR_BGR2GRAY)
                f = cv2.cvtColor(f, cv2.COLOR_GRAY2BGR)

            # ã‚µã‚¤ã‚ºã‚’ç¢ºå®Ÿã«æ­£ã®æ•´æ•°ã§è¨ˆç®—
            width = max(1, x2 - x1)
            height = max(1, y2 - y1)
            
            # äºˆæ¸¬çµæœã®ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            if width < 10 or height < 10:
                print(f"Warning: Very small face region detected: {width}x{height}")
                continue  # ã‚¹ã‚­ãƒƒãƒ—
                
            try:
                p = cv2.resize(p.astype(np.uint8), (width, height))
                cf = f[y1:y2, x1:x2]
            except Exception as e:
                print(f"Resize error at frame {idx}: {e}")
                print(f"p shape: {p.shape}, target size: ({width}, {height})")
                continue

            if args.quality == "Enhanced":
                try:
                    p = upscale(p, run_params)
                except Exception as e:
                    print(f"Upscale error at frame {idx}: {e}")

            if args.quality in ["Enhanced", "Improved"]:
                try:
                    if str(args.mouth_tracking) == "True":
                        p, last_mask = create_tracked_mask(p, cf)
                    else:
                        p, last_mask = create_mask(p, cf)
                except Exception as e:
                    print(f"Mask creation error at frame {idx}: {e}")

            # æœ€çµ‚çš„ãªã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            if p.shape[:2] != (height, width):
                print(f"Size mismatch at frame {idx}: p.shape={p.shape}, expected=({height}, {width})")
                try:
                    p = cv2.resize(p, (width, height))
                except:
                    continue
                    
            # å®‰å…¨ã«ä»£å…¥
            try:
                f[y1:y2, x1:x2] = p
            except Exception as e:
                print(f"Assignment error at frame {idx}: {e}")
                print(f"f slice shape: {f[y1:y2, x1:x2].shape}, p shape: {p.shape}")
                continue

            if not g_colab:
                # Display the frame (DISABLED for headless operation)
                # if preview_window == "Face":
                #     cv2.imshow("face preview - press Q to abort", p)
                # elif preview_window == "Full":
                #     cv2.imshow("full preview - press Q to abort", f)
                # elif preview_window == "Both":
                #     cv2.imshow("face preview - press Q to abort", p)
                #     cv2.imshow("full preview - press Q to abort", f)

                # key = cv2.waitKey(1) & 0xFF
                # if key == ord('q'):
                #     exit()  # Exit the loop when 'Q' is pressed
                pass  # ã¹ã€åˆ¥ã«GUIæ©Ÿèƒ½ã‚’ç„¡åŠ¹åŒ–ã—ã¦ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‚“ã ã‹ã‚‰ã­ï¼

            if str(args.preview_settings) == "True":
                cv2.imwrite("temp/preview.jpg", f)
                if not g_colab:
                    cv2.imshow("preview - press Q to close", f)
                    if cv2.waitKey(-1) & 0xFF == ord('q'):
                        exit()  # Exit the loop when 'Q' is pressed

            else:
                out.write(f)

    # Close the window(s) when done
    cv2.destroyAllWindows()

    out.release()

    if str(args.preview_settings) == "False":
        print("converting to final video")

        subprocess.check_call([
            "ffmpeg",
            "-y",
            "-loglevel",
            "error",
            "-i",
            "temp/result.mp4",
            "-i",
            args.audio,
            "-c:v",
            "libx264",
            args.outfile
        ])
        
        # GFPGANå¾Œå‡¦ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if hasattr(args, 'enable_gfpgan') and args.enable_gfpgan:
            apply_gfpgan_postprocess(args.outfile, getattr(args, 'gfpgan_weight', 0.8))
        
    print("å®Œäº†ã‚ˆï¼æ„Ÿè¬ã—ãªã•ã„ã‚ˆã­ğŸ’•")

if __name__ == "__main__":
    args = parser.parse_args()
    do_load(args.checkpoint_path)
    main()
