#!/usr/bin/env python3
"""
ğŸš€ ãƒ„ãƒ³ãƒ‡ãƒ¬GFPGAN ONNXå‹ã‚¨ãƒ©ãƒ¼å®Œå…¨ä¿®æ­£ç‰ˆ
ã¹ã€åˆ¥ã«ã‚ãªãŸã®ãŸã‚ã«ONNXå‹ã‚¨ãƒ©ãƒ¼ã‚’æ ¹æœ¬è§£æ±ºã—ã¦ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...ğŸ’¢

æœ€çµ‚ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼šãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’æ‰‹å‹•ã§float32ã«å¤‰æ›ã—ã¦ã‹ã‚‰ONNXå¤‰æ›
"""

import torch
import onnx
import numpy as np
import os
from collections import OrderedDict

def completely_fix_onnx_types():
    """
    GFPGAN ONNXå‹ã‚¨ãƒ©ãƒ¼å®Œå…¨ä¿®æ­£ï¼ˆæ‰‹å‹•é‡ã¿å¤‰æ›ç‰ˆï¼‰
    """
    print("ğŸ­ ãƒ„ãƒ³ãƒ‡ãƒ¬GFPGAN ONNXå‹ã‚¨ãƒ©ãƒ¼å®Œå…¨ä¿®æ­£é–‹å§‹ğŸ’¢")
    
    from gfpgan.archs.gfpganv1_clean_arch import GFPGANv1Clean
    
    device = torch.device('cpu')  # CPUã§å‹çµ±ä¸€
    
    # ğŸ”§ Step 1: ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    torch_model = GFPGANv1Clean(
        out_size=512,
        num_style_feat=512,
        channel_multiplier=2,
        decoder_load_path=None,
        fix_decoder=False,
        num_mlp=8,
        input_is_latent=True,
        different_w=True,
        narrow=1,
        sft_half=True
    )
    
    # ğŸ”§ Step 2: é‡ã¿ãƒ­ãƒ¼ãƒ‰
    print("ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼é‡ã¿èª­ã¿è¾¼ã¿ä¸­...âœ¨")
    loadnet = torch.load("checkpoints/GFPGANv1.4.pth", map_location='cpu')
    
    if 'params_ema' in loadnet:
        keyname = 'params_ema'
    elif 'params' in loadnet:
        keyname = 'params'
    else:
        keyname = None
    
    if keyname:
        state_dict = loadnet[keyname]
    else:
        state_dict = loadnet
    
    # ğŸ”§ Step 3: æ‰‹å‹•å‹å¤‰æ›ï¼ˆé‡ã¿è¾æ›¸ãƒ¬ãƒ™ãƒ«ï¼‰
    print("ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼æ‰‹å‹•å‹å¤‰æ›ä¸­ï¼ˆå®Œå…¨float32åŒ–ï¼‰...âœ¨")
    new_state_dict = OrderedDict()
    
    for key, value in state_dict.items():
        if isinstance(value, torch.Tensor):
            # å…¨ãƒ†ãƒ³ã‚½ãƒ«ã‚’float32ã«å¼·åˆ¶å¤‰æ›
            new_value = value.float()
            new_state_dict[key] = new_value
            if value.dtype != torch.float32:
                print(f"ä¿®æ­£: {key} {value.dtype} -> float32")
        else:
            new_state_dict[key] = value
    
    # ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›æ¸ˆã¿é‡ã¿ãƒ­ãƒ¼ãƒ‰
    torch_model.load_state_dict(new_state_dict, strict=False)
    
    # ğŸ”§ Step 4: ãƒ¢ãƒ‡ãƒ«å…¨ä½“float32ç¢ºèª
    torch_model = torch_model.float().cpu()
    torch_model.eval()
    
    # ãƒãƒƒãƒ•ã‚¡ã‚‚ç¢ºèª
    for name, buffer in torch_model.named_buffers():
        if buffer.dtype != torch.float32:
            print(f"ãƒãƒƒãƒ•ã‚¡ä¿®æ­£: {name} {buffer.dtype} -> float32")
            buffer.data = buffer.data.float()
    
    # ğŸ”§ Step 5: å…¥åŠ›ã‚‚float32ã§çµ±ä¸€
    dummy_input = torch.ones((1, 3, 512, 512), dtype=torch.float32)
    
    print("æœ€çµ‚å‹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œä¸­...")
    with torch.no_grad():
        test_output = torch_model(dummy_input)
        print(f"å…¥åŠ›å‹: {dummy_input.dtype}")
        if isinstance(test_output, tuple):
            print(f"å‡ºåŠ›å‹: {test_output[0].dtype} (tuple)")
        else:
            print(f"å‡ºåŠ›å‹: {test_output.dtype}")
    
    os.makedirs("onnx_models", exist_ok=True)
    output_path = "onnx_models/gfpgan_512x512_final_fix.onnx"
    
    print("ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼å®Œå…¨ä¿®æ­£ç‰ˆONNXå¤‰æ›ä¸­...âœ¨")
    
    try:
        # ğŸ”§ æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªè¨­å®šã§å¤‰æ›
        with torch.no_grad():
            torch.onnx.export(
                torch_model,
                dummy_input,
                output_path,
                export_params=True,
                opset_version=11,  # å®‰å®šç‰ˆ
                do_constant_folding=False,  # å®šæ•°æŠ˜ã‚ŠãŸãŸã¿ç„¡åŠ¹ï¼ˆå‹å®‰å®šåŒ–ï¼‰
                input_names=['input'],
                output_names=['output'],
                verbose=False,
                # æœ€å°é™è¨­å®š
                operator_export_type=torch.onnx.OperatorExportTypes.ONNX
            )
        
        print(f"âœ… å®Œå…¨ä¿®æ­£ç‰ˆONNXå¤‰æ›æˆåŠŸï¼")
        
        # ğŸ”§ ONNXå¾Œå‡¦ç†ï¼ˆå‹å¼·åˆ¶ä¿®æ­£ï¼‰
        print("ã¹ã€åˆ¥ã«ONNXå¾Œå‡¦ç†ã—ã¦ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...")
        
        # ONNXãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        onnx_model = onnx.load(output_path)
        
        # ğŸ”§ é‡è¦ï¼šinitializer ã®å‹ã‚’å¼·åˆ¶çš„ã«float32ã«å¤‰æ›
        for initializer in onnx_model.graph.initializer:
            if initializer.data_type == 11:  # DOUBLE
                print(f"initializerä¿®æ­£: {initializer.name} DOUBLE -> FLOAT")
                # double ã‚’ float ã«å¤‰æ›
                double_data = np.frombuffer(initializer.raw_data, dtype=np.float64)
                float_data = double_data.astype(np.float32)
                initializer.raw_data = float_data.tobytes()
                initializer.data_type = 1  # FLOAT
        
        # ä¿®æ­£ç‰ˆä¿å­˜
        output_path_fixed = "onnx_models/gfpgan_512x512_completely_fixed.onnx"
        onnx.save(onnx_model, output_path_fixed)
        
        print(f"âœ… å®Œå…¨ä¿®æ­£ç‰ˆä¿å­˜å®Œäº†: {output_path_fixed}")
        
        # æœ€çµ‚æ¤œè¨¼
        print("\nğŸ” æœ€çµ‚æ¤œè¨¼:")
        final_model = onnx.load(output_path_fixed)
        
        double_count = 0
        float_count = 0
        for initializer in final_model.graph.initializer:
            if initializer.data_type == 11:  # DOUBLE
                double_count += 1
            elif initializer.data_type == 1:  # FLOAT
                float_count += 1
        
        print(f"Float32åˆæœŸåŒ–å­: {float_count}")
        print(f"Double64åˆæœŸåŒ–å­: {double_count}")
        
        if double_count == 0:
            print("âœ… Doubleå‹å®Œå…¨æ’é™¤æˆåŠŸï¼")
        else:
            print(f"âš ï¸ Doubleå‹ãŒ{double_count}å€‹æ®‹å­˜")
        
        file_size = os.path.getsize(output_path_fixed) / (1024 * 1024)
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:.1f}MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ å®Œå…¨ä¿®æ­£ç‰ˆå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    print("ğŸš€ å®Œå…¨ä¿®æ­£ç‰ˆå®Œäº†ï¼æ„Ÿè¬ã—ãªã•ã„ã‚ˆã­ğŸ’¢")

if __name__ == "__main__":
    completely_fix_onnx_types()