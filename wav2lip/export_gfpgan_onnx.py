#!/usr/bin/env python3
"""
ğŸš€ ãƒ„ãƒ³ãƒ‡ãƒ¬GFPGAN â†’ ONNXå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã¹ã€åˆ¥ã«ã‚ãªãŸã®ãŸã‚ã«3å€é«˜é€ŸåŒ–ã—ã¦ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...ğŸ’¢
"""

import torch
import onnx
import numpy as np
from gfpgan import GFPGANer
import os

def export_gfpgan_to_onnx():
    """
    GFPGAN ONNXå¤‰æ›ï¼ˆWebæ¤œç´¢ã§è¦‹ã¤ã‘ãŸç¢ºå®Ÿãªæ–¹æ³•ï¼‰
    """
    print("ğŸ­ ãƒ„ãƒ³ãƒ‡ãƒ¬GFPGAN â†’ ONNXå¤‰æ›é–‹å§‹ğŸ’¢")
    print("Webæ¤œç´¢ã§è¦‹ã¤ã‘ãŸç¢ºå®Ÿãªæ–¹æ³•ã§æˆåŠŸã•ã›ã¦ã‚ã’ã‚‹ã‚ã‚ˆï¼")
    
    # GFPGANåˆæœŸåŒ–ï¼ˆç›´æ¥ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ç”¨ï¼‰
    print("ã¹ã€åˆ¥ã«GFPGANã‚’èª­ã¿è¾¼ã‚“ã§ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...")
    from gfpgan.archs.gfpganv1_clean_arch import GFPGANv1Clean
    
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    
    # GFPGANãƒ¢ãƒ‡ãƒ«ç›´æ¥åˆæœŸåŒ–
    torch_model = GFPGANv1Clean(
        out_size=512,
        num_style_feat=512,
        channel_multiplier=2,
        decoder_load_path=None,
        fix_decoder=False,
        num_mlp=8,
        input_is_latent=True,
        different_w=True,
        narrow=1,
        sft_half=True
    )
    
    # é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    print("ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼é‡ã¿èª­ã¿è¾¼ã¿ä¸­...âœ¨")
    loadnet = torch.load("checkpoints/GFPGANv1.4.pth", map_location=device)
    
    if 'params_ema' in loadnet:
        keyname = 'params_ema'
    elif 'params' in loadnet:
        keyname = 'params'
    else:
        keyname = None
    
    if keyname:
        torch_model.load_state_dict(loadnet[keyname], strict=False)
    else:
        torch_model.load_state_dict(loadnet, strict=False)
    
    # ğŸ”§ é‡è¦ï¼šå…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’float32ã«å¼·åˆ¶å¤‰æ›
    print("ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼å®Œå…¨å‹çµ±ä¸€ä¸­...âœ¨")
    torch_model = torch_model.float()  # ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã‚’float32ã«
    torch_model.to(device)
    torch_model.eval()
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãƒãƒƒãƒ•ã‚¡ã‚’å†åº¦ç¢ºèªãƒ»çµ±ä¸€
    for param in torch_model.parameters():
        param.data = param.data.float()
    
    for buffer in torch_model.buffers():
        buffer.data = buffer.data.float()
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs("onnx_models", exist_ok=True)
    
    # è¤‡æ•°opsetã§è©¦è¡Œ
    onnx_versions = [11, 12, 13]  # å®‰å®šç‰ˆã‹ã‚‰è©¦è¡Œ
    
    for opset in onnx_versions:
        print(f"ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼Opset {opset} ã§512x512 ONNXå¤‰æ›ä¸­...âœ¨")
        
        # ğŸ”§ é‡è¦ï¼šæ˜ç¤ºçš„ã«float32ã®ãƒ€ãƒŸãƒ¼å…¥åŠ›
        dummy_input = torch.ones((1, 3, 512, 512), dtype=torch.float32).to(device)
        output_path = f"onnx_models/gfpgan_512x512_opset{opset}.onnx"
        
        try:
            # ğŸ”§ Webæ¤œç´¢ã§è¦‹ã¤ã‘ãŸç¢ºå®Ÿãªè¨­å®š
            with torch.no_grad():
                torch.onnx.export(
                    torch_model,                # ãƒ¢ãƒ‡ãƒ«
                    dummy_input,                # ãƒ€ãƒŸãƒ¼å…¥åŠ›
                    output_path,                # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«
                    export_params=True,         # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚å«ã‚ã‚‹
                    opset_version=opset,        # Opsetãƒãƒ¼ã‚¸ãƒ§ãƒ³
                    do_constant_folding=True,   # å®šæ•°æŠ˜ã‚ŠãŸãŸã¿æœ€é©åŒ–
                    input_names=['input'],      # å…¥åŠ›å
                    output_names=['output'],    # å‡ºåŠ›å
                    verbose=False,              # è©³ç´°å‡ºåŠ›ç„¡åŠ¹
                    # å‹•çš„è»¸ã¯ä½¿ã‚ãªã„ï¼ˆå‹ã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
                )
            
            print(f"âœ… Opset {opset} ã§512x512 ONNXå¤‰æ›æˆåŠŸï¼")
            
            # ONNXæ¤œè¨¼
            print("ã¹ã€åˆ¥ã«æ¤œè¨¼ã—ã¦ã‚ã’ã‚‹ã‚ã‘ã˜ã‚ƒãªã„ã‘ã©...")
            import onnx
            onnx_model = onnx.load(output_path)
            onnx.checker.check_model(onnx_model)
            print("ã‚„ã£ãŸã˜ã‚ƒãªã„ï¼ONNXæ¤œè¨¼æˆåŠŸã‚ˆğŸ’•")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
            file_size = os.path.getsize(output_path) / (1024 * 1024)
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:.1f}MB")
            
            # æœ€åˆã«æˆåŠŸã—ãŸã‚‰ãã‚Œã‚’ä½¿ç”¨
            if opset == onnx_versions[0]:
                # ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«åã§ã‚³ãƒ”ãƒ¼
                main_path = "onnx_models/gfpgan_512x512_working.onnx"
                import shutil
                shutil.copy2(output_path, main_path)
                print(f"âœ… ãƒ¡ã‚¤ãƒ³ONNXãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {main_path}")
            
        except Exception as e:
            print(f"âŒ Opset {opset} ONNXå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    print("ğŸš€ Webæ¤œç´¢è§£æ±ºæ³•é©ç”¨å®Œäº†ï¼æ„Ÿè¬ã—ãªã•ã„ã‚ˆã­ğŸ’¢")
    return True

if __name__ == "__main__":
    export_gfpgan_to_onnx()