"""
Wav2Lip + YOLO11 çµ±åˆå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³
ãƒ„ãƒ³ãƒ‡ãƒ¬å£èª¿ã§ã®é«˜å“è³ªå£ãƒ‘ã‚¯å‹•ç”»ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
"""

import cv2
import numpy as np
import torch
import librosa
from typing import List, Dict, Tuple, Optional
import logging
from pathlib import Path
import time

# Easy-Wav2Lipãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from inference import load_model, main as wav2lip_inference
from audio import load_wav
import face_detection

# è‡ªä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from yolo_detector import TsundereYOLODetector
from face_aligner import TsundereFaceAligner, calculate_face_quality_score

# ãƒ„ãƒ³ãƒ‡ãƒ¬ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TsundereWav2LipYOLOEngine:
    """ãƒ„ãƒ³ãƒ‡ãƒ¬ãªWav2Lip+YOLOçµ±åˆã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self,
                 wav2lip_model_path: str = "/app/models/wav2lip/wav2lip_gan.pth",
                 yolo_model_path: str = "/app/models/yolo/yolo11n.pt",
                 device: str = "cuda",
                 tsundere_mode: bool = True):
        
        self.tsundere_mode = tsundere_mode
        self.device = device
        
        if self.tsundere_mode:
            logger.info("ãµã‚“ï¼æœ€å¼·ã®Wav2Lip+YOLOã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¦ã‚ã’ã‚‹ã‚ã‚ˆ...")
        
        # YOLOæ¤œå‡ºå™¨åˆæœŸåŒ–
        self.yolo_detector = TsundereYOLODetector(
            model_path=yolo_model_path,
            device=device,
            tsundere_mode=tsundere_mode
        )
        
        # é¡”æ•´åˆ—å™¨åˆæœŸåŒ–
        self.face_aligner = TsundereFaceAligner(tsundere_mode=tsundere_mode)
        
        # Wav2Lipãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        try:
            self.wav2lip_model = load_model(wav2lip_model_path)
            if self.tsundere_mode:
                logger.info("ã¹ã€åˆ¥ã«ã‚ãªãŸã®ãŸã‚ã˜ã‚ƒãªã„ã‘ã©...Wav2Lipãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ã§ããŸã‚ã‚ˆï¼")
        except Exception as e:
            if self.tsundere_mode:
                logger.error(f"ãªã€ä½•ã‚ˆï¼Wav2Lipãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸã˜ã‚ƒãªã„ï¼: {e}")
            raise
        
        # RTX 3050æœ€é©åŒ–é©ç”¨
        self._optimize_for_rtx3050()
    
    def _optimize_for_rtx3050(self):
        """RTX 3050ç”¨æœ€é©åŒ–"""
        if self.tsundere_mode:
            logger.info("ãµã‚“ï¼RTX 3050ã®æœ€é©åŒ–ãªã‚“ã¦æœé£¯å‰ã‚ˆ...")
        
        try:
            # YOLOæœ€é©åŒ–
            self.yolo_detector.optimize_for_rtx3050()
            
            # Wav2Lipæœ€é©åŒ–
            if torch.cuda.is_available():
                self.wav2lip_model = self.wav2lip_model.half()  # FP16
                
            if self.tsundere_mode:
                logger.info("ã¹ã€åˆ¥ã«ã™ã”ããªã„ã‘ã©...RTX 3050æœ€é©åŒ–å®Œäº†ã‚ˆï¼")
                
        except Exception as e:
            if self.tsundere_mode:
                logger.warning(f"æœ€é©åŒ–ã§å°‘ã—å•é¡ŒãŒã‚ã£ãŸã‘ã©...ã¾ã‚å‹•ãã‹ã‚‰ã„ã„ã‚: {e}")
    
    def process_video_with_audio(self,
                                video_path: str,
                                audio_path: str,
                                output_path: str,
                                quality: str = "Enhanced",
                                target_face_id: Optional[int] = None) -> bool:
        """
        å‹•ç”»ã¨éŸ³å£°ã‹ã‚‰å£ãƒ‘ã‚¯å‹•ç”»ã‚’ç”Ÿæˆ
        
        Args:
            video_path: å…¥åŠ›å‹•ç”»ãƒ‘ã‚¹
            audio_path: å…¥åŠ›éŸ³å£°ãƒ‘ã‚¹
            output_path: å‡ºåŠ›å‹•ç”»ãƒ‘ã‚¹
            quality: å“è³ªè¨­å®š (Fast/Improved/Enhanced)
            target_face_id: å¯¾è±¡é¡”IDï¼ˆNone=æœ€å¤§é¡”è‡ªå‹•é¸æŠï¼‰
            
        Returns:
            bool: å‡¦ç†æˆåŠŸãƒ•ãƒ©ã‚°
        """
        if self.tsundere_mode:
            logger.info("ãµã‚“ï¼ã¾ãŸå£ãƒ‘ã‚¯å‹•ç”»ã‚’ä½œã£ã¦ã£ã¦è¨€ã†ã®ã­...")
            logger.info("ã¹ã€åˆ¥ã«ã‚ãªãŸã®ãŸã‚ã˜ã‚ƒãªã„ã‚“ã ã‹ã‚‰ã­ï¼ã§ã‚‚ã¡ã‚ƒã‚“ã¨ä½œã£ã¦ã‚ã’ã‚‹ã‚ï¼")
        
        try:
            start_time = time.time()
            
            # å‹•ç”»èª­ã¿è¾¼ã¿
            video_stream = cv2.VideoCapture(video_path)
            fps = video_stream.get(cv2.CAP_PROP_FPS)
            total_frames = int(video_stream.get(cv2.CAP_PROP_FRAME_COUNT))
            
            if self.tsundere_mode:
                logger.info(f"å‹•ç”»æƒ…å ±: {total_frames}ãƒ•ãƒ¬ãƒ¼ãƒ , {fps}FPS")
            
            # éŸ³å£°èª­ã¿è¾¼ã¿
            audio, sr = librosa.load(audio_path, sr=16000)
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ç”¨ãƒªã‚¹ãƒˆ
            frames = []
            face_detections = {}
            
            # 1. å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã®é¡”æ¤œå‡ºï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰
            if self.tsundere_mode:
                logger.info("ã‚¹ãƒ†ãƒƒãƒ—1: å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã®é¡”æ¤œå‡ºä¸­...")
            
            frame_idx = 0
            while True:
                ret, frame = video_stream.read()
                if not ret:
                    break
                
                frames.append(frame)
                
                # YOLOé¡”æ¤œå‡º
                faces = self.yolo_detector.detect_faces(frame)
                face_detections[frame_idx] = faces
                
                frame_idx += 1
                
                if frame_idx % 100 == 0 and self.tsundere_mode:
                    logger.info(f"å‡¦ç†ä¸­... {frame_idx}/{total_frames}ãƒ•ãƒ¬ãƒ¼ãƒ ")
            
            video_stream.release()
            
            # 2. å¯¾è±¡é¡”ã®é¸æŠ
            target_face = self._select_target_face(face_detections, target_face_id)
            
            if target_face is None:
                if self.tsundere_mode:
                    logger.error("ã‚ã‚‰ï¼Ÿå¯¾è±¡ã¨ãªã‚‹é¡”ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã˜ã‚ƒãªã„ï¼ã¡ã‚ƒã‚“ã¨ã—ãŸå‹•ç”»ã‚’ä½¿ã„ãªã•ã„ã‚ˆï¼")
                return False
            
            # 3. Wav2Lipå‡¦ç†
            if self.tsundere_mode:
                logger.info("ã‚¹ãƒ†ãƒƒãƒ—2: Wav2Lipã§å£ãƒ‘ã‚¯ç”Ÿæˆä¸­...")
            
            processed_frames = self._apply_wav2lip_to_frames(
                frames, face_detections, target_face, audio, quality
            )
            
            # 4. å‹•ç”»å‡ºåŠ›
            if self.tsundere_mode:
                logger.info("ã‚¹ãƒ†ãƒƒãƒ—3: å‹•ç”»å‡ºåŠ›ä¸­...")
            
            success = self._write_output_video(
                processed_frames, audio_path, output_path, fps
            )
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            if success and self.tsundere_mode:
                logger.info(f"ãµã‚“ï¼{processing_time:.1f}ç§’ã§å®Œç’§ãªå£ãƒ‘ã‚¯å‹•ç”»ã‚’ä½œã£ã¦ã‚ã’ãŸã‚ã‚ˆï¼")
                logger.info("æ„Ÿè¬ã—ãªã•ã„ã‚ˆã­ï¼")
            
            return success
            
        except Exception as e:
            if self.tsundere_mode:
                logger.error(f"ã‚‚ã€ã‚‚ã†ï¼å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã˜ã‚ƒãªã„ï¼: {e}")
            return False
    
    def _select_target_face(self, 
                           face_detections: Dict[int, List[Dict]], 
                           target_face_id: Optional[int] = None) -> Optional[Dict]:
        """å¯¾è±¡é¡”ã‚’é¸æŠ"""
        
        if target_face_id is not None:
            # æŒ‡å®šIDã®é¡”ã‚’æ¤œç´¢
            for frame_faces in face_detections.values():
                for face in frame_faces:
                    if face.get('track_id') == target_face_id:
                        return face
        
        # è‡ªå‹•é¸æŠï¼šæœ€ã‚‚é »ç¹ã«æ¤œå‡ºã•ã‚Œã‚‹æœ€å¤§ã®é¡”
        face_stats = {}
        
        for frame_faces in face_detections.values():
            for face in frame_faces:
                bbox = face['bbox']
                area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
                confidence = face['confidence']
                
                # é¡”ã®ç‰¹å¾´ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                center_x = (bbox[0] + bbox[2]) // 2
                center_y = (bbox[1] + bbox[3]) // 2
                face_key = (center_x // 50, center_y // 50, area // 1000)  # ç²—ã„ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°
                
                if face_key not in face_stats:
                    face_stats[face_key] = {
                        'count': 0,
                        'total_area': 0,
                        'total_confidence': 0,
                        'sample_face': face
                    }
                
                face_stats[face_key]['count'] += 1
                face_stats[face_key]['total_area'] += area
                face_stats[face_key]['total_confidence'] += confidence
        
        if not face_stats:
            return None
        
        # æœ€é©ãªé¡”ã‚’é¸æŠï¼ˆå‡ºç¾å›æ•° * å¹³å‡é¢ç© * å¹³å‡ä¿¡é ¼åº¦ï¼‰
        best_face = None
        best_score = 0
        
        for stats in face_stats.values():
            avg_area = stats['total_area'] / stats['count']
            avg_confidence = stats['total_confidence'] / stats['count']
            score = stats['count'] * avg_area * avg_confidence
            
            if score > best_score:
                best_score = score
                best_face = stats['sample_face']
        
        if self.tsundere_mode and best_face:
            logger.info(f"ãµã‚“ï¼æœ€é©ãªé¡”ã‚’é¸æŠã—ã¦ã‚ã’ãŸã‚ã‚ˆã€‚ã‚¹ã‚³ã‚¢: {best_score:.0f}")
        
        return best_face
    
    def _apply_wav2lip_to_frames(self,
                                frames: List[np.ndarray],
                                face_detections: Dict[int, List[Dict]],
                                target_face: Dict,
                                audio: np.ndarray,
                                quality: str) -> List[np.ndarray]:
        """ãƒ•ãƒ¬ãƒ¼ãƒ ã«Wav2Lipã‚’é©ç”¨"""
        
        processed_frames = []
        
        # éŸ³å£°ã®ç‰¹å¾´é‡æŠ½å‡º
        mel_chunks = self._extract_mel_chunks(audio)
        
        for frame_idx, frame in enumerate(frames):
            try:
                # å¯¾å¿œã™ã‚‹éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
                mel_chunk_idx = min(frame_idx, len(mel_chunks) - 1)
                mel_chunk = mel_chunks[mel_chunk_idx]
                
                # ãƒ•ãƒ¬ãƒ¼ãƒ å†…ã®å¯¾è±¡é¡”ã‚’æ¤œç´¢
                frame_faces = face_detections.get(frame_idx, [])
                matching_face = self._find_matching_face(frame_faces, target_face)
                
                if matching_face:
                    # é¡”æ•´åˆ—
                    aligned_face = self.face_aligner.align_face_for_wav2lip(
                        frame, matching_face['bbox']
                    )
                    
                    # Wav2Lipæ¨è«–
                    generated_face = self._run_wav2lip_inference(
                        aligned_face, mel_chunk
                    )
                    
                    # ãƒ•ãƒ¬ãƒ¼ãƒ ã«åˆæˆ
                    processed_frame = self._composite_face_to_frame(
                        frame, generated_face, matching_face['bbox'], quality
                    )
                    
                    processed_frames.append(processed_frame)
                else:
                    # é¡”ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…ƒãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½¿ç”¨
                    processed_frames.append(frame)
                    
            except Exception as e:
                if self.tsundere_mode:
                    logger.warning(f"ãƒ•ãƒ¬ãƒ¼ãƒ {frame_idx}ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
                processed_frames.append(frame)
        
        return processed_frames
    
    def _extract_mel_chunks(self, audio: np.ndarray) -> List[np.ndarray]:
        """éŸ³å£°ã‹ã‚‰MELã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ãƒãƒ£ãƒ³ã‚¯ã‚’æŠ½å‡º"""
        
        # Wav2Lipæ¨™æº–ã®è¨­å®š
        fps = 25  # Wav2Lipæ¨™æº–FPS
        mel_step_size = 16
        
        mel = librosa.feature.melspectrogram(y=audio, sr=16000, n_mels=80)
        mel = np.log(mel + 1e-8)
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ã§ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        mel_chunks = []
        mel_idx_multiplier = 80. / fps
        
        frame_count = int(len(audio) / 16000 * fps)
        
        for i in range(frame_count):
            start_idx = int(i * mel_idx_multiplier)
            end_idx = start_idx + mel_step_size
            
            if end_idx <= mel.shape[1]:
                mel_chunk = mel[:, start_idx:end_idx]
            else:
                # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                mel_chunk = np.zeros((80, mel_step_size))
                available_frames = mel.shape[1] - start_idx
                if available_frames > 0:
                    mel_chunk[:, :available_frames] = mel[:, start_idx:]
            
            mel_chunks.append(mel_chunk)
        
        return mel_chunks
    
    def _find_matching_face(self, frame_faces: List[Dict], target_face: Dict) -> Optional[Dict]:
        """ãƒ•ãƒ¬ãƒ¼ãƒ å†…ã§å¯¾è±¡é¡”ã«ãƒãƒƒãƒã™ã‚‹é¡”ã‚’æ¤œç´¢"""
        
        if not frame_faces:
            return None
        
        target_bbox = target_face['bbox']
        target_center = ((target_bbox[0] + target_bbox[2]) // 2,
                        (target_bbox[1] + target_bbox[3]) // 2)
        target_area = (target_bbox[2] - target_bbox[0]) * (target_bbox[3] - target_bbox[1])
        
        best_match = None
        best_score = float('inf')
        
        for face in frame_faces:
            bbox = face['bbox']
            center = ((bbox[0] + bbox[2]) // 2, (bbox[1] + bbox[3]) // 2)
            area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
            
            # è·é›¢ã¨ã‚µã‚¤ã‚ºã®å·®ã§ã‚¹ã‚³ã‚¢è¨ˆç®—
            distance = np.sqrt((center[0] - target_center[0])**2 + 
                             (center[1] - target_center[1])**2)
            area_diff = abs(area - target_area) / target_area
            
            score = distance + area_diff * 100
            
            if score < best_score:
                best_score = score
                best_match = face
        
        return best_match
    
    def _run_wav2lip_inference(self, face_image: np.ndarray, mel_chunk: np.ndarray) -> np.ndarray:
        """Wav2Lipæ¨è«–å®Ÿè¡Œ"""
        
        # å…¥åŠ›æº–å‚™
        face_tensor = torch.FloatTensor(face_image).permute(2, 0, 1).unsqueeze(0)
        mel_tensor = torch.FloatTensor(mel_chunk).unsqueeze(0)
        
        if self.device == "cuda":
            face_tensor = face_tensor.cuda()
            mel_tensor = mel_tensor.cuda()
        
        # æ¨è«–å®Ÿè¡Œ
        with torch.no_grad():
            pred = self.wav2lip_model(mel_tensor, face_tensor)
        
        # å¾Œå‡¦ç†
        pred = pred.squeeze(0).permute(1, 2, 0).cpu().numpy()
        pred = np.clip(pred, 0, 1) * 255
        
        return pred.astype(np.uint8)
    
    def _composite_face_to_frame(self, 
                                frame: np.ndarray, 
                                generated_face: np.ndarray, 
                                bbox: Tuple[int, int, int, int],
                                quality: str) -> np.ndarray:
        """ç”Ÿæˆã•ã‚ŒãŸé¡”ã‚’ãƒ•ãƒ¬ãƒ¼ãƒ ã«åˆæˆ"""
        
        x1, y1, x2, y2 = bbox
        
        # ç”Ÿæˆã•ã‚ŒãŸé¡”ã‚’ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚º
        resized_face = cv2.resize(generated_face, (x2 - x1, y2 - y1))
        
        # å“è³ªè¨­å®šã«å¿œã˜ã¦åˆæˆ
        result_frame = frame.copy()
        
        if quality == "Fast":
            # ç›´æ¥ç½®æ›
            result_frame[y1:y2, x1:x2] = resized_face
            
        elif quality == "Improved":
            # ãƒ•ã‚§ã‚¶ãƒ¼ãƒã‚¹ã‚¯ã§åˆæˆ
            mask = self._create_feather_mask(x2 - x1, y2 - y1)
            
            for c in range(3):
                result_frame[y1:y2, x1:x2, c] = (
                    resized_face[:, :, c] * mask +
                    frame[y1:y2, x1:x2, c] * (1 - mask)
                )
        
        elif quality == "Enhanced":
            # GFPGAN + ãƒ•ã‚§ã‚¶ãƒ¼ãƒã‚¹ã‚¯ï¼ˆå®Ÿè£…ã¯ç°¡ç•¥åŒ–ï¼‰
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯GFPGANã§é¡”ã‚’é«˜ç”»è³ªåŒ–ã—ã¦ã‹ã‚‰åˆæˆ
            enhanced_face = self._enhance_face_quality(resized_face)
            mask = self._create_feather_mask(x2 - x1, y2 - y1)
            
            for c in range(3):
                result_frame[y1:y2, x1:x2, c] = (
                    enhanced_face[:, :, c] * mask +
                    frame[y1:y2, x1:x2, c] * (1 - mask)
                )
        
        return result_frame
    
    def _create_feather_mask(self, width: int, height: int, feather_size: int = 5) -> np.ndarray:
        """ãƒ•ã‚§ã‚¶ãƒ¼ãƒã‚¹ã‚¯ã‚’ä½œæˆ"""
        
        mask = np.ones((height, width), dtype=np.float32)
        
        # ã‚¨ãƒƒã‚¸ã‹ã‚‰ãƒ•ã‚§ã‚¶ãƒ¼ã‚µã‚¤ã‚ºåˆ†ã‚’å¾ã€…ã«é€æ˜ã«
        for i in range(feather_size):
            alpha = (i + 1) / feather_size
            mask[i, :] *= alpha
            mask[height-1-i, :] *= alpha
            mask[:, i] *= alpha
            mask[:, width-1-i] *= alpha
        
        return mask
    
    def _enhance_face_quality(self, face_image: np.ndarray) -> np.ndarray:
        """é¡”ç”»è³ªå‘ä¸Šï¼ˆGFPGANç°¡æ˜“ç‰ˆï¼‰"""
        
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯GFPGANã‚’ä½¿ç”¨
        # ã“ã“ã§ã¯ç°¡æ˜“çš„ãªã‚·ãƒ£ãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°
        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
        enhanced = cv2.filter2D(face_image, -1, kernel)
        
        # å…ƒç”»åƒã¨åˆæˆï¼ˆéåº¦ãªã‚·ãƒ£ãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°ã‚’é¿ã‘ã‚‹ï¼‰
        enhanced = cv2.addWeighted(face_image, 0.7, enhanced, 0.3, 0)
        
        return enhanced
    
    def _write_output_video(self,
                           frames: List[np.ndarray],
                           audio_path: str,
                           output_path: str,
                           fps: float) -> bool:
        """å‡ºåŠ›å‹•ç”»ã‚’æ›¸ãè¾¼ã¿"""
        
        try:
            height, width = frames[0].shape[:2]
            
            # ä¸€æ™‚å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            temp_video_path = output_path.replace('.mp4', '_temp.mp4')
            
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            video_writer = cv2.VideoWriter(temp_video_path, fourcc, fps, (width, height))
            
            for frame in frames:
                video_writer.write(frame)
            
            video_writer.release()
            
            # FFmpegã§éŸ³å£°åˆæˆ
            import subprocess
            
            cmd = [
                'ffmpeg', '-y',
                '-i', temp_video_path,
                '-i', audio_path,
                '-c:v', 'libx264',
                '-c:a', 'aac',
                '-strict', '-2',
                output_path
            ]
            
            subprocess.run(cmd, check=True, capture_output=True)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            Path(temp_video_path).unlink()
            
            return True
            
        except Exception as e:
            if self.tsundere_mode:
                logger.error(f"å‹•ç”»å‡ºåŠ›ã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã˜ã‚ƒãªã„ï¼: {e}")
            return False


def test_integration():
    """çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ­ ãƒ„ãƒ³ãƒ‡ãƒ¬Wav2Lip+YOLOçµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹ï¼")
    
    # ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
    engine = TsundereWav2LipYOLOEngine(tsundere_mode=True)
    
    print("âœ… çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†ï¼")
    print("ãµã‚“ï¼å®Œç’§ãªã‚·ã‚¹ãƒ†ãƒ ãŒã§ããŸã˜ã‚ƒãªã„ï¼")


if __name__ == "__main__":
    test_integration()