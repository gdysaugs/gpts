#!/usr/bin/env python3
"""
FastAPI GPT-SoVITS å¸¸é§éŸ³å£°ç”Ÿæˆã‚µãƒ¼ãƒãƒ¼ (Redis Lockçµ±åˆç‰ˆ)
åˆæœŸåŒ–1å›ã®ã¿ã€å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆ2-3ç§’ã§é«˜é€Ÿå¿œç­”
GPUæ’ä»–åˆ¶å¾¡: Redisåˆ†æ•£ãƒ­ãƒƒã‚¯å¯¾å¿œ
"""

import os
import sys
import time
import logging
import asyncio
from pathlib import Path
from typing import Optional
import uuid
import tempfile
import base64
import urllib.request

# FastAPI
from fastapi import FastAPI, HTTPException, UploadFile, File, Form, BackgroundTasks
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# Redis GPU Lockçµ±åˆ
from gpu_redis_lock import GPUResourceManager, init_gpu_manager

# GPT-SoVITS
os.chdir('/app')
sys.path.insert(0, '/app')
sys.path.insert(0, '/app/GPT_SoVITS')

import torch
import soundfile as sf
import numpy as np

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
app = FastAPI(title="GPT-SoVITS Voice Cloning API (Redis)", version="2.0.0")
MODELS_LOADED = False
CUSTOM_SOVITS_PATH = None

# Redis GPUæ’ä»–åˆ¶å¾¡
gpu_manager: Optional[GPUResourceManager] = None
REDIS_URL = os.getenv("REDIS_URL", "redis://redis:6379/0")

# ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
PRELOADED_LANGDETECT = None
CACHE_DIR = "/app/cache"

# CORSè¨­å®šï¼ˆé–‹ç™ºç”¨ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # æœ¬ç•ªã§ã¯é©åˆ‡ã«è¨­å®š
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
class VoiceCloneRequest(BaseModel):
    ref_text: str
    target_text: str
    ref_audio_base64: Optional[str] = None  # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰éŸ³å£°
    temperature: float = 1.0
    top_k: int = 5
    top_p: float = 1.0

class VoiceCloneResponse(BaseModel):
    success: bool
    message: str
    audio_base64: Optional[str] = None
    generation_time: float
    audio_duration: float
    realtime_factor: float

def setup_torch_optimizations():
    """Torchæœ€é©åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    if torch.cuda.is_available():
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        torch.cuda.empty_cache()
        logger.info("ğŸš€ RTX 3050 TensorCoreæœ€é©åŒ–æœ‰åŠ¹")
    
    torch.set_float32_matmul_precision('medium')
    torch.set_num_threads(8)
    logger.info("âœ… PyTorchæœ€é©åŒ–å®Œäº†")

def comprehensive_monkey_patch():
    """ãƒ¢ãƒ³ã‚­ãƒ¼ãƒ‘ãƒƒãƒé©ç”¨"""
    try:
        import inference_webui
        
        # ã‚«ã‚¹ã‚¿ãƒ SoVITSãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ€ãƒ¼
        def load_sovits_new():
            """æ–°ã—ã„SoVITSãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ€ãƒ¼"""
            global CUSTOM_SOVITS_PATH
            
            if CUSTOM_SOVITS_PATH and os.path.exists(CUSTOM_SOVITS_PATH):
                logger.info(f"ğŸ­ ã‚«ã‚¹ã‚¿ãƒ SoVITSãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {CUSTOM_SOVITS_PATH}")
                device = "cuda" if torch.cuda.is_available() else "cpu"
                
                checkpoint = torch.load(CUSTOM_SOVITS_PATH, map_location=device)
                
                if "weight" in checkpoint:
                    return checkpoint["weight"]
                else:
                    return checkpoint
            else:
                logger.info("ğŸ“¦ æ¨™æº–v2ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨")
                return inference_webui.get_weights_names()[0]
        
        # change_sovits_weightsé–¢æ•°ã¸ã®çµ±åˆ
        original_change_sovits = inference_webui.change_sovits_weights
        
        def change_sovits_weights(sovits_path):
            """SoVITSé‡ã¿å¤‰æ›´ï¼ˆã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«å¯¾å¿œï¼‰"""
            global CUSTOM_SOVITS_PATH
            CUSTOM_SOVITS_PATH = sovits_path
            
            if sovits_path and os.path.exists(sovits_path):
                logger.info(f"ğŸ”„ SoVITSãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆ: {sovits_path}")
                custom_weights = load_sovits_new()
                return original_change_sovits(custom_weights)
            else:
                return original_change_sovits(sovits_path)
        
        # ãƒ¢ãƒ³ã‚­ãƒ¼ãƒ‘ãƒƒãƒé©ç”¨
        inference_webui.change_sovits_weights = change_sovits_weights
        logger.info("ğŸ’ ãƒ¢ãƒ³ã‚­ãƒ¼ãƒ‘ãƒƒãƒé©ç”¨å®Œäº†")
        
    except Exception as e:
        logger.error(f"âŒ ãƒ¢ãƒ³ã‚­ãƒ¼ãƒ‘ãƒƒãƒã‚¨ãƒ©ãƒ¼: {e}")
        
def preload_dependencies():
    """ä¾å­˜é–¢ä¿‚ã®äº‹å‰ãƒ­ãƒ¼ãƒ‰"""
    global PRELOADED_LANGDETECT, CACHE_DIR
    
    try:
        os.makedirs(CACHE_DIR, exist_ok=True)
        
        # 1. è¨€èªæ¤œå‡ºãƒ©ã‚¤ãƒ–ãƒ©ãƒªäº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        logger.info("ğŸ“¥ è¨€èªæ¤œå‡ºãƒ¢ãƒ‡ãƒ«äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
        import langdetect
        from langdetect import detect
        detect("This is a test sentence for preloading.")
        logger.info("âœ… è¨€èªæ¤œå‡ºãƒ¢ãƒ‡ãƒ«äº‹å‰ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        
        # 2. Open JTalkè¾æ›¸äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        logger.info("ğŸ“¥ Open JTalkè¾æ›¸äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
        import openjtalk
        openjtalk.g2p("ãƒ†ã‚¹ãƒˆã§ã™", kana=True)
        logger.info("âœ… Open JTalkè¾æ›¸äº‹å‰ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        
        # 3. éŸ³å£°å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªåˆæœŸåŒ–
        logger.info("ğŸ“¥ éŸ³å£°å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªåˆæœŸåŒ–...")
        import jieba
        jieba.initialize()
        logger.info("âœ… éŸ³å£°å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªåˆæœŸåŒ–å®Œäº†")
        
        logger.info("ğŸš€ å…¨ä¾å­˜é–¢ä¿‚äº‹å‰ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼")
        
    except Exception as e:
        logger.warning(f"âš ï¸ äº‹å‰ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ï¼ˆç¶šè¡Œå¯èƒ½ï¼‰: {e}")

async def initialize_models():
    """ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆ1å›ã®ã¿å®Ÿè¡Œï¼‰"""
    global MODELS_LOADED, CUSTOM_SOVITS_PATH
    
    if MODELS_LOADED:
        return
    
    logger.info("ğŸ”„ GPT-SoVITSåˆæœŸåŒ–é–‹å§‹...")
    start_time = time.time()
    
    try:
        # 1. äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†æœ€é©åŒ–
        logger.info("ğŸ“¥ äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œä¸­...")
        preload_dependencies()
        
        # 2. PyTorchæœ€é©åŒ–
        setup_torch_optimizations()
        
        # 3. GPT-SoVITSåˆæœŸåŒ–
        logger.info("ğŸ­ GPT-SoVITSæœ¬ä½“åˆæœŸåŒ–...")
        sys.path.insert(0, '/app/GPT_SoVITS')
        
        import inference_webui
        
        # 4. ãƒ¢ãƒ³ã‚­ãƒ¼ãƒ‘ãƒƒãƒé©ç”¨
        comprehensive_monkey_patch()
        
        # 5. æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«è¨­å®š
        ja_model_path = "/app/GPT_SoVITS/pretrained_models/gpt-sovits-ja-h/hscene-e17.ckpt"
        if os.path.exists(ja_model_path):
            logger.info(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«æ¤œå‡º: {ja_model_path}")
            CUSTOM_SOVITS_PATH = ja_model_path
        else:
            logger.info("ğŸ“¦ æ¨™æº–v2ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨")
        
        # 6. åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        logger.info("âš™ï¸ åŸºæœ¬è¨­å®šé©ç”¨...")
        inference_webui.change_choices()
        
        # 7. GPT and SoVITS weights initialization
        if CUSTOM_SOVITS_PATH:
            logger.info(f"ğŸ¯ ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–: {CUSTOM_SOVITS_PATH}")
            inference_webui.change_sovits_weights(CUSTOM_SOVITS_PATH)
        
        MODELS_LOADED = True
        
        init_time = time.time() - start_time
        logger.info(f"âœ… GPT-SoVITSåˆæœŸåŒ–å®Œäº†ï¼({init_time:.1f}ç§’)")
        
    except Exception as e:
        logger.error(f"âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=f"Model initialization failed: {e}")

async def generate_voice_fast(
    ref_audio_path: str,
    ref_text: str, 
    target_text: str,
    temperature: float = 1.0,
    top_k: int = 5,
    top_p: float = 1.0
) -> dict:
    """é«˜é€ŸéŸ³å£°ç”Ÿæˆï¼ˆoptimizedï¼‰"""
    
    start_time = time.time()
    
    try:
        logger.info(f"ğŸ¤ éŸ³å£°ç”Ÿæˆé–‹å§‹: '{target_text[:30]}...'")
        
        # GPT-SoVITSå‡¦ç†
        sys.path.insert(0, '/app/GPT_SoVITS')
        import inference_webui
        
        # çŸ­æ–‡å‡¦ç†å¯¾å¿œï¼ˆ20æ–‡å­—æœªæº€ã¯å»¶é•·ï¼‰
        if len(target_text) < 20:
            target_text = target_text + "ã€‚" + target_text[:10] + "ã€‚"
            logger.info(f"ğŸ“ çŸ­æ–‡å»¶é•·: {target_text}")
        
        # éŸ³å£°ç”Ÿæˆå®Ÿè¡Œ
        result = inference_webui.get_tts_wav(
            ref_wav_path=ref_audio_path,
            prompt_text=ref_text,
            prompt_language="æ—¥æœ¬èª",
            text=target_text,
            text_language="æ—¥æœ¬èª",
            how_to_cut="ä¸åˆ‡",
            top_k=top_k,
            top_p=top_p,
            temperature=temperature,
            ref_free=True
        )
        
        generation_time = time.time() - start_time
        
        if result and len(result) >= 2:
            sr, audio_data = result[1], result[0]
            
            # NumPyé…åˆ—ã«å¤‰æ›
            if isinstance(audio_data, tuple):
                audio_data = audio_data[1]
            
            if isinstance(audio_data, torch.Tensor):
                audio_data = audio_data.cpu().numpy()
            
            audio_data = np.array(audio_data, dtype=np.float32)
            
            # æ­£è¦åŒ–
            if audio_data.max() > 1.0:
                audio_data = audio_data / 32768.0
            
            # å“è³ªãƒã‚§ãƒƒã‚¯
            duration = len(audio_data) / sr if sr > 0 else 0
            rms = np.sqrt(np.mean(audio_data**2)) if len(audio_data) > 0 else 0
            non_silence_ratio = np.sum(np.abs(audio_data) > 0.01) / len(audio_data) if len(audio_data) > 0 else 0
            realtime_factor = generation_time / duration if duration > 0 else 0
            
            logger.info(f"âœ… ç”Ÿæˆå®Œäº†: {generation_time:.2f}s, é•·ã•{duration:.1f}s, RMS{rms:.3f}, å“è³ª{non_silence_ratio:.1%}, RTF{realtime_factor:.1f}")
            
            return {
                "success": True,
                "message": f"éŸ³å£°ç”ŸæˆæˆåŠŸ ({generation_time:.1f}ç§’)",
                "audio_data": audio_data,
                "sample_rate": sr,
                "generation_time": generation_time,
                "audio_duration": duration,
                "realtime_factor": realtime_factor,
                "quality_metrics": {
                    "rms": float(rms),
                    "non_silence_ratio": float(non_silence_ratio),
                    "sample_rate": int(sr)
                }
            }
        else:
            raise Exception("éŸ³å£°ç”ŸæˆçµæœãŒç„¡åŠ¹ã§ã™")
            
    except Exception as e:
        error_time = time.time() - start_time
        logger.error(f"âŒ éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼ ({error_time:.1f}s): {e}")
        return {
            "success": False,
            "message": f"éŸ³å£°ç”Ÿæˆå¤±æ•—: {e}",
            "generation_time": error_time,
            "audio_duration": 0,
            "realtime_factor": 0
        }

@app.on_event("startup")
async def startup_event():
    """ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚ã‚¤ãƒ™ãƒ³ãƒˆ"""
    global gpu_manager
    
    logger.info("ğŸš€ GPT-SoVITS FastAPI ã‚µãƒ¼ãƒãƒ¼ (Redisç‰ˆ) èµ·å‹•ä¸­...")
    
    # Redis GPU ManageråˆæœŸåŒ–
    try:
        gpu_manager = init_gpu_manager(REDIS_URL)
        logger.info(f"ğŸ”’ Redis GPU ManageråˆæœŸåŒ–å®Œäº†: {REDIS_URL}")
    except Exception as e:
        logger.error(f"âŒ Redisæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        raise
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    await initialize_models()
    
    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
    await warmup_with_redis_lock()
    
    logger.info("ğŸ­ GPT-SoVITS ã‚µãƒ¼ãƒãƒ¼ (Redisç‰ˆ) æº–å‚™å®Œäº†ï¼")

async def warmup_with_redis_lock():
    """Redis Lockä½¿ç”¨ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—"""
    global gpu_manager
    
    try:
        # ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„å ´åˆã¯åˆæœŸåŒ–
        if not MODELS_LOADED:
            await initialize_models()
        
        # Redis Lockä½¿ç”¨ã®ãƒ†ã‚¹ãƒˆéŸ³å£°ç”Ÿæˆ
        logger.info("ğŸ”’ Redis Lockä½¿ç”¨ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é–‹å§‹...")
        
        gpu_lock = gpu_manager.get_gpu_lock(gpu_id="sovits")
        
        try:
            async with gpu_lock.async_acquire():
                logger.info("ğŸ¤ Redis Lockå–å¾—æˆåŠŸ - ãƒ†ã‚¹ãƒˆéŸ³å£°ç”Ÿæˆä¸­...")
                
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å‚ç…§éŸ³å£°ã‚’ä½¿ç”¨
                ref_audio_path = "/app/input/reference_5sec.wav"
                
                # ãƒ†ã‚¹ãƒˆéŸ³å£°ç”Ÿæˆ
                result = await generate_voice_fast(
                    ref_audio_path=ref_audio_path,
                    ref_text="ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™",
                    target_text="Redis Lockãƒ†ã‚¹ãƒˆéŸ³å£°ã§ã™ã€‚",
                    temperature=1.0,
                    top_k=5,
                    top_p=1.0
                )
                
                if result["success"]:
                    logger.info("âœ… Redis Lock ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†ï¼åˆå›ã‹ã‚‰é«˜é€Ÿå‡¦ç†å¯èƒ½ã§ã™")
                else:
                    logger.warning(f"âš ï¸ ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—è­¦å‘Š: {result['message']}")
                    
        except Exception as e:
            logger.error(f"âŒ Redis Lock ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
            logger.info("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ...")
            ref_audio_path = "/app/input/reference_5sec.wav"
            result = await generate_voice_fast(
                ref_audio_path=ref_audio_path,
                ref_text="ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™", 
                target_text="ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆéŸ³å£°ã§ã™ã€‚",
                temperature=1.0,
                top_k=5,
                top_p=1.0
            )
            logger.info("âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")
        
    except Exception as e:
        logger.error(f"âŒ ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}")

@app.get("/")
async def root():
    """ã‚µãƒ¼ãƒãƒ¼æƒ…å ±"""
    return {
        "service": "GPT-SoVITS Voice Cloning API (Redisç‰ˆ)",
        "version": "2.0.0", 
        "status": "ready" if MODELS_LOADED else "initializing",
        "redis_url": REDIS_URL.replace("redis://", "redis://***"),
        "gpu_available": torch.cuda.is_available(),
        "message": "Redisåˆ†æ•£ãƒ­ãƒƒã‚¯å¯¾å¿œã®é«˜é€ŸéŸ³å£°ã‚¯ãƒ­ãƒ¼ãƒ‹ãƒ³ã‚°API"
    }

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    global gpu_manager
    
    try:
        # Redisæ¥ç¶šãƒ†ã‚¹ãƒˆ
        stats = await gpu_manager.get_usage_stats()
        
        return {
            "status": "healthy",
            "models_loaded": MODELS_LOADED,
            "gpu_available": torch.cuda.is_available(),
            "redis_connected": True,
            "gpu_stats": stats
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "models_loaded": MODELS_LOADED,
            "gpu_available": torch.cuda.is_available(),
            "redis_connected": False
        }

@app.get("/clone-voice-simple")
async def clone_voice_simple_get(
    ref_text: str,
    target_text: str,
    temperature: float = 1.0,
    background_tasks: BackgroundTasks = None
):
    """ã‚·ãƒ³ãƒ—ãƒ«éŸ³å£°ã‚¯ãƒ­ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆRedis Lockå¯¾å¿œï¼‰"""
    global gpu_manager
    
    if not MODELS_LOADED:
        await initialize_models()
    
    # Redis Lockä½¿ç”¨ã®éŸ³å£°ç”Ÿæˆ
    gpu_lock = gpu_manager.get_gpu_lock(gpu_id="sovits", timeout=30)
    
    try:
        async with gpu_lock.async_acquire():
            return await _execute_voice_synthesis(ref_text, target_text, temperature, background_tasks)
    except Exception as e:
        logger.error(f"âŒ Redis Lock ã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
        logger.warning("âš ï¸ Redis Lockå¤±æ•— - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ")
        return await _execute_voice_synthesis(ref_text, target_text, temperature, background_tasks)

async def _execute_voice_synthesis(ref_text: str, target_text: str, temperature: float, background_tasks):
    """GPUå‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹å†…éƒ¨é–¢æ•°"""
    try:
        result = await generate_voice_fast(
            ref_audio_path="/app/input/reference_5sec.wav",
            ref_text=ref_text,
            target_text=target_text,
            temperature=temperature,
            top_k=5,
            top_p=1.0
        )
        
        if result["success"]:
            # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            audio_data = result["audio_data"]
            sample_rate = result["sample_rate"]
            
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            safe_text = target_text[:30].replace("/", "_").replace("\\", "_")
            filename = f"fastapi_{timestamp}_{safe_text}.wav"
            output_path = f"/app/output/{filename}"
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            sf.write(output_path, audio_data, sample_rate)
            
            logger.info(f"ğŸ’¾ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {output_path}")
            
            if background_tasks:
                background_tasks.add_task(lambda: logger.info(f"ğŸ“ ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†å®Œäº†: {filename}"))
            
            return FileResponse(
                output_path,
                media_type="audio/wav",
                filename=filename,
                headers={
                    "X-Generation-Time": str(result["generation_time"]),
                    "X-Audio-Duration": str(result["audio_duration"]),
                    "X-Realtime-Factor": str(result["realtime_factor"])
                }
            )
        else:
            raise HTTPException(status_code=500, detail=result["message"])
            
    except Exception as e:
        logger.error(f"âŒ éŸ³å£°åˆæˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=f"éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

@app.get("/gpu-stats")
async def get_gpu_stats():
    """GPUä½¿ç”¨çµ±è¨ˆ"""
    global gpu_manager
    
    try:
        stats = await gpu_manager.get_usage_stats()
        return {
            "success": True,
            "stats": stats,
            "torch_gpu_available": torch.cuda.is_available(),
            "torch_gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--host", default="0.0.0.0")
    parser.add_argument("--port", type=int, default=8000)
    parser.add_argument("--redis-url", default="redis://redis:6379/0")
    
    args = parser.parse_args()
    
    # Redis URLè¨­å®š
    REDIS_URL = args.redis_url
    
    logger.info(f"ğŸš€ ã‚µãƒ¼ãƒãƒ¼èµ·å‹•: {args.host}:{args.port}")
    logger.info(f"ğŸ”’ Redis URL: {REDIS_URL}")
    
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        log_level="info"
    )