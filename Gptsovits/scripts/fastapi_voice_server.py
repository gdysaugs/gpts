#!/usr/bin/env python3
"""
FastAPI GPT-SoVITS å¸¸é§éŸ³å£°ç”Ÿæˆã‚µãƒ¼ãƒãƒ¼
åˆæœŸåŒ–1å›ã®ã¿ã€å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆ2-3ç§’ã§é«˜é€Ÿå¿œç­”
"""

import os
import sys
import time
import logging
import asyncio
from pathlib import Path
from typing import Optional
import uuid
import tempfile
import base64
import urllib.request

# FastAPI
from fastapi import FastAPI, HTTPException, UploadFile, File, Form, BackgroundTasks
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# ã‚·ãƒ³ãƒ—ãƒ«ãªasyncio.Lockä½¿ç”¨

# GPT-SoVITS
os.chdir('/app')
sys.path.insert(0, '/app')
sys.path.insert(0, '/app/GPT_SoVITS')

import torch
import soundfile as sf
import numpy as np

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
app = FastAPI(title="GPT-SoVITS Voice Cloning API", version="1.0.0")
MODELS_LOADED = False
CUSTOM_SOVITS_PATH = None

# ã‚·ãƒ³ãƒ—ãƒ«ãªGPUæ’ä»–åˆ¶å¾¡
gpu_lock: Optional[asyncio.Lock] = None

# ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
PRELOADED_LANGDETECT = None
CACHE_DIR = "/app/cache"

# CORSè¨­å®šï¼ˆé–‹ç™ºç”¨ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # æœ¬ç•ªã§ã¯é©åˆ‡ã«è¨­å®š
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
class VoiceCloneRequest(BaseModel):
    ref_text: str
    target_text: str
    ref_audio_base64: Optional[str] = None  # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰éŸ³å£°
    temperature: float = 1.0
    top_k: int = 5
    top_p: float = 1.0

class VoiceCloneResponse(BaseModel):
    success: bool
    message: str
    audio_base64: Optional[str] = None
    generation_time: float
    audio_duration: float
    realtime_factor: float

def setup_torch_optimizations():
    """Torchæœ€é©åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    if torch.cuda.is_available():
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        torch.cuda.empty_cache()
        logger.info("ğŸš€ RTX 3050 TensorCoreæœ€é©åŒ–æœ‰åŠ¹")
    
    torch.set_float32_matmul_precision('medium')
    torch.set_num_threads(8)
    logger.info("âœ… PyTorchæœ€é©åŒ–å®Œäº†")

def comprehensive_monkey_patch():
    """ãƒ¢ãƒ³ã‚­ãƒ¼ãƒ‘ãƒƒãƒé©ç”¨"""
    from GPT_SoVITS import inference_webui
    
    original_load_sovits = inference_webui.load_sovits_new
    original_change_sovits = inference_webui.change_sovits_weights
    
    def custom_load_sovits_new(sovits_path):
        global CUSTOM_SOVITS_PATH
        if CUSTOM_SOVITS_PATH and os.path.exists(CUSTOM_SOVITS_PATH):
            actual_path = CUSTOM_SOVITS_PATH
        else:
            actual_path = sovits_path
        
        if actual_path.endswith('.ckpt'):
            checkpoint = torch.load(actual_path, map_location='cpu')
            if 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint
            
            dict_s2 = {
                'weight': state_dict,
                'config': {
                    'model': {
                        'version': 'v2',
                        'semantic_frame_rate': '25hz',
                        'inter_channels': 192,
                        'hidden_channels': 192,
                        'filter_channels': 768,
                        'n_heads': 2,
                        'n_layers': 6,
                        'kernel_size': 3,
                        'p_dropout': 0.1,
                        'ssl_dim': 768,
                        'n_speakers': 300
                    },
                    'data': {
                        'sampling_rate': 32000,
                        'filter_length': 2048,
                        'hop_length': 640,
                        'win_length': 2048,
                        'n_speakers': 300,
                        'cleaned_text': True,
                        'add_blank': True,
                        'n_symbols': 178
                    }
                }
            }
            return dict_s2
        else:
            return original_load_sovits(actual_path)
    
    def custom_change_sovits_weights(sovits_path, prompt_language=None, text_language=None):
        global CUSTOM_SOVITS_PATH
        if CUSTOM_SOVITS_PATH:
            sovits_path = "/app/GPT_SoVITS/pretrained_models/gsv-v2final-pretrained/s2G2333k.pth"
        return original_change_sovits(sovits_path, prompt_language, text_language)
    
    inference_webui.load_sovits_new = custom_load_sovits_new
    inference_webui.change_sovits_weights = custom_change_sovits_weights
    logger.info("ğŸ”§ ãƒ¢ãƒ³ã‚­ãƒ¼ãƒ‘ãƒƒãƒé©ç”¨å®Œäº†")

def apply_torch_compile_optimization():
    """Torch.compileæœ€é©åŒ–é©ç”¨"""
    try:
        if not hasattr(torch, 'compile'):
            logger.warning("âš ï¸ PyTorch 2.0+ãŒå¿…è¦ï¼ˆTorch.compileéå¯¾å¿œï¼‰")
            return
        
        from GPT_SoVITS import inference_webui
        
        # SoVITSãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–
        if hasattr(inference_webui, 'vq_model') and inference_webui.vq_model is not None:
            logger.info("ğŸ”¥ SoVITSãƒ¢ãƒ‡ãƒ«compileæœ€é©åŒ–ä¸­...")
            inference_webui.vq_model = torch.compile(
                inference_webui.vq_model,
                mode="max-autotune",
                dynamic=True,
                backend="inductor"
            )
        
        # GPTãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–
        if hasattr(inference_webui, 't2s_model') and inference_webui.t2s_model is not None:
            logger.info("ğŸ”¥ GPTãƒ¢ãƒ‡ãƒ«compileæœ€é©åŒ–ä¸­...")
            inference_webui.t2s_model = torch.compile(
                inference_webui.t2s_model,
                mode="max-autotune",
                dynamic=True,
                backend="inductor"
            )
        
        # HuBERTãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–
        if hasattr(inference_webui, 'hubert_model') and inference_webui.hubert_model is not None:
            logger.info("ğŸ”¥ HuBERTãƒ¢ãƒ‡ãƒ«compileæœ€é©åŒ–ä¸­...")
            inference_webui.hubert_model = torch.compile(
                inference_webui.hubert_model,
                mode="reduce-overhead",
                dynamic=True,
                backend="inductor"
            )
        
        logger.info("ğŸš€ Torch.compileæœ€é©åŒ–å®Œäº†")
        
    except Exception as e:
        logger.error(f"âŒ Torch.compileæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")

async def initialize_models():
    """ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚1å›ã®ã¿ï¼‰"""
    global MODELS_LOADED, CUSTOM_SOVITS_PATH
    
    if MODELS_LOADED:
        return
    
    logger.info("ğŸš€ === ã‚µãƒ¼ãƒãƒ¼åˆæœŸåŒ–é–‹å§‹ ===")
    init_start = time.time()
    
    try:
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹è¨­å®š
        CUSTOM_SOVITS_PATH = "/app/GPT_SoVITS/pretrained_models/gpt-sovits-ja-h/hscene-e17.ckpt"
        
        # æœ€é©åŒ–è¨­å®š
        setup_torch_optimizations()
        comprehensive_monkey_patch()
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        from GPT_SoVITS.inference_webui import change_sovits_weights
        default_path = "/app/GPT_SoVITS/pretrained_models/gsv-v2final-pretrained/s2G2333k.pth"
        change_sovits_weights(default_path)
        
        # Warm-upæ¨è«–
        logger.info("ğŸ”¥ Warm-upæ¨è«–å®Ÿè¡Œä¸­...")
        from GPT_SoVITS.inference_webui import get_tts_wav
        
        dummy_gen = get_tts_wav(
            ref_wav_path="/app/input/reference_5sec.wav",
            prompt_text="ã“ã‚“ã«ã¡ã¯",
            prompt_language="Japanese",
            text="ãƒ†ã‚¹ãƒˆ",
            text_language="Japanese",
            how_to_cut="ä¸åˆ‡",
            top_k=5,
            top_p=1.0,
            temperature=1.0,
            ref_free=True
        )
        
        # ä¸€ã¤ã ã‘ç”Ÿæˆã—ã¦Warm-upå®Œäº†
        for i, item in enumerate(dummy_gen):
            if i == 0:
                break
        
        # Torch.compileæœ€é©åŒ–
        apply_torch_compile_optimization()
        
        # CUDAã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            torch.cuda.empty_cache()
        
        MODELS_LOADED = True
        init_time = time.time() - init_start
        
        logger.info(f"âœ… === ã‚µãƒ¼ãƒãƒ¼åˆæœŸåŒ–å®Œäº†: {init_time:.2f}ç§’ ===")
        logger.info("ğŸ¯ ä»¥é™ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯2-3ç§’ã§å¿œç­”äºˆå®š")
        
    except Exception as e:
        logger.error(f"âŒ ã‚µãƒ¼ãƒãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        raise

def ensure_text_length(text: str, min_length: int = 20) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆãŒæŒ‡å®šæ–‡å­—æ•°æœªæº€ã®å ´åˆã€è‡ªç„¶ã«å»¶é•·ã™ã‚‹"""
    if len(text) >= min_length:
        return text
    
    # çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã®å»¶é•·ãƒ‘ã‚¿ãƒ¼ãƒ³
    extensions = [
        "ã¨ã¦ã‚‚è‰¯ã„éŸ³å£°ã§ã™ã­ã€‚",
        "ç´ æ™´ã‚‰ã—ã„çµæœã ã¨æ€ã„ã¾ã™ã€‚",
        "éŸ³å£°ã‚¯ãƒ­ãƒ¼ãƒ‹ãƒ³ã‚°ã®æŠ€è¡“ã¯å‡„ã„ã§ã™ã€‚",
        "ã“ã‚Œã¯é«˜å“è³ªãªéŸ³å£°ç”Ÿæˆã§ã™ã€‚",
        "æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚"
    ]
    
    # æ–‡æœ«ã«å¥ç‚¹ãŒãªã„å ´åˆã¯è¿½åŠ 
    if not text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')):
        text += "ã€‚"
    
    # 20æ–‡å­—ä»¥ä¸Šã«ãªã‚‹ã¾ã§å»¶é•·
    while len(text) < min_length:
        # ä¸€ç•ªé©å½“ãªå»¶é•·ã‚’é¸æŠï¼ˆå¥ç‚¹ã®é‡è¤‡ã‚’é¿ã‘ã‚‹ï¼‰
        extension = extensions[len(text) % len(extensions)]
        text += extension
    
    logger.info(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆè‡ªå‹•å»¶é•·: {len(text)}æ–‡å­— â†’ {text}")
    return text

async def generate_voice_fast(ref_audio_path: str, ref_text: str, target_text: str, 
                             temperature: float = 1.0, top_k: int = 5, top_p: float = 1.0) -> dict:
    """é«˜é€ŸéŸ³å£°ç”Ÿæˆï¼ˆåˆæœŸåŒ–æ¸ˆã¿å‰æï¼‰"""
    
    if not MODELS_LOADED:
        raise HTTPException(status_code=500, detail="ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’20æ–‡å­—ä»¥ä¸Šã«è‡ªå‹•å»¶é•·
    # target_text = ensure_text_length(target_text, 20)  # è‡ªå‹•æ‹¡å¼µç„¡åŠ¹åŒ–
    
    generation_start = time.time()
    
    try:
        from GPT_SoVITS.inference_webui import get_tts_wav
        
        # éŸ³å£°ç”Ÿæˆå®Ÿè¡Œ
        result_generator = get_tts_wav(
            ref_wav_path=ref_audio_path,
            prompt_text=ref_text,
            prompt_language="Japanese",
            text=target_text,
            text_language="Japanese",
            how_to_cut="ä¸åˆ‡",
            top_k=top_k,
            top_p=top_p,
            temperature=temperature,
            ref_free=True
        )
        
        # çµæœå‡¦ç†
        audio_segments = []
        for i, item in enumerate(result_generator):
            if isinstance(item, tuple) and len(item) == 2:
                sample_rate, audio_data = item
                audio_segments.append(audio_data)
        
        if not audio_segments:
            raise Exception("éŸ³å£°ç”Ÿæˆå¤±æ•—")
        
        # éŸ³å£°é€£çµ
        if len(audio_segments) > 1:
            final_audio = np.concatenate(audio_segments)
        else:
            final_audio = audio_segments[0]
        
        generation_time = time.time() - generation_start
        audio_duration = len(final_audio) / 32000
        realtime_factor = audio_duration / generation_time
        
        # éŸ³å£°å“è³ªçµ±è¨ˆ
        audio_rms = float(np.sqrt(np.mean(final_audio ** 2)))
        non_silence_ratio = float(np.sum(np.abs(final_audio) > np.max(np.abs(final_audio)) * 0.01) / len(final_audio))
        
        return {
            'audio_data': final_audio,
            'sample_rate': 32000,
            'generation_time': generation_time,
            'audio_duration': audio_duration,
            'realtime_factor': realtime_factor,
            'audio_rms': audio_rms,
            'non_silence_ratio': non_silence_ratio
        }
        
    except Exception as e:
        logger.error(f"âŒ éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=f"éŸ³å£°ç”Ÿæˆå¤±æ•—: {str(e)}")

# === äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–¢æ•° ===

async def preload_all_dependencies():
    """å…¨ã¦ã®ä¾å­˜é–¢ä¿‚ã‚’äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    global PRELOADED_LANGDETECT
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs(CACHE_DIR, exist_ok=True)
    
    # 1. è¨€èªæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã®äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    logger.info("ğŸ”¥ è¨€èªæ¤œå‡ºãƒ¢ãƒ‡ãƒ«äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    try:
        langdetect_model_path = f"{CACHE_DIR}/lid.176.bin"
        if not os.path.exists(langdetect_model_path):
            logger.info("ğŸ“¥ è¨€èªæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            urllib.request.urlretrieve(
                "https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin",
                langdetect_model_path
            )
            logger.info("âœ… è¨€èªæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        else:
            logger.info("âœ… è¨€èªæ¤œå‡ºãƒ¢ãƒ‡ãƒ«æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨")
            
        # ç’°å¢ƒå¤‰æ•°ã§ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š
        os.environ["FASTTEXT_MODEL_PATH"] = langdetect_model_path
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰
        import fast_langdetect
        from fast_langdetect import detect
        
        # å¼·åˆ¶çš„ã«ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        detect("Hello", low_memory=False)
        detect("ã“ã‚“ã«ã¡ã¯", low_memory=False)
        PRELOADED_LANGDETECT = True
        
        logger.info("âœ… è¨€èªæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰å®Œäº†")
    except Exception as e:
        logger.warning(f"âš ï¸ è¨€èªæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
    
    # 2. Open JTalkè¾æ›¸ã®äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    logger.info("ğŸ”¥ Open JTalkè¾æ›¸äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    try:
        jtalk_dict_path = f"{CACHE_DIR}/open_jtalk_dic_utf_8-1.11.tar.gz"
        if not os.path.exists(jtalk_dict_path):
            logger.info("ğŸ“¥ Open JTalkè¾æ›¸ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            urllib.request.urlretrieve(
                "https://github.com/r9y9/open_jtalk/releases/download/v1.11.1/open_jtalk_dic_utf_8-1.11.tar.gz",
                jtalk_dict_path
            )
            logger.info("âœ… Open JTalkè¾æ›¸ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        else:
            logger.info("âœ… Open JTalkè¾æ›¸æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨")
            
        # ç’°å¢ƒå¤‰æ•°ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ã‚¹ã‚’è¨­å®š
        os.environ["OPEN_JTALK_DICT_PATH"] = jtalk_dict_path
        
        logger.info("âœ… Open JTalkè¾æ›¸ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰å®Œäº†")
    except Exception as e:
        logger.warning(f"âš ï¸ Open JTalkè¾æ›¸ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
    
    # 3. ãã®ä»–ã®ä¾å­˜é–¢ä¿‚ã®ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰
    logger.info("ğŸ”¥ ãã®ä»–ä¾å­˜é–¢ä¿‚ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰ä¸­...")
    try:
        # jiebaè¾æ›¸ã®ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰
        import jieba
        jieba.initialize()
        
        # TorchAudioå‘¨ã‚Šã®ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰
        import torchaudio
        torchaudio.set_audio_backend("sox_io")
        
        logger.info("âœ… ãã®ä»–ä¾å­˜é–¢ä¿‚ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰å®Œäº†")
    except Exception as e:
        logger.warning(f"âš ï¸ ãã®ä»–ä¾å­˜é–¢ä¿‚ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")

# === FastAPI ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ===

@app.on_event("startup")
async def startup_event():
    """ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚åˆæœŸåŒ–"""
    global gpu_lock
    try:
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        await initialize_models()
        
        # äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        await preload_all_dependencies()
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªasyncio.LockåˆæœŸåŒ–
        gpu_lock = asyncio.Lock()
        logger.info("âœ… AsyncIO GPU LockåˆæœŸåŒ–å®Œäº†ï¼")
        
    except Exception as e:
        logger.error(f"âŒ ã‚µãƒ¼ãƒãƒ¼åˆæœŸåŒ–å¤±æ•—: {e}")
        raise

@app.get("/")
async def root():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "message": "GPT-SoVITS Voice Cloning API",
        "models_loaded": MODELS_LOADED,
        "gpu_available": torch.cuda.is_available(),
        "gpu_name": torch.cuda.get_device_name() if torch.cuda.is_available() else None
    }

@app.get("/health")
async def health():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ï¼‰"""
    return {
        "status": "healthy" if MODELS_LOADED else "initializing",
        "models_loaded": MODELS_LOADED,
        "gpu_available": torch.cuda.is_available()
    }

@app.post("/warmup")
async def warmup():
    """ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã€‚ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ãƒ†ã‚¹ãƒˆéŸ³å£°ã‚’ç”Ÿæˆ"""
    try:
        logger.info("ğŸ”¥ SoVITS APIã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é–‹å§‹...")
        start_time = time.time()
        
        # ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„å ´åˆã¯åˆæœŸåŒ–
        if not MODELS_LOADED:
            await initialize_models()
        
        # ãƒ†ã‚¹ãƒˆéŸ³å£°ç”Ÿæˆã‚’å®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
        try:
            await asyncio.wait_for(gpu_lock.acquire(), timeout=1.0)
            try:
                logger.info("ğŸ¤ ãƒ†ã‚¹ãƒˆéŸ³å£°ç”Ÿæˆä¸­...")
                
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å‚ç…§éŸ³å£°ã‚’ä½¿ç”¨
                ref_audio_path = "/app/input/reference_5sec.wav"
                
                # ãƒ†ã‚¹ãƒˆéŸ³å£°ç”Ÿæˆ
                result = await generate_voice_fast(
                    ref_audio_path=ref_audio_path,
                    ref_text="ã“ã‚“ã«ã¡ã¯ã€èª¿å­ã¯ã©ã†ï¼Ÿ",
                    target_text="ãƒ†ã‚¹ãƒˆéŸ³å£°ã§ã™ã€‚",
                    temperature=1.0,
                    top_k=5,
                    top_p=1.0
                )
            finally:
                gpu_lock.release()
        except asyncio.TimeoutError:
            # GPUãƒ­ãƒƒã‚¯å–å¾—å¤±æ•—æ™‚ã¯ç›´æ¥å®Ÿè¡Œ
            logger.warning("âš ï¸ GPUãƒ­ãƒƒã‚¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - ç›´æ¥å®Ÿè¡Œ")
            logger.info("ğŸ¤ ãƒ†ã‚¹ãƒˆéŸ³å£°ç”Ÿæˆä¸­...")
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å‚ç…§éŸ³å£°ã‚’ä½¿ç”¨
            ref_audio_path = "/app/input/reference_5sec.wav"
            
            # ãƒ†ã‚¹ãƒˆéŸ³å£°ç”Ÿæˆ
            result = await generate_voice_fast(
                ref_audio_path=ref_audio_path,
                ref_text="ã“ã‚“ã«ã¡ã¯ã€èª¿å­ã¯ã©ã†ï¼Ÿ",
                target_text="ãƒ†ã‚¹ãƒˆéŸ³å£°ã§ã™ã€‚",
                temperature=1.0,
                top_k=5,
                top_p=1.0
            )
            
            warmup_time = time.time() - start_time
            logger.info(f"âœ… SoVITS APIã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†: {warmup_time:.2f}ç§’")
            
            return {
                "status": "success",
                "message": "SoVITSã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†",
                "warmup_time": warmup_time,
                "audio_duration": result['audio_duration'],
                "realtime_factor": result['realtime_factor'],
                "audio_rms": result['audio_rms'],
                "models_loaded": MODELS_LOADED
            }
            
    except Exception as e:
        logger.error(f"âŒ SoVITS APIã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=f"Warmup failed: {str(e)}")

@app.post("/clone-voice", response_model=VoiceCloneResponse)
async def clone_voice_endpoint(
    ref_text: str = Form(...),
    target_text: str = Form(...),
    temperature: float = Form(1.0),
    top_k: int = Form(5),
    top_p: float = Form(1.0),
    ref_audio: UploadFile = File(...)
):
    """éŸ³å£°ã‚¯ãƒ­ãƒ¼ãƒ‹ãƒ³ã‚°API"""
    
    try:
        # å‚ç…§éŸ³å£°ã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        temp_audio_path = f"/tmp/{uuid.uuid4()}.wav"
        
        with open(temp_audio_path, "wb") as f:
            f.write(await ref_audio.read())
        
        # éŸ³å£°ç”Ÿæˆ
        result = await generate_voice_fast(
            ref_audio_path=temp_audio_path,
            ref_text=ref_text,
            target_text=target_text,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p
        )
        
        # ç”ŸæˆéŸ³å£°ã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        output_path = f"/tmp/{uuid.uuid4()}_output.wav"
        sf.write(output_path, result['audio_data'], result['sample_rate'])
        
        # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        with open(output_path, "rb") as f:
            audio_base64 = base64.b64encode(f.read()).decode('utf-8')
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        os.remove(temp_audio_path)
        os.remove(output_path)
        
        return VoiceCloneResponse(
            success=True,
            message="éŸ³å£°ç”ŸæˆæˆåŠŸ",
            audio_base64=audio_base64,
            generation_time=result['generation_time'],
            audio_duration=result['audio_duration'],
            realtime_factor=result['realtime_factor']
        )
        
    except Exception as e:
        logger.error(f"âŒ API ã‚¨ãƒ©ãƒ¼: {e}")
        return VoiceCloneResponse(
            success=False,
            message=f"ã‚¨ãƒ©ãƒ¼: {str(e)}",
            generation_time=0,
            audio_duration=0,
            realtime_factor=0
        )

@app.post("/clone-voice-simple")
async def clone_voice_simple(
    ref_text: str = Form(...),
    target_text: str = Form(...),
    temperature: float = Form(1.0),
    ref_audio: UploadFile = File(...),
    background_tasks: BackgroundTasks = BackgroundTasks()
):
    """ã‚·ãƒ³ãƒ—ãƒ«éŸ³å£°ã‚¯ãƒ­ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‚ç…§éŸ³å£°ãƒ»GPUæ’ä»–åˆ¶å¾¡ä»˜ãï¼‰"""
    
    # è»½é‡GPUæ’ä»–åˆ¶å¾¡ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
    try:
        await asyncio.wait_for(gpu_lock.acquire(), timeout=1.0)
        try:
            return await _execute_voice_synthesis_with_ref(ref_text, target_text, temperature, ref_audio, background_tasks)
        finally:
            gpu_lock.release()
    except asyncio.TimeoutError:
        # GPUãƒ­ãƒƒã‚¯å–å¾—å¤±æ•—æ™‚ã¯ç›´æ¥å®Ÿè¡Œ
        logger.warning("âš ï¸ GPUãƒ­ãƒƒã‚¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - ç›´æ¥å®Ÿè¡Œ")
        return await _execute_voice_synthesis_with_ref(ref_text, target_text, temperature, ref_audio, background_tasks)


async def _execute_voice_synthesis_with_ref(ref_text: str, target_text: str, temperature: float, ref_audio: UploadFile, background_tasks: BackgroundTasks):
    """GPUå‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹å†…éƒ¨é–¢æ•°ï¼ˆå‚ç…§éŸ³å£°ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾å¿œï¼‰"""
    try:
        # å‚ç…§éŸ³å£°ã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        temp_ref_path = f"/tmp/{uuid.uuid4()}_ref.wav"
        with open(temp_ref_path, "wb") as f:
            content = await ref_audio.read()
            f.write(content)
        
        result = await generate_voice_fast(
            ref_audio_path=temp_ref_path,
            ref_text=ref_text,
            target_text=target_text,
            temperature=temperature
        )
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        temp_output_path = f"/tmp/{uuid.uuid4()}_output.wav"
        sf.write(temp_output_path, result['audio_data'], result['sample_rate'])
        
        # outputãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚‚æ°¸ç¶šä¿å­˜
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_target = target_text.replace(' ', '_').replace('ï¼', '').replace('ï¼Ÿ', '')[:30]
        permanent_path = f"/app/output/fastapi_{timestamp}_{safe_target}.wav"
        sf.write(permanent_path, result['audio_data'], result['sample_rate'])
        
        # é€ä¿¡å¾Œã«ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã«è¿½åŠ 
        background_tasks.add_task(os.remove, temp_output_path)
        background_tasks.add_task(os.remove, temp_ref_path)
        
        return FileResponse(
            temp_output_path,
            media_type="audio/wav",
            filename="generated_voice.wav"
        )
            
    except Exception as e:
        logger.error(f"âŒ éŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    # ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
    uvicorn.run(
        "fastapi_voice_server:app",
        host="0.0.0.0",
        port=8000,
        reload=False,
        log_level="info"
    )
@app.post("/clear-memory")
async def clear_gpu_memory():
    """GPU ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾"""
    try:
        import gc
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
        return {"status": "success", "message": "GPU memory cleared"}
    except Exception as e:
        return {"status": "error", "message": str(e)}
