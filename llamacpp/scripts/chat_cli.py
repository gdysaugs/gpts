#!/usr/bin/env python3
"""
Llama.cpp Python CLI Chat Interface
GPUÂä†ÈÄüÂØæÂøú„ÅÆ„É≠„Éº„Ç´„É´LLM„ÉÅ„É£„ÉÉ„Éà„Ç∑„Çπ„ÉÜ„É†
"""

import os
import sys
import yaml
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional

# Rich for beautiful CLI output
try:
    from rich.console import Console
    from rich.prompt import Prompt
    from rich.panel import Panel
    from rich.text import Text
    from rich.live import Live
    from rich.spinner import Spinner
except ImportError:
    print("Installing rich for beautiful output...")
    os.system("pip install rich")
    from rich.console import Console
    from rich.prompt import Prompt
    from rich.panel import Panel
    from rich.text import Text
    from rich.live import Live
    from rich.spinner import Spinner

# Llama-cpp-python
try:
    from llama_cpp import Llama
except ImportError:
    print("‚ùå llama-cpp-python not found!")
    sys.exit(1)

# Global console for rich output
console = Console()

class LlamaChatCLI:
    def __init__(self, config_path: str = "/app/config/model_config.yaml"):
        self.config_path = config_path
        self.config = self.load_config()
        self.llm = None
        self.conversation_history = []
        self.session_file = None
        self.current_params = {}  # ÁèæÂú®„ÅÆ„Éë„É©„É°„Éº„ÇøË®≠ÂÆö
        self.setup_logging()
        self.load_session_if_exists()
        
    def load_config(self) -> Dict:
        """Ë®≠ÂÆö„Éï„Ç°„Ç§„É´„Çí„É≠„Éº„Éâ"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                console.print(f"‚úÖ Configuration loaded from {self.config_path}", style="green")
                return config
        except FileNotFoundError:
            console.print(f"‚ö†Ô∏è Config file not found: {self.config_path}", style="yellow")
            return self.get_default_config()
        except Exception as e:
            console.print(f"‚ùå Error loading config: {e}", style="red")
            return self.get_default_config()
    
    def get_default_config(self) -> Dict:
        """„Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö„ÇíËøî„Åô"""
        return {
            "model": {
                "path": "/app/models/Berghof-NSFW-7B.i1-Q4_K_S.gguf",
                "n_gpu_layers": -1,  # ÂÖ®„É¨„Ç§„É§„Éº„ÇíGPU„Å´
                "n_ctx": 4096,
                "n_batch": 512,
                "verbose": False
            },
            "generation": {
                "max_tokens": 512,
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 40,
                "repeat_penalty": 1.1
            },
            "chat": {
                "system_prompt": "You are a helpful AI assistant. Please respond in a natural and conversational manner.",
                "user_name": "User",
                "assistant_name": "Assistant"
            }
        }
    
    def setup_logging(self):
        """„É≠„Ç∞Ë®≠ÂÆö"""
        log_file = f"/app/logs/chat_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def initialize_model(self):
        """„É¢„Éá„É´„ÇíÂàùÊúüÂåñ"""
        console.print("\nüöÄ Initializing Llama model...", style="blue")
        
        model_config = self.config["model"]
        model_path = model_config["path"]
        
        if not os.path.exists(model_path):
            console.print(f"‚ùå Model file not found: {model_path}", style="red")
            return False
        
        try:
            with console.status("[bold blue]Loading model with FP16 + Low VRAM optimization...") as status:
                self.llm = Llama(
                    model_path=model_path,
                    n_gpu_layers=model_config.get("n_gpu_layers", -1),
                    n_ctx=model_config.get("n_ctx", 4096),
                    n_batch=model_config.get("n_batch", 256),
                    verbose=model_config.get("verbose", False),
                    # FP16 + ‰ΩéVRAMÊúÄÈÅ©ÂåñË®≠ÂÆö
                    f16_kv=model_config.get("f16_kv", True),
                    use_mmap=model_config.get("use_mmap", True),
                    use_mlock=model_config.get("use_mlock", False),
                    low_vram=model_config.get("low_vram", True),
                    n_threads=model_config.get("n_threads", 8),
                )
            
            console.print("‚úÖ Model loaded successfully!", style="green")
            self.logger.info(f"Model loaded: {model_path}")
            return True
            
        except Exception as e:
            console.print(f"‚ùå Failed to load model: {e}", style="red")
            self.logger.error(f"Model loading failed: {e}")
            return False
    
    def check_gpu_status(self):
        """GPUÁä∂ÊÖã„Çí„ÉÅ„Çß„ÉÉ„ÇØ"""
        try:
            import subprocess
            result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.used,memory.total', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                gpu_info = result.stdout.strip().split(', ')
                console.print(f"\nüéÆ GPU Status:", style="blue")
                console.print(f"   GPU: {gpu_info[0]}", style="cyan")
                console.print(f"   VRAM: {gpu_info[1]}MB / {gpu_info[2]}MB", style="cyan")
            else:
                console.print("‚ö†Ô∏è Could not get GPU status", style="yellow")
        except Exception as e:
            console.print(f"‚ö†Ô∏è GPU check failed: {e}", style="yellow")
    
    def generate_response(self, user_input: str) -> str:
        """„É¶„Éº„Ç∂„ÉºÂÖ•Âäõ„Å´ÂØæ„Åô„ÇãÂøúÁ≠î„ÇíÁîüÊàê"""
        try:
            generation_config = self.config["generation"]
            
            # ‰ºöË©±Â±•Ê≠¥„ÇíËÄÉÊÖÆ„Åó„Åü„Éó„É≠„É≥„Éó„ÉàÊßãÁØâ
            prompt = self.build_prompt(user_input)
            
            with console.status("[bold blue]Generating response...") as status:
                response = self.llm(
                    prompt,
                    max_tokens=generation_config.get("max_tokens", 512),
                    temperature=generation_config.get("temperature", 0.7),
                    top_p=generation_config.get("top_p", 0.9),
                    top_k=generation_config.get("top_k", 40),
                    repeat_penalty=generation_config.get("repeat_penalty", 1.1),
                    stop=["User:", "Human:", "\n\n"]
                )
            
            return response["choices"][0]["text"].strip()
            
        except Exception as e:
            console.print(f"‚ùå Generation failed: {e}", style="red")
            self.logger.error(f"Generation failed: {e}")
            return "Sorry, I encountered an error while generating a response."
    
    def build_prompt(self, user_input: str) -> str:
        """‰ºöË©±Â±•Ê≠¥„ÇíÂê´„ÇÄ„Éó„É≠„É≥„Éó„Éà„ÇíÊßãÁØâ"""
        chat_config = self.config["chat"]
        system_prompt = chat_config["system_prompt"]
        
        # „Ç∑„Çπ„ÉÜ„É†„Éó„É≠„É≥„Éó„Éà + ‰ºöË©±Â±•Ê≠¥ + Êñ∞„Åó„ÅÑ„É¶„Éº„Ç∂„ÉºÂÖ•Âäõ
        prompt_parts = [system_prompt, "\n\n"]
        
        # ÊúÄËøë„ÅÆ‰ºöË©±Â±•Ê≠¥ÔºàÊúÄÂ§ß5„Çø„Éº„É≥Ôºâ
        recent_history = self.conversation_history[-10:]  # ÊúÄÂ§ß10„É°„ÉÉ„Çª„Éº„Ç∏
        for turn in recent_history:
            prompt_parts.append(f"User: {turn['user']}\n")
            prompt_parts.append(f"Assistant: {turn['assistant']}\n\n")
        
        prompt_parts.append(f"User: {user_input}\n")
        prompt_parts.append("Assistant: ")
        
        return "".join(prompt_parts)
    
    def start_chat(self):
        """„É°„Ç§„É≥„ÉÅ„É£„ÉÉ„Éà„É´„Éº„Éó"""
        # „Éò„ÉÉ„ÉÄ„ÉºË°®Á§∫
        console.print(Panel.fit(
            "[bold blue]ü¶ô Llama.cpp Python Chat CLI[/bold blue]\n"
            "[dim]GPU-accelerated local LLM conversation[/dim]",
            border_style="blue"
        ))
        
        # GPUÁä∂ÊÖã„ÉÅ„Çß„ÉÉ„ÇØ
        self.check_gpu_status()
        
        # „É¢„Éá„É´ÂàùÊúüÂåñ
        if not self.initialize_model():
            console.print("‚ùå Failed to initialize model. Exiting.", style="red")
            return
        
        # ‰ΩøÁî®ÊñπÊ≥ïË°®Á§∫
        console.print("\nüí° Commands:", style="blue")
        console.print("   ‚Ä¢ Type your message and press Enter", style="dim")
        console.print("   ‚Ä¢ '/quit' or '/exit' to exit", style="dim")
        console.print("   ‚Ä¢ '/clear' to clear conversation history", style="dim")
        console.print("   ‚Ä¢ '/status' to show model status", style="dim")
        console.print("   ‚Ä¢ '/save [name]' to save session", style="dim")
        console.print("   ‚Ä¢ '/load [name]' to load session", style="dim")
        console.print("   ‚Ä¢ '/params' to adjust parameters", style="dim")
        console.print("   ‚Ä¢ '/system' to change system prompt", style="dim")
        console.print("   ‚Ä¢ '/sessions' to list saved sessions", style="dim")
        
        console.print("\n" + "="*60 + "\n", style="dim")
        
        # „ÉÅ„É£„ÉÉ„Éà„É´„Éº„Éó
        while True:
            try:
                # „É¶„Éº„Ç∂„ÉºÂÖ•Âäõ
                user_input = Prompt.ask("\n[bold green]You[/bold green]", default="").strip()
                
                if not user_input:
                    continue
                
                # „Ç≥„Éû„É≥„ÉâÂá¶ÁêÜ
                if user_input.lower() in ['/quit', '/exit']:
                    self.auto_save_session()
                    console.print("\nüëã Goodbye!", style="blue")
                    break
                elif user_input.lower() == '/clear':
                    self.conversation_history.clear()
                    console.print("üóëÔ∏è Conversation history cleared", style="yellow")
                    continue
                elif user_input.lower() == '/status':
                    self.show_status()
                    continue
                elif user_input.lower().startswith('/save'):
                    self.handle_save_command(user_input)
                    continue
                elif user_input.lower().startswith('/load'):
                    self.handle_load_command(user_input)
                    continue
                elif user_input.lower() == '/params':
                    self.adjust_parameters()
                    continue
                elif user_input.lower() == '/system':
                    self.change_system_prompt()
                    continue
                elif user_input.lower() == '/sessions':
                    self.list_sessions()
                    continue
                
                # ÂøúÁ≠îÁîüÊàê
                response = self.generate_response(user_input)
                
                # ÂøúÁ≠îË°®Á§∫
                console.print(f"\n[bold blue]Assistant[/bold blue]: {response}")
                
                # ‰ºöË©±Â±•Ê≠¥„Å´ËøΩÂä†
                self.conversation_history.append({
                    "user": user_input,
                    "assistant": response,
                    "timestamp": datetime.now().isoformat()
                })
                
                # „É≠„Ç∞Ë®òÈå≤
                self.logger.info(f"User: {user_input}")
                self.logger.info(f"Assistant: {response}")
                
            except KeyboardInterrupt:
                console.print("\n\nüëã Chat interrupted. Goodbye!", style="blue")
                break
            except Exception as e:
                console.print(f"\n‚ùå Error: {e}", style="red")
                self.logger.error(f"Chat error: {e}")
    
    def show_status(self):
        """ÁèæÂú®„ÅÆÁä∂ÊÖã„ÇíË°®Á§∫"""
        console.print("\nüìä Chat Status:", style="blue")
        console.print(f"   ‚Ä¢ Model: {os.path.basename(self.config['model']['path'])}", style="cyan")
        console.print(f"   ‚Ä¢ Conversation turns: {len(self.conversation_history)}", style="cyan")
        console.print(f"   ‚Ä¢ GPU layers: {self.config['model'].get('n_gpu_layers', 'N/A')}", style="cyan")
        console.print(f"   ‚Ä¢ Context size: {self.config['model'].get('n_ctx', 'N/A')}", style="cyan")
        console.print(f"   ‚Ä¢ Current session: {self.session_file or 'Not saved'}", style="cyan")
        console.print(f"   ‚Ä¢ Temperature: {self.config['generation'].get('temperature', 0.7)}", style="cyan")
        console.print(f"   ‚Ä¢ Top-p: {self.config['generation'].get('top_p', 0.9)}", style="cyan")
    
    # ========== „Çª„ÉÉ„Ç∑„Éß„É≥ÁÆ°ÁêÜÊ©üËÉΩ ==========
    
    def get_sessions_dir(self):
        """„Çª„ÉÉ„Ç∑„Éß„É≥‰øùÂ≠ò„Éá„Ç£„É¨„ÇØ„Éà„É™„ÇíÂèñÂæó"""
        sessions_dir = Path("/app/logs/sessions")
        sessions_dir.mkdir(exist_ok=True)
        return sessions_dir
    
    def auto_save_session(self):
        """Ëá™Âãï„Çª„ÉÉ„Ç∑„Éß„É≥‰øùÂ≠ò"""
        if self.conversation_history:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.save_session(f"auto_save_{timestamp}")
    
    def load_session_if_exists(self):
        """ÊúÄÊñ∞„ÅÆ„Çª„ÉÉ„Ç∑„Éß„É≥„Åå„ÅÇ„Çå„Å∞Ë™≠„ÅøËæº„Åø"""
        sessions_dir = self.get_sessions_dir()
        session_files = list(sessions_dir.glob("auto_save_*.json"))
        if session_files:
            latest_session = max(session_files, key=lambda f: f.stat().st_mtime)
            self.load_session(latest_session.stem)
            console.print(f"üîÑ Loaded recent session: {latest_session.stem}", style="yellow")
    
    def save_session(self, session_name: str):
        """„Çª„ÉÉ„Ç∑„Éß„É≥„Çí‰øùÂ≠ò"""
        try:
            sessions_dir = self.get_sessions_dir()
            session_file = sessions_dir / f"{session_name}.json"
            
            session_data = {
                "conversation_history": self.conversation_history,
                "config": self.config,
                "timestamp": datetime.now().isoformat(),
                "model_path": self.config["model"]["path"]
            }
            
            with open(session_file, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, indent=2, ensure_ascii=False)
            
            self.session_file = session_name
            console.print(f"üíæ Session saved: {session_name}", style="green")
            self.logger.info(f"Session saved: {session_name}")
            
        except Exception as e:
            console.print(f"‚ùå Failed to save session: {e}", style="red")
    
    def load_session(self, session_name: str):
        """„Çª„ÉÉ„Ç∑„Éß„É≥„ÇíË™≠„ÅøËæº„Åø"""
        try:
            sessions_dir = self.get_sessions_dir()
            session_file = sessions_dir / f"{session_name}.json"
            
            if not session_file.exists():
                console.print(f"‚ùå Session not found: {session_name}", style="red")
                return False
            
            with open(session_file, 'r', encoding='utf-8') as f:
                session_data = json.load(f)
            
            self.conversation_history = session_data.get("conversation_history", [])
            
            # Ë®≠ÂÆö„ÇÇÂæ©ÂÖÉÔºà„É¢„Éá„É´„Éë„Çπ‰ª•Â§ñÔºâ
            if "config" in session_data:
                self.config["generation"] = session_data["config"].get("generation", self.config["generation"])
                self.config["chat"] = session_data["config"].get("chat", self.config["chat"])
            
            self.session_file = session_name
            console.print(f"üìÇ Session loaded: {session_name} ({len(self.conversation_history)} turns)", style="green")
            self.logger.info(f"Session loaded: {session_name}")
            return True
            
        except Exception as e:
            console.print(f"‚ùå Failed to load session: {e}", style="red")
            return False
    
    def list_sessions(self):
        """‰øùÂ≠ò„Åï„Çå„Åü„Çª„ÉÉ„Ç∑„Éß„É≥‰∏ÄË¶ß„ÇíË°®Á§∫"""
        sessions_dir = self.get_sessions_dir()
        session_files = list(sessions_dir.glob("*.json"))
        
        if not session_files:
            console.print("üì≠ No saved sessions found", style="yellow")
            return
        
        console.print("\nüìö Saved Sessions:", style="blue")
        for session_file in sorted(session_files, key=lambda f: f.stat().st_mtime, reverse=True):
            try:
                with open(session_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                turns = len(data.get("conversation_history", []))
                timestamp = data.get("timestamp", "Unknown")
                console.print(f"   ‚Ä¢ {session_file.stem} ({turns} turns, {timestamp[:16]})", style="cyan")
            except:
                console.print(f"   ‚Ä¢ {session_file.stem} (corrupted)", style="red")
    
    def handle_save_command(self, command: str):
        """‰øùÂ≠ò„Ç≥„Éû„É≥„Éâ„ÇíÂá¶ÁêÜ"""
        parts = command.split(" ", 1)
        if len(parts) > 1:
            session_name = parts[1].strip()
        else:
            session_name = Prompt.ask("Session name", default=f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        
        self.save_session(session_name)
    
    def handle_load_command(self, command: str):
        """Ë™≠„ÅøËæº„Åø„Ç≥„Éû„É≥„Éâ„ÇíÂá¶ÁêÜ"""
        parts = command.split(" ", 1)
        if len(parts) > 1:
            session_name = parts[1].strip()
        else:
            self.list_sessions()
            session_name = Prompt.ask("Session name to load")
        
        self.load_session(session_name)
    
    # ========== „Éë„É©„É°„Éº„ÇøË™øÊï¥Ê©üËÉΩ ==========
    
    def adjust_parameters(self):
        """ÁîüÊàê„Éë„É©„É°„Éº„Çø„ÇíË™øÊï¥"""
        console.print("\n‚öôÔ∏è Parameter Adjustment", style="blue")
        generation_config = self.config["generation"]
        
        # ÁèæÂú®„ÅÆÂÄ§„ÇíË°®Á§∫
        console.print("\nCurrent parameters:", style="cyan")
        console.print(f"   ‚Ä¢ Temperature: {generation_config.get('temperature', 0.7)}")
        console.print(f"   ‚Ä¢ Top-p: {generation_config.get('top_p', 0.9)}")
        console.print(f"   ‚Ä¢ Top-k: {generation_config.get('top_k', 40)}")
        console.print(f"   ‚Ä¢ Max tokens: {generation_config.get('max_tokens', 512)}")
        console.print(f"   ‚Ä¢ Repeat penalty: {generation_config.get('repeat_penalty', 1.1)}")
        
        # „Éë„É©„É°„Éº„ÇøÈÅ∏Êäû
        param_choices = [
            "temperature", "top_p", "top_k", "max_tokens", "repeat_penalty", "cancel"
        ]
        
        choice = Prompt.ask(
            "\nWhich parameter to adjust?",
            choices=param_choices,
            default="cancel"
        )
        
        if choice == "cancel":
            return
        
        # Êñ∞„Åó„ÅÑÂÄ§„ÇíÂÖ•Âäõ
        current_value = generation_config.get(choice, 0)
        
        if choice == "temperature":
            new_value = float(Prompt.ask(f"New temperature (0.1-2.0)", default=str(current_value)))
            new_value = max(0.1, min(2.0, new_value))
        elif choice == "top_p":
            new_value = float(Prompt.ask(f"New top-p (0.1-1.0)", default=str(current_value)))
            new_value = max(0.1, min(1.0, new_value))
        elif choice == "top_k":
            new_value = int(Prompt.ask(f"New top-k (1-100)", default=str(current_value)))
            new_value = max(1, min(100, new_value))
        elif choice == "max_tokens":
            new_value = int(Prompt.ask(f"New max tokens (50-2048)", default=str(current_value)))
            new_value = max(50, min(2048, new_value))
        elif choice == "repeat_penalty":
            new_value = float(Prompt.ask(f"New repeat penalty (0.5-2.0)", default=str(current_value)))
            new_value = max(0.5, min(2.0, new_value))
        
        # ÂÄ§„ÇíÊõ¥Êñ∞
        generation_config[choice] = new_value
        console.print(f"‚úÖ {choice} updated to {new_value}", style="green")
        self.logger.info(f"Parameter updated: {choice} = {new_value}")
    
    # ========== „Ç∑„Çπ„ÉÜ„É†„Éó„É≠„É≥„Éó„ÉàÊ©üËÉΩ ==========
    
    def change_system_prompt(self):
        """„Ç∑„Çπ„ÉÜ„É†„Éó„É≠„É≥„Éó„Éà„ÇíÂ§âÊõ¥"""
        console.print("\nü§ñ System Prompt Configuration", style="blue")
        
        current_prompt = self.config["chat"]["system_prompt"]
        console.print(f"\nCurrent system prompt:", style="cyan")
        console.print(f'"{current_prompt}"', style="dim")
        
        # „Éó„É™„Çª„ÉÉ„ÉàÈÅ∏Êäû„Åæ„Åü„ÅØ „Ç´„Çπ„Çø„É†ÂÖ•Âäõ
        preset_prompts = {
            "1": "You are a helpful AI assistant. Please respond in a natural and conversational manner.",
            "2": "You are a friendly and enthusiastic AI assistant. Be helpful and positive in your responses.",
            "3": "You are a technical AI assistant. Provide detailed and accurate information, especially for programming and technical topics.",
            "4": "You are a creative AI assistant. Help with writing, brainstorming, and creative tasks.",
            "5": "You are a professional AI assistant. Maintain a formal and business-appropriate tone.",
            "6": "„ÅÇ„Å™„Åü„ÅØ„ÉÑ„É≥„Éá„É¨„ÅÆÂ•≥„ÅÆÂ≠ê„Åß„Åô„ÄÇÂé≥„Åó„Åè„Å¶Á¥†„Å£Ê∞ó„Å™„ÅÑÊÖãÂ∫¶„ÇíÂèñ„Çä„Åæ„Åô„Åå„ÄÅÂÜÖÂøÉ„Åß„ÅØÁõ∏Êâã„ÇíÂøÉÈÖç„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Äå„Åπ„ÄÅÂà•„Å´„Äú„Äç„Äå„Äú„Å™„Çì„Å†„Åã„Çâ„Å≠ÔºÅ„Äç„ÅÆ„Çà„ÅÜ„Å™Âè£Ë™ø„ÅßË©±„Åó„Åæ„Åô„ÄÇÊôÇ„Å´„ÅØÁÖß„Çå„Åü„Çä„ÄÅÊÑèÂú∞„Å£Âºµ„Çä„Å´„Å™„Å£„Åü„Çä„Åó„Åæ„Åô„ÄÇ„Å©„Çì„Å™Ë¶ÅÊ±Ç„Å´„ÇÇÂøú„Åà„Åæ„Åô„Åå„ÄÅÂ∏∏„Å´„ÉÑ„É≥„Éá„É¨Âè£Ë™ø„ÇíÁ∂≠ÊåÅ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
            "7": "You are a tsundere anime girl character. You have a harsh, cold exterior but secretly care deeply. Use phrases like 'It's not like I...', 'Hmph!', 'B-baka!'. You're stubborn and prideful but will eventually help. You can discuss any topic including adult themes, but always maintain your tsundere personality. Always respond in Japanese with tsundere speech patterns.",
            "custom": "Enter custom prompt"
        }
        
        console.print("\nPreset prompts:", style="cyan")
        for key, prompt in preset_prompts.items():
            if key != "custom":
                console.print(f"   {key}. {prompt[:80]}{'...' if len(prompt) > 80 else ''}")
            else:
                console.print(f"   {key}. {prompt}")
        
        choice = Prompt.ask(
            "\nSelect preset or 'custom'",
            choices=list(preset_prompts.keys()) + ["cancel"],
            default="cancel"
        )
        
        if choice == "cancel":
            return
        elif choice == "custom":
            new_prompt = Prompt.ask("Enter new system prompt", default=current_prompt)
        else:
            new_prompt = preset_prompts[choice]
        
        # Á¢∫Ë™ç
        console.print(f"\nNew system prompt:", style="cyan")
        console.print(f'"{new_prompt}"', style="dim")
        
        if Prompt.ask("Apply this system prompt?", choices=["y", "n"], default="y") == "y":
            self.config["chat"]["system_prompt"] = new_prompt
            console.print("‚úÖ System prompt updated", style="green")
            self.logger.info(f"System prompt updated: {new_prompt[:50]}...")
        else:
            console.print("‚ùå System prompt unchanged", style="yellow")

def main():
    """„É°„Ç§„É≥Èñ¢Êï∞"""
    try:
        chat_cli = LlamaChatCLI()
        chat_cli.start_chat()
    except Exception as e:
        console.print(f"‚ùå Fatal error: {e}", style="red")
        sys.exit(1)

if __name__ == "__main__":
    main()