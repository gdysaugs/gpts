#!/usr/bin/env python3
"""
Llama.cpp Model Test Script
ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬å‹•ä½œã¨GPUèªè­˜ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹
"""

import os
import sys
import time
from pathlib import Path

try:
    from llama_cpp import Llama
except ImportError:
    print("âŒ llama-cpp-python not installed!")
    sys.exit(1)

def test_gpu_availability():
    """GPUä½¿ç”¨å¯èƒ½æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ” Checking GPU availability...")
    
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi'], capture_output=True)
        if result.returncode == 0:
            print("âœ… NVIDIA GPU detected")
            
            # GPUè©³ç´°æƒ…å ±
            gpu_info = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], 
                                    capture_output=True, text=True)
            if gpu_info.returncode == 0:
                print(f"   GPU Info: {gpu_info.stdout.strip()}")
            return True
        else:
            print("âŒ nvidia-smi failed")
            return False
    except FileNotFoundError:
        print("âŒ nvidia-smi not found")
        return False
    except Exception as e:
        print(f"âŒ GPU check error: {e}")
        return False

def test_model_loading():
    """ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ¦™ Testing model loading...")
    
    model_path = "/app/models/Berghof-NSFW-7B.i1-Q4_K_S.gguf"
    
    if not os.path.exists(model_path):
        print(f"âŒ Model file not found: {model_path}")
        return False
    
    print(f"ğŸ“ Model file found: {Path(model_path).name}")
    print(f"   Size: {os.path.getsize(model_path) / (1024**3):.2f} GB")
    
    try:
        print("â³ Loading model...")
        start_time = time.time()
        
        llm = Llama(
            model_path=model_path,
            n_gpu_layers=-1,  # å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’GPUã«
            n_ctx=1024,       # ãƒ†ã‚¹ãƒˆç”¨ã«å°ã•ã‚
            n_batch=256,      # ä½VRAMæœ€é©åŒ–
            verbose=False,
            # FP16 + ä½VRAMæœ€é©åŒ–è¨­å®š
            f16_kv=True,      # FP16ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            use_mmap=True,    # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ”ãƒ³ã‚°æœ‰åŠ¹
            use_mlock=False,  # ä½VRAMæ™‚ã¯ç„¡åŠ¹
            low_vram=True,    # ä½VRAMæœ€é©åŒ–
            n_threads=8,      # CPUè£œåŠ©ã‚¹ãƒ¬ãƒƒãƒ‰
        )
        
        load_time = time.time() - start_time
        print(f"âœ… Model loaded successfully in {load_time:.2f} seconds")
        
        return llm
        
    except Exception as e:
        print(f"âŒ Model loading failed: {e}")
        return None

def test_inference(llm):
    """æ¨è«–ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ§  Testing inference...")
    
    test_prompt = "Hello! How are you today?"
    print(f"ğŸ“ Prompt: {test_prompt}")
    
    try:
        start_time = time.time()
        
        response = llm(
            test_prompt,
            max_tokens=50,
            temperature=0.7,
            stop=["\n"]
        )
        
        inference_time = time.time() - start_time
        response_text = response["choices"][0]["text"].strip()
        
        print(f"ğŸ¤– Response: {response_text}")
        print(f"â±ï¸ Inference time: {inference_time:.2f} seconds")
        print(f"ğŸ“Š Tokens: {len(response_text.split())} words")
        
        return True
        
    except Exception as e:
        print(f"âŒ Inference failed: {e}")
        return False

def test_gpu_memory():
    """GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒã‚§ãƒƒã‚¯"""
    print("\nğŸ’¾ Checking GPU memory usage...")
    
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            memory_info = result.stdout.strip().split(', ')
            used_memory = int(memory_info[0])
            total_memory = int(memory_info[1])
            usage_percent = (used_memory / total_memory) * 100
            
            print(f"   Used: {used_memory} MB / {total_memory} MB ({usage_percent:.1f}%)")
            
            if usage_percent > 90:
                print("âš ï¸ High GPU memory usage!")
            elif usage_percent > 50:
                print("âœ… Moderate GPU memory usage")
            else:
                print("âœ… Low GPU memory usage")
                
        else:
            print("âŒ Could not get GPU memory info")
            
    except Exception as e:
        print(f"âŒ GPU memory check failed: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆé–¢æ•°"""
    print("ğŸš€ Llama.cpp Python Model Test")
    print("=" * 50)
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    tests_passed = 0
    total_tests = 3
    
    # 1. GPU ãƒ†ã‚¹ãƒˆ
    if test_gpu_availability():
        tests_passed += 1
    
    # 2. ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ
    llm = test_model_loading()
    if llm:
        tests_passed += 1
        
        # 3. æ¨è«–ãƒ†ã‚¹ãƒˆ
        if test_inference(llm):
            tests_passed += 1
    
    # GPU ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
    test_gpu_memory()
    
    # çµæœè¡¨ç¤º
    print("\n" + "=" * 50)
    print(f"ğŸ“Š Test Results: {tests_passed}/{total_tests} tests passed")
    
    if tests_passed == total_tests:
        print("ğŸ‰ All tests passed! System is ready for chat.")
        return True
    else:
        print("âŒ Some tests failed. Please check the configuration.")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)