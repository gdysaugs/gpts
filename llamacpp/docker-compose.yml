version: '3.8'

services:
  llama-chat:
    build:
      context: .
      dockerfile: Dockerfile
    image: llama-cpp-python:cuda
    container_name: llama-chat-cli
    
    # GPU設定
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    
    # WSL2 GPU アクセス設定
    privileged: true
    volumes:
      - /usr/lib/wsl:/usr/lib/wsl:ro
      - ./models:/app/models:ro
      - ./config:/app/config:ro
      - ./logs:/app/logs
      - ./scripts:/app/scripts
    
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - LD_LIBRARY_PATH=/usr/lib/wsl/lib
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    
    # インタラクティブ モード
    stdin_open: true
    tty: true
    
    # 自動再起動
    restart: unless-stopped
    
    # ネットワーク設定（オプション）
    # ports:
    #   - "8000:8000"  # API server用（将来使用）

  # GPU監視用サービス（オプション）
  gpu-monitor:
    image: nvidia/cuda:12.1-runtime-ubuntu22.04
    container_name: gpu-monitor
    privileged: true
    volumes:
      - /usr/lib/wsl:/usr/lib/wsl:ro
    environment:
      - LD_LIBRARY_PATH=/usr/lib/wsl/lib
    command: watch -n 1 nvidia-smi
    profiles: ["monitor"]  # docker-compose --profile monitor up で起動