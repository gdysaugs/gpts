#!/usr/bin/env python3
"""
Gradio Server using existing FastAPI logic
æ—¢å­˜ã®api_server.pyã®æˆåŠŸãƒ­ã‚¸ãƒƒã‚¯ã‚’Gradioã«ç§»æ¤
"""

import gradio as gr
import logging
import asyncio
from datetime import datetime
from typing import List, Dict, Optional

# æ—¢å­˜ã®FastAPIã‚µãƒ¼ãƒãƒ¼ã¨åŒã˜ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from llm_engine import LLMEngine, GenerationConfig, create_engine

# ===== Global Variables (FastAPIã¨åŒã˜) =====
llm_engine: Optional[LLMEngine] = None
start_time = datetime.now()

# ===== Initialization (FastAPIã®startup_eventã¨åŒã˜) =====
def initialize_engine():
    """LLMã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ï¼ˆFastAPIã®startup_eventã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰"""
    global llm_engine
    try:
        logging.info("Initializing LLM Engine...")
        llm_engine = create_engine()  # FastAPIã¨åŒã˜é–¢æ•°ä½¿ç”¨
        logging.info("LLM Engine initialized successfully!")
        return True
    except Exception as e:
        logging.error(f"Failed to initialize LLM Engine: {e}")
        return False

# ===== Helper Functions (FastAPIã¨åŒã˜) =====
def check_engine():
    """ã‚¨ãƒ³ã‚¸ãƒ³ã®å¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆFastAPIã¨åŒã˜ï¼‰"""
    if llm_engine is None:
        return False
    return True

def create_generation_config(temperature=0.7, max_tokens=512, top_p=0.9) -> GenerationConfig:
    """ç”Ÿæˆè¨­å®šã‚’ä½œæˆï¼ˆFastAPIã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰"""
    return GenerationConfig(
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        top_k=40,
        repeat_penalty=1.1
    )

# ===== Chat Function (FastAPIã®chatã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯) =====
def chat_with_llm(message: str, history: List, temperature: float, max_tokens: int, top_p: float):
    """ãƒãƒ£ãƒƒãƒˆå‡¦ç†ï¼ˆFastAPIã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰"""
    if not message.strip():
        return history, ""
    
    if not check_engine():
        error_msg = "âŒ LLM Engine not initialized"
        history.append([message, error_msg])
        return history, ""
    
    try:
        # FastAPIã¨åŒã˜ç”Ÿæˆè¨­å®šä½œæˆ
        generation_config = create_generation_config(temperature, max_tokens, top_p)
        
        # FastAPIã¨åŒã˜æ¨è«–å®Ÿè¡Œ
        start_time = datetime.now()
        response = llm_engine.generate_response(message, generation_config)
        inference_time = (datetime.now() - start_time).total_seconds()
        
        # å±¥æ­´æ›´æ–°
        history.append([message, response])
        
        logging.info(f"Generated response in {inference_time:.2f}s")
        return history, ""
        
    except Exception as e:
        error_msg = f"âŒ Generation error: {str(e)}"
        logging.error(f"Chat error: {e}")
        history.append([message, error_msg])
        return history, ""

def clear_conversation():
    """ä¼šè©±å±¥æ­´ã‚¯ãƒªã‚¢"""
    if llm_engine:
        llm_engine.clear_conversation()
    return [], ""

def get_health_status():
    """ãƒ˜ãƒ«ã‚¹çŠ¶æ…‹å–å¾—ï¼ˆFastAPIã®healthã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨åŒã˜ï¼‰"""
    uptime = datetime.now() - start_time
    return {
        "status": "healthy" if check_engine() else "unhealthy",
        "model_loaded": check_engine(),
        "gpu_available": True,
        "uptime": str(uptime)
    }

# ===== Gradio Interface =====
def create_gradio_interface():
    """Gradio UIä½œæˆ"""
    
    with gr.Blocks(
        title="ğŸ¦™ LlamaCPP Gradio Chat",
        theme=gr.themes.Default()
    ) as demo:
        
        gr.HTML("""
        <div style="text-align: center; padding: 20px;">
            <h1>ğŸ¦™ LlamaCPP Gradio Chat</h1>
            <p>æ—¢å­˜FastAPIãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ãŸGradioç‰ˆ</p>
        </div>
        """)
        
        with gr.Row():
            with gr.Column(scale=3):
                # ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆ
                chatbot = gr.Chatbot(
                    label="ãƒãƒ£ãƒƒãƒˆ",
                    height=450,
                    show_copy_button=True
                )
                
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¥åŠ›
                with gr.Row():
                    msg_input = gr.Textbox(
                        label="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        placeholder="ä½•ã‹è©±ã—ã‹ã‘ã¦...",
                        scale=4
                    )
                    send_btn = gr.Button("é€ä¿¡", variant="primary", scale=1)
                
                # ãƒœã‚¿ãƒ³ç¾¤
                with gr.Row():
                    clear_btn = gr.Button("å±¥æ­´ã‚¯ãƒªã‚¢", variant="secondary")
                    status_btn = gr.Button("çŠ¶æ…‹ç¢ºèª", variant="secondary")
            
            with gr.Column(scale=1):
                # è¨­å®šãƒ‘ãƒãƒ«ï¼ˆFastAPIã¨åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
                gr.HTML("<h3>âš™ï¸ ç”Ÿæˆè¨­å®š</h3>")
                
                temperature = gr.Slider(
                    label="Temperature",
                    minimum=0.1,
                    maximum=1.0,
                    value=0.7,
                    step=0.1
                )
                
                max_tokens = gr.Slider(
                    label="Max Tokens",
                    minimum=50,
                    maximum=1000,
                    value=512,
                    step=50
                )
                
                top_p = gr.Slider(
                    label="Top-p",
                    minimum=0.1,
                    maximum=1.0,
                    value=0.9,
                    step=0.1
                )
                
                # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
                gr.HTML("<h3>ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±</h3>")
                status_display = gr.JSON(
                    label="ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹",
                    value=get_health_status()
                )
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        msg_input.submit(
            fn=chat_with_llm,
            inputs=[msg_input, chatbot, temperature, max_tokens, top_p],
            outputs=[chatbot, msg_input]
        )
        
        send_btn.click(
            fn=chat_with_llm,
            inputs=[msg_input, chatbot, temperature, max_tokens, top_p],
            outputs=[chatbot, msg_input]
        )
        
        clear_btn.click(
            fn=clear_conversation,
            outputs=[chatbot, msg_input]
        )
        
        status_btn.click(
            fn=get_health_status,
            outputs=[status_display]
        )
    
    return demo

# ===== Main Function =====
def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # LLMã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ï¼ˆFastAPIã¨åŒã˜ï¼‰
    print("ğŸš€ Initializing LLM Engine using FastAPI logic...")
    success = initialize_engine()
    
    if not success:
        print("âŒ Failed to initialize LLM Engine!")
        return
    
    print("âœ… LLM Engine initialized successfully!")
    
    # Gradio UIä½œæˆãƒ»èµ·å‹•
    demo = create_gradio_interface()
    
    print("ğŸŒŸ Starting Gradio server...")
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,
        debug=False
    )

if __name__ == "__main__":
    main()