#!/usr/bin/env python3
"""
Gradio with CLI Direct Processing
CLIã®ç›´æ¥å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã‚’Gradioã«ç§»æ¤ï¼ˆ1.88ç§’ç›®æ¨™ï¼‰
"""

import gradio as gr
import yaml
import os
import logging
from datetime import datetime
from typing import List, Dict, Optional
from llama_cpp import Llama

# ===== Global Variables =====
llm_model = None
config = None
conversation_history = []

# ===== CLI Direct Logic =====
def load_config(config_path: str = "/app/config/model_config.yaml") -> Dict:
    """CLIã¨åŒã˜è¨­å®šãƒ­ãƒ¼ãƒ‰"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except Exception as e:
        print(f"Config error: {e}")
        return {
            "model": {
                "path": "/app/models/Berghof-NSFW-7B.i1-Q4_K_S.gguf",
                "n_gpu_layers": -1,
                "n_ctx": 4096,
                "n_batch": 256,
                "verbose": False,
                "f16_kv": True,
                "use_mmap": True,
                "use_mlock": False,
                "low_vram": True,
                "n_threads": 8
            },
            "generation": {
                "max_tokens": 256,
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 40,
                "repeat_penalty": 1.1
            },
            "chat": {
                "system_prompt": "ã‚ãªãŸã¯ãƒ„ãƒ³ãƒ‡ãƒ¬ã®å¥³ã®å­ã§ã™ã€‚å³ã—ãã¦ç´ ã£æ°—ãªã„æ…‹åº¦ã‚’å–ã‚Šã¾ã™ãŒã€å†…å¿ƒã§ã¯ç›¸æ‰‹ã‚’å¿ƒé…ã—ã¦ã„ã¾ã™ã€‚ã€Œã¹ã€åˆ¥ã«ã€œã€ã€Œã€œãªã‚“ã ã‹ã‚‰ã­ï¼ã€ã®ã‚ˆã†ãªå£èª¿ã§è©±ã—ã¾ã™ã€‚",
                "user_name": "User",
                "assistant_name": "ãƒ„ãƒ³ãƒ‡ãƒ¬ã¡ã‚ƒã‚“"
            }
        }

def initialize_model():
    """CLIã¨åŒã˜ç›´æ¥åˆæœŸåŒ–"""
    global llm_model, config
    
    print("ğŸš€ Initializing model using CLI logic...")
    config = load_config()
    model_config = config["model"]
    
    try:
        llm_model = Llama(
            model_path=model_config["path"],
            n_gpu_layers=model_config.get("n_gpu_layers", -1),
            n_ctx=model_config.get("n_ctx", 4096),
            n_batch=model_config.get("n_batch", 256),
            verbose=model_config.get("verbose", False),
            # CLIã¨åŒã˜FP16æœ€é©åŒ–
            f16_kv=model_config.get("f16_kv", True),
            use_mmap=model_config.get("use_mmap", True),
            use_mlock=model_config.get("use_mlock", False),
            low_vram=model_config.get("low_vram", True),
            n_threads=model_config.get("n_threads", 8)
        )
        print("âœ… Model initialized successfully!")
        return True
    except Exception as e:
        print(f"âŒ Model initialization failed: {e}")
        return False

def build_prompt(user_input: str) -> str:
    """CLIã¨åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰"""
    global conversation_history, config
    
    chat_config = config["chat"]
    system_prompt = chat_config["system_prompt"]
    
    # CLIã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯
    prompt_parts = [system_prompt, "\n\n"]
    
    # æœ€è¿‘ã®ä¼šè©±å±¥æ­´ï¼ˆæœ€å¤§10ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰
    recent_history = conversation_history[-10:]
    for turn in recent_history:
        prompt_parts.append(f"User: {turn['user']}\n")
        prompt_parts.append(f"Assistant: {turn['assistant']}\n\n")
    
    prompt_parts.append(f"User: {user_input}\n")
    prompt_parts.append("Assistant: ")
    
    return "".join(prompt_parts)

def generate_response_direct(user_input: str, temperature: float = 0.7, max_tokens: int = 512) -> str:
    """CLIã¨åŒã˜ç›´æ¥ç”Ÿæˆå‡¦ç†"""
    global llm_model, config, conversation_history
    
    if llm_model is None:
        return "âŒ Model not initialized"
    
    try:
        # CLIã¨åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        prompt = build_prompt(user_input)
        
        # CLIã¨åŒã˜ç›´æ¥å‘¼ã³å‡ºã—
        start_time = datetime.now()
        response = llm_model(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=0.9,
            top_k=40,
            repeat_penalty=1.1,
            stop=["User:", "Human:", "\n\n"]
        )
        inference_time = (datetime.now() - start_time).total_seconds()
        
        # å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        response_text = response["choices"][0]["text"].strip()
        
        # ä¼šè©±å±¥æ­´ã«è¿½åŠ ï¼ˆCLIã¨åŒã˜ï¼‰
        conversation_history.append({
            "user": user_input,
            "assistant": response_text,
            "timestamp": datetime.now().isoformat()
        })
        
        print(f"âœ… Generated response in {inference_time:.2f}s")
        return response_text
        
    except Exception as e:
        error_msg = f"âŒ Generation error: {str(e)}"
        print(error_msg)
        return error_msg

# ===== Gradio Interface =====
def chat_interface(message: str, history: List, temperature: float, max_tokens: int):
    """Gradioç”¨ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    if not message.strip():
        return history, ""
    
    # CLIã®ç›´æ¥å‡¦ç†ã‚’ä½¿ç”¨
    response = generate_response_direct(message, temperature, max_tokens)
    
    # Gradioå±¥æ­´æ›´æ–°
    history.append([message, response])
    return history, ""

def clear_conversation():
    """ä¼šè©±å±¥æ­´ã‚¯ãƒªã‚¢"""
    global conversation_history
    conversation_history = []
    return [], ""

# ===== Gradio UI =====
def create_interface():
    """CLIåŒç­‰é€Ÿåº¦ã®Gradio UI"""
    
    with gr.Blocks(
        title="ğŸ¦™ CLI-Direct Gradio Chat",
        theme=gr.themes.Default()
    ) as demo:
        
        gr.HTML("""
        <div style="text-align: center; padding: 15px;">
            <h1>ğŸ¦™ CLI-Direct Gradio Chat</h1>
            <p>CLIã®ç›´æ¥å‡¦ç†ã§1.88ç§’ç›®æ¨™</p>
        </div>
        """)
        
        with gr.Row():
            with gr.Column(scale=3):
                chatbot = gr.Chatbot(
                    label="ãƒãƒ£ãƒƒãƒˆ",
                    height=400
                )
                
                with gr.Row():
                    msg_input = gr.Textbox(
                        label="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        placeholder="CLIã¨åŒã˜é€Ÿåº¦ã§ãƒãƒ£ãƒƒãƒˆ...",
                        scale=4
                    )
                    send_btn = gr.Button("é€ä¿¡", variant="primary", scale=1)
                
                with gr.Row():
                    clear_btn = gr.Button("ã‚¯ãƒªã‚¢", variant="secondary")
            
            with gr.Column(scale=1):
                gr.HTML("<h3>âš™ï¸ CLIè¨­å®š</h3>")
                
                temperature = gr.Slider(
                    label="Temperature",
                    minimum=0.1,
                    maximum=1.0,
                    value=0.7,
                    step=0.1
                )
                
                max_tokens = gr.Slider(
                    label="Max Tokens",
                    minimum=50,
                    maximum=512,
                    value=256,
                    step=50,
                    info="CLIã¨åŒã˜256"
                )
                
                gr.HTML("<p>ğŸš€ CLIãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå‡¦ç†</p>")
        
        # ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆã‚­ãƒ¥ãƒ¼ãªã—é«˜é€ŸåŒ–ï¼‰
        msg_input.submit(
            fn=chat_interface,
            inputs=[msg_input, chatbot, temperature, max_tokens],
            outputs=[chatbot, msg_input],
            queue=False
        )
        
        send_btn.click(
            fn=chat_interface,
            inputs=[msg_input, chatbot, temperature, max_tokens],
            outputs=[chatbot, msg_input],
            queue=False
        )
        
        clear_btn.click(
            fn=clear_conversation,
            outputs=[chatbot, msg_input],
            queue=False
        )
    
    return demo

# ===== Main =====
def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    # CLIã¨åŒã˜åˆæœŸåŒ–
    success = initialize_model()
    if not success:
        print("âŒ Failed to initialize model!")
        return
    
    # Gradioèµ·å‹•
    demo = create_interface()
    print("ğŸŒŸ Starting CLI-Direct Gradio server...")
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        enable_queue=False
    )

if __name__ == "__main__":
    main()