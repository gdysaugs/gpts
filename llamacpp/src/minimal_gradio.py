#!/usr/bin/env python3
"""
Minimal Gradio UI - æœ€å°é™ã§ç¢ºå®Ÿã«å‹•ä½œã™ã‚‹ç‰ˆ
"""

import gradio as gr
import logging
from datetime import datetime
from llm_engine import LLMEngine, GenerationConfig

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
engine = None

def init_engine():
    global engine
    if engine is None:
        print("Loading LLM...")
        engine = LLMEngine()
        print("LLM Ready!")
    return engine

def chat(message, history):
    """æœ€å°é™ã®ãƒãƒ£ãƒƒãƒˆé–¢æ•°"""
    if not message:
        return history, ""
    
    try:
        llm = init_engine()
        config = GenerationConfig(max_tokens=256, temperature=0.7)
        response = llm.generate_response(message, config)
        history.append((message, response))
        return history, ""
    except Exception as e:
        history.append((message, f"Error: {str(e)}"))
        return history, ""

# æœ€å°é™UI
demo = gr.Interface(
    fn=lambda message, history: chat(message, history or []),
    inputs=[
        gr.Textbox(placeholder="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›..."),
        gr.State([])
    ],
    outputs=[
        gr.Chatbot(),
        gr.Textbox()
    ],
    title="ğŸ¦™ LlamaCPP Chat",
    description="ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ„ãƒ³ãƒ‡ãƒ¬ãƒãƒ£ãƒƒãƒˆ"
)

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)