#!/usr/bin/env python3
"""
Fast Gradio - CLIã¨åŒã˜é€Ÿåº¦ã‚’ç›®æŒ‡ã™è»½é‡ç‰ˆ
"""

import gradio as gr
import logging
import asyncio
import threading
from datetime import datetime
from typing import List, Dict, Optional

# æ—¢å­˜ã®LLMã‚¨ãƒ³ã‚¸ãƒ³ã‚’ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆFastAPIã¨åŒã˜ï¼‰
from llm_engine import LLMEngine, GenerationConfig, create_engine

# ===== Global Variables =====
llm_engine: Optional[LLMEngine] = None

# ===== Initialization =====
def initialize_engine():
    """LLMã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ï¼ˆCLIã¨åŒã˜ç›´æ¥å‡¦ç†ï¼‰"""
    global llm_engine
    try:
        logging.info("Initializing LLM Engine...")
        llm_engine = create_engine()
        logging.info("LLM Engine initialized successfully!")
        return True
    except Exception as e:
        logging.error(f"Failed to initialize LLM Engine: {e}")
        return False

# ===== Fast Chat Function =====
def fast_chat(message: str, history: List, temperature: float = 0.7, max_tokens: int = 256):
    """é«˜é€Ÿãƒãƒ£ãƒƒãƒˆå‡¦ç†ï¼ˆCLIã¨åŒã˜ç›´æ¥å‡¦ç†ï¼‰"""
    if not message.strip():
        return history, ""
    
    if llm_engine is None:
        error_msg = "âŒ LLM Engine not ready"
        history.append([message, error_msg])
        return history, ""
    
    try:
        # CLIã¨åŒã˜è»½é‡è¨­å®š
        config = GenerationConfig(
            max_tokens=int(max_tokens),
            temperature=float(temperature),
            top_p=0.9,
            top_k=40,
            repeat_penalty=1.1
        )
        
        # ç›´æ¥æ¨è«–å®Ÿè¡Œï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãªã—ï¼‰
        start_time = datetime.now()
        response = llm_engine.generate_response(message, config)
        inference_time = (datetime.now() - start_time).total_seconds()
        
        # å±¥æ­´æ›´æ–°
        history.append([message, response])
        
        logging.info(f"Generated response in {inference_time:.2f}s")
        return history, ""
        
    except Exception as e:
        error_msg = f"âŒ Error: {str(e)}"
        logging.error(f"Chat error: {e}")
        history.append([message, error_msg])
        return history, ""

def clear_chat():
    """å±¥æ­´ã‚¯ãƒªã‚¢"""
    if llm_engine:
        llm_engine.clear_conversation()
    return [], ""

# ===== Minimal Gradio Interface =====
def create_fast_interface():
    """è»½é‡Gradio UI"""
    
    with gr.Blocks(
        title="ğŸ¦™ Fast LlamaCPP Chat",
        css=".gradio-container {max-width: 900px; margin: auto;}"
    ) as demo:
        
        gr.HTML("""
        <div style="text-align: center; padding: 15px;">
            <h1>ğŸ¦™ Fast LlamaCPP Chat</h1>
            <p>é«˜é€Ÿãƒ„ãƒ³ãƒ‡ãƒ¬ãƒãƒ£ãƒƒãƒˆï¼ˆCLIåŒç­‰é€Ÿåº¦ï¼‰</p>
        </div>
        """)
        
        with gr.Row():
            with gr.Column(scale=4):
                # ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆ
                chatbot = gr.Chatbot(
                    label="ãƒãƒ£ãƒƒãƒˆ",
                    height=400
                )
                
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¥åŠ›
                with gr.Row():
                    msg_input = gr.Textbox(
                        label="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        placeholder="é«˜é€Ÿãƒãƒ£ãƒƒãƒˆã‚’è©¦ã—ã¦ã¿ã¦...",
                        scale=4
                    )
                    send_btn = gr.Button("é€ä¿¡", variant="primary", scale=1)
                
                # ãƒœã‚¿ãƒ³
                with gr.Row():
                    clear_btn = gr.Button("ã‚¯ãƒªã‚¢", variant="secondary")
            
            with gr.Column(scale=1):
                # è»½é‡è¨­å®š
                gr.HTML("<h3>âš™ï¸ è¨­å®š</h3>")
                
                temperature = gr.Slider(
                    label="Temperature",
                    minimum=0.1,
                    maximum=1.0,
                    value=0.7,
                    step=0.1
                )
                
                max_tokens = gr.Slider(
                    label="Max Tokens",
                    minimum=50,
                    maximum=512,
                    value=256,
                    step=50,
                    info="CLIã¨åŒã˜256æ¨å¥¨"
                )
                
                gr.HTML("<p>ğŸš€ CLIåŒç­‰ã®é«˜é€Ÿå‡¦ç†</p>")
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆè»½é‡åŒ–ï¼‰
        msg_input.submit(
            fn=fast_chat,
            inputs=[msg_input, chatbot, temperature, max_tokens],
            outputs=[chatbot, msg_input],
            queue=False  # ã‚­ãƒ¥ãƒ¼ãªã—ã§é«˜é€ŸåŒ–
        )
        
        send_btn.click(
            fn=fast_chat,
            inputs=[msg_input, chatbot, temperature, max_tokens],
            outputs=[chatbot, msg_input],
            queue=False  # ã‚­ãƒ¥ãƒ¼ãªã—ã§é«˜é€ŸåŒ–
        )
        
        clear_btn.click(
            fn=clear_chat,
            outputs=[chatbot, msg_input],
            queue=False  # ã‚­ãƒ¥ãƒ¼ãªã—ã§é«˜é€ŸåŒ–
        )
    
    return demo

# ===== Main Function =====
def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # LLMã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
    print("ğŸš€ Initializing LLM Engine...")
    success = initialize_engine()
    
    if not success:
        print("âŒ Failed to initialize LLM Engine!")
        return
    
    print("âœ… LLM Engine ready!")
    
    # è»½é‡Gradio UIèµ·å‹•
    demo = create_fast_interface()
    
    print("ğŸŒŸ Starting Fast Gradio server...")
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,
        debug=False,
        enable_queue=False  # ã‚­ãƒ¥ãƒ¼ãªã—ã§é«˜é€ŸåŒ–
    )

if __name__ == "__main__":
    main()