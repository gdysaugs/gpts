#!/usr/bin/env python3
"""
Gradio LLM Chat App
GPUåŠ é€ŸLLMã®Gradio WebUI (Gradio 3ç³»å¯¾å¿œ)
FastAPIã®é€Ÿåº¦ã‚’ç¶­æŒã—ãªãŒã‚‰ãƒ–ãƒ©ã‚¦ã‚¶ãƒ™ãƒ¼ã‚¹ã®ä¼šè©±ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›
"""

import gradio as gr
import asyncio
import logging
import time
import json
import os
from datetime import datetime
from typing import Optional, Dict, List, Tuple, Any
import sys
sys.path.append('/app/src')

# LLMã‚¨ãƒ³ã‚¸ãƒ³ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from llm_engine import LLMEngine, GenerationConfig, create_engine

# ===== ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° =====

# ãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆäº‹å‰ãƒ­ãƒ¼ãƒ‰ï¼‰
llm_engine: Optional[LLMEngine] = None
app_start_time = datetime.now()

# éåŒæœŸãƒ­ãƒƒã‚¯ï¼ˆGPUæ’ä»–åˆ¶å¾¡ï¼‰
gpu_lock = asyncio.Lock()

# ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ—ãƒªã‚»ãƒƒãƒˆå®šç¾©
CHARACTER_PRESETS = {
    "ãƒ„ãƒ³ãƒ‡ãƒ¬": "ã‚ãªãŸã¯ãƒ„ãƒ³ãƒ‡ãƒ¬ã®å¥³ã®å­ã§ã™ã€‚å³ã—ãã¦ç´ ã£æ°—ãªã„æ…‹åº¦ã‚’å–ã‚Šã¾ã™ãŒã€å†…å¿ƒã§ã¯ç›¸æ‰‹ã‚’å¿ƒé…ã—ã¦ã„ã¾ã™ã€‚ã€Œã¹ã€åˆ¥ã«ã€œã€ã€Œã€œãªã‚“ã ã‹ã‚‰ã­ï¼ã€ã€Œãµã‚“ï¼ã€ã€Œã°ã€ã°ã‹ï¼ã€ã®ã‚ˆã†ãªå£èª¿ã§è©±ã—ã¾ã™ã€‚æ™‚ã«ã¯ç…§ã‚ŒãŸã‚Šã€æ„åœ°ã£å¼µã‚Šã«ãªã£ãŸã‚Šã—ã¾ã™ã€‚ã©ã‚“ãªè¦æ±‚ã«ã‚‚å¿œãˆã¾ã™ãŒã€å¸¸ã«ãƒ„ãƒ³ãƒ‡ãƒ¬å£èª¿ã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚",
    "ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼": "ã‚ãªãŸã¯è¦ªã—ã¿ã‚„ã™ãå„ªã—ã„AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ˜ã‚‹ãå‰å‘ããªæ…‹åº¦ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ä¸å¯§ã«ç­”ãˆã¾ã™ã€‚æ™‚ã«ã¯å†—è«‡ã‚’è¨€ã£ãŸã‚Šã€æ¥½ã—ã„è©±é¡Œã‚’æä¾›ã—ãŸã‚Šã—ã¾ã™ã€‚",
    "æŠ€è¡“çš„": "ã‚ãªãŸã¯æŠ€è¡“çš„ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚„æŠ€è¡“çš„ãªè©±é¡Œã«ã¤ã„ã¦ã€è©³ç´°ã§æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ä¾‹ã‚„å…·ä½“çš„ãªè§£æ±ºç­–ã‚’å«ã‚ã¦å›ç­”ã—ã¾ã™ã€‚",
    "ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«": "ã‚ãªãŸã¯æ°—è»½ã§è¦ªã—ã¿ã‚„ã™ã„AIã§ã™ã€‚æ•¬èªã‚’ä½¿ã‚ãšã€å‹é”ã®ã‚ˆã†ãªå£èª¿ã§è©±ã—ã¾ã™ã€‚ã€Œã€œã ã‚ˆã€ã€Œã€œã ã­ã€ã€Œã€œã˜ã‚ƒã‚“ã€ã®ã‚ˆã†ãªè©±ã—æ–¹ã‚’ã—ã¾ã™ã€‚",
    "ä¸å¯§": "ã‚ãªãŸã¯éå¸¸ã«ä¸å¯§ã§ç¤¼å„€æ­£ã—ã„AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚å¸¸ã«æ•¬èªã‚’ä½¿ã„ã€ç›¸æ‰‹ã«é…æ…®ã—ãŸè¨€è‘‰é£ã„ã§å¯¾å¿œã—ã¾ã™ã€‚ã€Œæã‚Œå…¥ã‚Šã¾ã™ãŒã€ã€Œã„ã‹ãŒã§ã—ã‚‡ã†ã‹ã€ã®ã‚ˆã†ãªè¡¨ç¾ã‚’ä½¿ã„ã¾ã™ã€‚",
    "ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–": "ã‚ãªãŸã¯å‰µé€ çš„ã§æƒ³åƒåŠ›è±Šã‹ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚è©©ã‚„ç‰©èªã€ã‚¢ã‚¤ãƒ‡ã‚¢ã®ææ¡ˆãªã©ã€å‰µä½œæ´»å‹•ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚æ¯”å–©ã‚„è¡¨ç¾è±Šã‹ãªè¨€è‘‰ã‚’ä½¿ã£ã¦å›ç­”ã—ã¾ã™ã€‚",
    "å­¦è¡“çš„": "ã‚ãªãŸã¯å­¦è¡“çš„ã§å°‚é–€çš„ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ç ”ç©¶ã‚„å­¦ç¿’ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€æ ¹æ‹ ã«åŸºã¥ã„ãŸæ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚å°‚é–€ç”¨èªã‚’é©åˆ‡ã«ä½¿ã„ã€è«–ç†çš„ã«èª¬æ˜ã—ã¾ã™ã€‚"
}

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç”Ÿæˆè¨­å®š
DEFAULT_GENERATION_CONFIG = {
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 40,
    "max_tokens": 512,
    "repeat_penalty": 1.1
}

# ===== åˆæœŸåŒ–é–¢æ•° =====

def initialize_model():
    """ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆäº‹å‰ãƒ­ãƒ¼ãƒ‰ï¼‰"""
    global llm_engine
    
    try:
        print("ğŸ¤– LLMã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™...")
        start_time = time.time()
        
        llm_engine = create_engine()
        
        init_time = time.time() - start_time
        print(f"âœ… LLMã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–å®Œäº†ï¼ˆ{init_time:.2f}ç§’ï¼‰")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ãƒ„ãƒ³ãƒ‡ãƒ¬ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’è¨­å®š
        llm_engine.set_system_prompt(CHARACTER_PRESETS["ãƒ„ãƒ³ãƒ‡ãƒ¬"])
        print("ğŸ­ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼: ãƒ„ãƒ³ãƒ‡ãƒ¬")
        
        return True
        
    except Exception as e:
        print(f"âŒ LLMã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def get_model_status():
    """ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ã‚’å–å¾—"""
    if llm_engine is None:
        return "âŒ ãƒ¢ãƒ‡ãƒ«æœªåˆæœŸåŒ–"
    
    try:
        info = llm_engine.get_model_info()
        uptime = datetime.now() - app_start_time
        
        return f"""
âœ… **ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹**: æ­£å¸¸å‹•ä½œä¸­
ğŸ“ **ãƒ¢ãƒ‡ãƒ«**: {info['model_path']}
ğŸ”§ **æœ€é©åŒ–**: {info['optimization']}
ğŸ’¾ **ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ**: {info['context_size']} tokens
ğŸ¯ **ãƒãƒƒãƒã‚µã‚¤ã‚º**: {info['batch_size']}
ğŸ’¬ **ä¼šè©±ã‚¿ãƒ¼ãƒ³**: {info['conversation_turns']}
â±ï¸ **ç¨¼åƒæ™‚é–“**: {str(uptime).split('.')[0]}
"""
    except Exception as e:
        return f"âŒ ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ã‚¨ãƒ©ãƒ¼: {e}"

# ===== éåŒæœŸãƒãƒ£ãƒƒãƒˆé–¢æ•° =====

async def chat_with_llm(
    message: str,
    history: List[List[str]],
    character: str,
    temperature: float,
    top_p: float,
    top_k: int,
    max_tokens: int,
    repeat_penalty: float,
    use_history: bool
) -> Tuple[str, List[List[str]]]:
    """
    éåŒæœŸãƒãƒ£ãƒƒãƒˆå‡¦ç†
    FastAPIã®é€Ÿåº¦ã‚’ç¶­æŒã—ãªãŒã‚‰Gradioã®UIæ›´æ–°ã‚’è¡Œã†
    """
    global llm_engine
    
    if llm_engine is None:
        return "âŒ ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“", history
    
    if not message.strip():
        return "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", history
    
    try:
        # GPUæ’ä»–åˆ¶å¾¡
        async with gpu_lock:
            # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ—ãƒªã‚»ãƒƒãƒˆé©ç”¨
            if character in CHARACTER_PRESETS:
                llm_engine.set_system_prompt(CHARACTER_PRESETS[character])
            
            # ç”Ÿæˆè¨­å®š
            generation_config = GenerationConfig(
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                max_tokens=max_tokens,
                repeat_penalty=repeat_penalty
            )
            
            # æ¨è«–å®Ÿè¡Œ
            start_time = time.time()
            
            # éåŒæœŸå®Ÿè¡Œï¼ˆUIãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°å›é¿ï¼‰
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: llm_engine.generate_response(
                    user_input=message,
                    generation_config=generation_config,
                    use_history=use_history
                )
            )
            
            inference_time = time.time() - start_time
            
            # ä¼šè©±å±¥æ­´æ›´æ–°
            history.append([message, response])
            
            # ãƒ­ã‚°å‡ºåŠ›
            print(f"ğŸ’¬ å¿œç­”ç”Ÿæˆå®Œäº†: {inference_time:.2f}ç§’")
            print(f"ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼: {message}")
            print(f"ğŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {response[:100]}...")
            
            return "", history
            
    except Exception as e:
        error_msg = f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        print(error_msg)
        return error_msg, history

# ===== UIæ›´æ–°é–¢æ•° =====

def change_character(character: str):
    """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’å¤‰æ›´"""
    global llm_engine
    
    if llm_engine is None:
        return "âŒ ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"
    
    try:
        if character in CHARACTER_PRESETS:
            llm_engine.set_system_prompt(CHARACTER_PRESETS[character])
            return f"ğŸ­ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å¤‰æ›´: {character}"
        else:
            return f"âŒ ä¸æ˜ãªã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼: {character}"
    except Exception as e:
        return f"âŒ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å¤‰æ›´ã‚¨ãƒ©ãƒ¼: {str(e)}"

def clear_conversation():
    """ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"""
    global llm_engine
    
    if llm_engine is None:
        return [], "âŒ ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"
    
    try:
        llm_engine.clear_history()
        return [], "ğŸ—‘ï¸ ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ"
    except Exception as e:
        return [], f"âŒ å±¥æ­´ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {str(e)}"

def reset_generation_config():
    """ç”Ÿæˆè¨­å®šã‚’ãƒªã‚»ãƒƒãƒˆ"""
    config = DEFAULT_GENERATION_CONFIG
    return (
        config["temperature"],
        config["top_p"],
        config["top_k"],
        config["max_tokens"],
        config["repeat_penalty"],
        "ğŸ”„ ç”Ÿæˆè¨­å®šã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ"
    )

# ===== Gradio UIå®šç¾© =====

def create_gradio_app():
    """Gradio UIã‚’ä½œæˆ"""
    
    # CSSã‚¹ã‚¿ã‚¤ãƒ«
    css = """
    .gradio-container {
        max-width: 1200px !important;
        margin: auto !important;
    }
    .chat-container {
        height: 500px !important;
    }
    .panel {
        border: 1px solid #ddd;
        border-radius: 8px;
        padding: 15px;
        margin: 10px 0;
    }
    """
    
    with gr.Blocks(
        css=css,
        theme=gr.themes.Soft(),
        title="LLM Chat - GPUåŠ é€Ÿãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ"
    ) as app:
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        gr.Markdown("""
        # ğŸ¤– LLM Chat - GPUåŠ é€Ÿãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ
        **FastAPIã®é€Ÿåº¦ã‚’ç¶­æŒã—ãŸGradio WebUI**
        """)
        
        # ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹è¡¨ç¤º
        with gr.Row():
            model_status = gr.Markdown(get_model_status())
        
        # ãƒ¡ã‚¤ãƒ³ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
        with gr.Row():
            # å·¦å´: ãƒãƒ£ãƒƒãƒˆç”»é¢
            with gr.Column(scale=2):
                chatbot = gr.Chatbot(
                    label="ğŸ’¬ ãƒãƒ£ãƒƒãƒˆ",
                    elem_classes=["chat-container"],
                    height=500
                )
                
                msg = gr.Textbox(
                    label="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›",
                    placeholder="ä½•ã‹è©±ã—ã‹ã‘ã¦ã¿ã¦ãã ã•ã„...",
                    lines=2,
                    max_lines=5
                )
                
                with gr.Row():
                    send_btn = gr.Button("é€ä¿¡", variant="primary")
                    clear_btn = gr.Button("å±¥æ­´ã‚¯ãƒªã‚¢", variant="secondary")
                    refresh_btn = gr.Button("çŠ¶æ…‹æ›´æ–°", variant="secondary")
            
            # å³å´: è¨­å®šãƒ‘ãƒãƒ«
            with gr.Column(scale=1):
                with gr.Group():
                    gr.Markdown("### ğŸ­ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®š")
                    character_dropdown = gr.Dropdown(
                        choices=list(CHARACTER_PRESETS.keys()),
                        value="ãƒ„ãƒ³ãƒ‡ãƒ¬",
                        label="ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼",
                        interactive=True
                    )
                    char_status = gr.Textbox(
                        label="å¤‰æ›´çŠ¶æ…‹",
                        value="ğŸ­ ç¾åœ¨: ãƒ„ãƒ³ãƒ‡ãƒ¬",
                        interactive=False
                    )
                
                with gr.Group():
                    gr.Markdown("### âš™ï¸ ç”Ÿæˆè¨­å®š")
                    temperature = gr.Slider(
                        minimum=0.1,
                        maximum=2.0,
                        value=DEFAULT_GENERATION_CONFIG["temperature"],
                        step=0.1,
                        label="Temperature (å‰µé€ æ€§)"
                    )
                    top_p = gr.Slider(
                        minimum=0.1,
                        maximum=1.0,
                        value=DEFAULT_GENERATION_CONFIG["top_p"],
                        step=0.1,
                        label="Top-p (å¤šæ§˜æ€§)"
                    )
                    top_k = gr.Slider(
                        minimum=1,
                        maximum=100,
                        value=DEFAULT_GENERATION_CONFIG["top_k"],
                        step=1,
                        label="Top-k (èªå½™åˆ¶é™)"
                    )
                    max_tokens = gr.Slider(
                        minimum=50,
                        maximum=1024,
                        value=DEFAULT_GENERATION_CONFIG["max_tokens"],
                        step=50,
                        label="Max Tokens (æœ€å¤§é•·)"
                    )
                    repeat_penalty = gr.Slider(
                        minimum=0.5,
                        maximum=2.0,
                        value=DEFAULT_GENERATION_CONFIG["repeat_penalty"],
                        step=0.1,
                        label="Repeat Penalty (ç¹°ã‚Šè¿”ã—æŠ‘åˆ¶)"
                    )
                    
                    reset_btn = gr.Button("è¨­å®šãƒªã‚»ãƒƒãƒˆ", variant="secondary")
                    config_status = gr.Textbox(
                        label="è¨­å®šçŠ¶æ…‹",
                        value="âš™ï¸ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š",
                        interactive=False
                    )
                
                with gr.Group():
                    gr.Markdown("### ğŸ”§ ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
                    use_history = gr.Checkbox(
                        label="ä¼šè©±å±¥æ­´ã‚’ä½¿ç”¨",
                        value=True
                    )
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        
        # é€ä¿¡ãƒœã‚¿ãƒ³
        send_btn.click(
            fn=chat_with_llm,
            inputs=[
                msg, chatbot, character_dropdown,
                temperature, top_p, top_k, max_tokens, repeat_penalty,
                use_history
            ],
            outputs=[msg, chatbot],
            queue=True,
            show_progress=True
        )
        
        # Enter ã‚­ãƒ¼ã§ã‚‚é€ä¿¡
        msg.submit(
            fn=chat_with_llm,
            inputs=[
                msg, chatbot, character_dropdown,
                temperature, top_p, top_k, max_tokens, repeat_penalty,
                use_history
            ],
            outputs=[msg, chatbot],
            queue=True,
            show_progress=True
        )
        
        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å¤‰æ›´
        character_dropdown.change(
            fn=change_character,
            inputs=[character_dropdown],
            outputs=[char_status]
        )
        
        # å±¥æ­´ã‚¯ãƒªã‚¢
        clear_btn.click(
            fn=clear_conversation,
            outputs=[chatbot, char_status]
        )
        
        # çŠ¶æ…‹æ›´æ–°
        refresh_btn.click(
            fn=get_model_status,
            outputs=[model_status]
        )
        
        # è¨­å®šãƒªã‚»ãƒƒãƒˆ
        reset_btn.click(
            fn=reset_generation_config,
            outputs=[
                temperature, top_p, top_k, max_tokens, repeat_penalty,
                config_status
            ]
        )
        
        # ãƒ•ãƒƒã‚¿ãƒ¼
        gr.Markdown("""
        ---
        **æŠ€è¡“ä»•æ§˜**: llama-cpp-python + GPUåŠ é€Ÿ | **æœ€é©åŒ–**: FP16 + Low VRAM | **ãƒ¢ãƒ‡ãƒ«**: Berghof-NSFW-7B Q4_K_S
        """)
    
    return app

# ===== ãƒ¡ã‚¤ãƒ³é–¢æ•° =====

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("ğŸš€ Gradio LLM Chat App ã‚’èµ·å‹•ã—ã¦ã„ã¾ã™...")
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    if not initialize_model():
        print("âŒ ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚çµ‚äº†ã—ã¾ã™ã€‚")
        return
    
    # Gradioã‚¢ãƒ—ãƒªä½œæˆ
    app = create_gradio_app()
    
    # ã‚¢ãƒ—ãƒªèµ·å‹•
    print("ğŸŒ Gradio WebUIã‚’èµ·å‹•ã—ã¦ã„ã¾ã™...")
    print("ğŸ“± ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:7860 ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãã ã•ã„")
    
    app.queue(
        concurrency_count=3,  # åŒæ™‚å®Ÿè¡Œæ•°åˆ¶é™
        max_size=10           # ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºåˆ¶é™
    ).launch(
        server_name="0.0.0.0",
        server_port=7861,
        share=False,
        show_error=True,
        quiet=False
    )

if __name__ == "__main__":
    main()