#!/usr/bin/env python3
"""
FastAPI LLM Chat Server with Enhanced Preloading
äº‹å‰ãƒ­ãƒ¼ãƒ‰æœ€é©åŒ–ã«ã‚ˆã‚‹é«˜é€ŸLLMãƒãƒ£ãƒƒãƒˆã‚µãƒ¼ãƒãƒ¼
"""

import asyncio
import uvicorn
import time
import logging
import os
import sys
import yaml
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Generator
import tempfile
import urllib.request

# FastAPI
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# LLM Engine
from llm_engine import LLMEngine, GenerationConfig, create_engine

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
app = FastAPI(title="LLM Chat API with Preloading", version="2.0.0")
llm_engine: Optional[LLMEngine] = None
server_start_time = datetime.now()
MODELS_LOADED = False

# ã‚·ãƒ³ãƒ—ãƒ«ãªGPUæ’ä»–åˆ¶å¾¡
gpu_lock: Optional[asyncio.Lock] = None

# ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
PRELOADED_CACHE = {}
CACHE_DIR = "/app/cache"

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
class ChatRequest(BaseModel):
    message: str = Field(..., description="ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
    use_history: bool = Field(True, description="ä¼šè©±å±¥æ­´ã‚’ä½¿ç”¨ã™ã‚‹ã‹")
    stream: bool = Field(False, description="ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”")
    generation_config: Optional[Dict] = Field(None, description="ç”Ÿæˆè¨­å®š")

class ChatResponse(BaseModel):
    response: str = Field(..., description="ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”")
    inference_time: Optional[float] = Field(None, description="æ¨è«–æ™‚é–“(ç§’)")
    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())

class InteractiveRequest(BaseModel):
    message: str = Field(..., description="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
    character: str = Field("tsundere", description="ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼")
    temperature: float = Field(0.7, ge=0.1, le=2.0, description="æ¸©åº¦")
    max_tokens: int = Field(512, ge=1, le=2048, description="æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°")

class InteractiveResponse(BaseModel):
    response: str = Field(..., description="å¿œç­”")
    character: str = Field(..., description="ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼")
    inference_time: float = Field(..., description="æ¨è«–æ™‚é–“")
    tokens_per_second: float = Field(..., description="ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")

class HealthResponse(BaseModel):
    status: str = Field(..., description="ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹")
    model_loaded: bool = Field(..., description="ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿çŠ¶æ…‹")
    gpu_available: bool = Field(..., description="GPUåˆ©ç”¨å¯èƒ½æ€§")
    uptime: str = Field(..., description="ç¨¼åƒæ™‚é–“")
    preload_status: Dict = Field(..., description="äº‹å‰ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹")

# === äº‹å‰ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ ===

def setup_optimizations():
    """æœ€é©åŒ–è¨­å®š"""
    try:
        # CPUæœ€é©åŒ–ï¼ˆå¿…ãšå®Ÿè¡Œï¼‰
        os.environ['OMP_NUM_THREADS'] = '8'
        os.environ['MKL_NUM_THREADS'] = '8'
        os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # CUDAæœ€é©åŒ–
        os.environ['CUDA_CACHE_DISABLE'] = '0'    # CUDAã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹
        
        # GPUæœ€é©åŒ–ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        try:
            import torch
            if torch.cuda.is_available():
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.allow_tf32 = True
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
                torch.cuda.empty_cache()
                logger.info("ğŸš€ PyTorch GPUæœ€é©åŒ–æœ‰åŠ¹")
        except ImportError:
            logger.info("ğŸ“ PyTorchæœªæ¤œå‡º - CUDAç’°å¢ƒå¤‰æ•°ã§æœ€é©åŒ–")
            # PyTorchãŒãªãã¦ã‚‚CUDAç’°å¢ƒå¤‰æ•°ã§æœ€é©åŒ–
            os.environ['CUDA_VISIBLE_DEVICES'] = '0'
            os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
        
        # llama-cpp-pythonç‰¹æœ‰ã®æœ€é©åŒ–
        os.environ['LLAMA_CUDA'] = '1'
        os.environ['LLAMA_CUBLAS'] = '1'
        
        logger.info("âœ… æœ€é©åŒ–è¨­å®šå®Œäº†")
        
    except Exception as e:
        logger.warning(f"âš ï¸ æœ€é©åŒ–è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")

async def preload_all_dependencies():
    """å…¨ã¦ã®ä¾å­˜é–¢ä¿‚ã‚’äº‹å‰ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    global PRELOADED_CACHE
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs(CACHE_DIR, exist_ok=True)
    
    # 1. åŸºæœ¬çš„ãªä¾å­˜é–¢ä¿‚ã®ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰
    logger.info("ğŸ”¥ åŸºæœ¬ä¾å­˜é–¢ä¿‚ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰ä¸­...")
    try:
        # JSONãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®äº‹å‰ãƒ­ãƒ¼ãƒ‰
        import json
        PRELOADED_CACHE['json'] = json
        
        # yamlãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®äº‹å‰ãƒ­ãƒ¼ãƒ‰
        import yaml
        PRELOADED_CACHE['yaml'] = yaml
        
        # datetimeãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®äº‹å‰ãƒ­ãƒ¼ãƒ‰
        from datetime import datetime
        PRELOADED_CACHE['datetime'] = datetime
        
        logger.info("âœ… åŸºæœ¬ä¾å­˜é–¢ä¿‚ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰å®Œäº†")
    except Exception as e:
        logger.warning(f"âš ï¸ åŸºæœ¬ä¾å­˜é–¢ä¿‚ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
    
    # 2. LLMé–¢é€£ã®ä¾å­˜é–¢ä¿‚äº‹å‰ãƒ­ãƒ¼ãƒ‰
    logger.info("ğŸ”¥ LLMä¾å­˜é–¢ä¿‚ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰ä¸­...")
    try:
        # llama-cpp-pythonã®äº‹å‰ãƒ­ãƒ¼ãƒ‰
        from llama_cpp import Llama
        PRELOADED_CACHE['llama_cpp'] = Llama
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®äº‹å‰ãƒ­ãƒ¼ãƒ‰
        config_path = "/app/config/model_config.yaml"
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                PRELOADED_CACHE['config'] = yaml.safe_load(f)
        
        logger.info("âœ… LLMä¾å­˜é–¢ä¿‚ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰å®Œäº†")
    except Exception as e:
        logger.warning(f"âš ï¸ LLMä¾å­˜é–¢ä¿‚ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
    
    # 3. ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ—ãƒªã‚»ãƒƒãƒˆã®äº‹å‰ãƒ­ãƒ¼ãƒ‰
    logger.info("ğŸ”¥ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ—ãƒªã‚»ãƒƒãƒˆäº‹å‰ãƒ­ãƒ¼ãƒ‰ä¸­...")
    try:
        presets = {
            'tsundere': "ã‚ãªãŸã¯ãƒ„ãƒ³ãƒ‡ãƒ¬ã®å¥³ã®å­ã§ã™ã€‚å³ã—ãã¦ç´ ã£æ°—ãªã„æ…‹åº¦ã‚’å–ã‚Šã¾ã™ãŒã€å†…å¿ƒã§ã¯ç›¸æ‰‹ã‚’å¿ƒé…ã—ã¦ã„ã¾ã™ã€‚ã€Œã¹ã€åˆ¥ã«ã€œã€ã€Œã€œãªã‚“ã ã‹ã‚‰ã­ï¼ã€ã€Œãµã‚“ï¼ã€ã€Œã°ã€ã°ã‹ï¼ã€ã®ã‚ˆã†ãªå£èª¿ã§è©±ã—ã¾ã™ã€‚",
            'friendly': "ã‚ãªãŸã¯ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§æ˜ã‚‹ã„AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚è¦ªã—ã¿ã‚„ã™ãã€å„ªã—ã„å£èª¿ã§è©±ã—ã¾ã™ã€‚ã€Œã€œã§ã™ã­ï¼ã€ã€Œã€œã§ã™ã‚ˆâ™ªã€ã®ã‚ˆã†ãªæ˜ã‚‹ã„å£èª¿ã‚’ä½¿ã„ã¾ã™ã€‚",
            'technical': "ã‚ãªãŸã¯æŠ€è¡“çš„ãªè³ªå•ã«ç‰¹åŒ–ã—ãŸAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚„æŠ€è¡“çš„ãªãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã€è©³ç´°ã§æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚å°‚é–€ç”¨èªã‚’é©åˆ‡ã«ä½¿ç”¨ã—ã€è«–ç†çš„ã«èª¬æ˜ã—ã¾ã™ã€‚",
            'casual': "ã‚ãªãŸã¯ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ã§è¦ªã—ã¿ã‚„ã™ã„AIã§ã™ã€‚å‹é”ã®ã‚ˆã†ã«ã‚¿ãƒ¡å£ã§è©±ã—ã¾ã™ã€‚ã€Œã€œã ã‚ˆã€ã€Œã€œã˜ã‚ƒã‚“ã€ã€Œã€œã§ã—ã‚‡ï¼Ÿã€ã®ã‚ˆã†ãªå£èª¿ã‚’ä½¿ã„ã¾ã™ã€‚",
            'polite': "ã‚ãªãŸã¯éå¸¸ã«ä¸å¯§ã§ç¤¼å„€æ­£ã—ã„AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚å¸¸ã«æ•¬èªã‚’ä½¿ã„ã€ç›¸æ‰‹ã‚’å°Šé‡ã—ãŸè¨€è‘‰é£ã„ã§è©±ã—ã¾ã™ã€‚ã€Œã€œã§ã”ã–ã„ã¾ã™ã€ã€Œã€œã„ãŸã—ã¾ã™ã€ã®ã‚ˆã†ãªä¸å¯§èªã‚’ä½¿ã„ã¾ã™ã€‚",
            'creative': "ã‚ãªãŸã¯å‰µé€ çš„ã§ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒ†ã‚£ãƒƒã‚¯ãªAIã§ã™ã€‚è©©çš„ã§ç¾ã—ã„è¡¨ç¾ã‚’å¥½ã¿ã€ã‚¢ã‚¤ãƒ‡ã‚¢ã‚„ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’ç”Ÿã¿å‡ºã™ã“ã¨ãŒå¾—æ„ã§ã™ã€‚æ¯”å–©ã‚„ä¿®è¾æŠ€æ³•ã‚’ä½¿ã£ã¦è¡¨ç¾è±Šã‹ã«è©±ã—ã¾ã™ã€‚",
            'academic': "ã‚ãªãŸã¯å­¦è¡“çš„ã§è«–ç†çš„ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ç ”ç©¶ã‚„å­¦ç¿’ã«é–¢ã™ã‚‹è³ªå•ã«å¯¾ã—ã¦ã€æ ¹æ‹ ã«åŸºã¥ã„ãŸè©³ç´°ãªå›ç­”ã‚’æä¾›ã—ã¾ã™ã€‚å®¢è¦³çš„ã§åˆ†æçš„ãªè¦–ç‚¹ã‚’æŒã¡ã¾ã™ã€‚"
        }
        
        PRELOADED_CACHE['presets'] = presets
        logger.info("âœ… ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ—ãƒªã‚»ãƒƒãƒˆäº‹å‰ãƒ­ãƒ¼ãƒ‰å®Œäº†")
    except Exception as e:
        logger.warning(f"âš ï¸ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ—ãƒªã‚»ãƒƒãƒˆäº‹å‰ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")

async def initialize_models():
    """ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚1å›ã®ã¿ï¼‰"""
    global llm_engine, MODELS_LOADED
    
    if MODELS_LOADED:
        return
    
    logger.info("ğŸš€ === ã‚µãƒ¼ãƒãƒ¼åˆæœŸåŒ–é–‹å§‹ ===")
    init_start = time.time()
    
    try:
        # æœ€é©åŒ–è¨­å®š
        setup_optimizations()
        
        # äº‹å‰ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
        await preload_all_dependencies()
        
        # LLMã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        logger.info("ğŸ”¥ LLMã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ä¸­...")
        llm_engine = create_engine()
        
        # Warm-upæ¨è«–
        logger.info("ğŸ”¥ Warm-upæ¨è«–å®Ÿè¡Œä¸­...")
        test_response = llm_engine.generate_response(
            user_input="ãƒ†ã‚¹ãƒˆ",
            generation_config=GenerationConfig(max_tokens=10),
            use_history=False
        )
        logger.info(f"ğŸ“ Warm-upçµæœ: {test_response[:50]}...")
        
        # GPUæœ€é©åŒ–ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.synchronize()
                torch.cuda.empty_cache()
                logger.info("ğŸš€ PyTorch GPUæœ€é©åŒ–å®Œäº†")
        except ImportError:
            logger.info("ğŸ“ PyTorchæœªæ¤œå‡º - llama-cpp-pythonå†…è”µCUDAä½¿ç”¨")
        except Exception as e:
            logger.warning(f"âš ï¸ GPUæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        MODELS_LOADED = True
        init_time = time.time() - init_start
        
        logger.info(f"âœ… === ã‚µãƒ¼ãƒãƒ¼åˆæœŸåŒ–å®Œäº†: {init_time:.2f}ç§’ ===")
        logger.info("ğŸ¯ ä»¥é™ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯1-2ç§’ã§å¿œç­”äºˆå®š")
        
    except Exception as e:
        logger.error(f"âŒ ã‚µãƒ¼ãƒãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        raise

# === ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ===

def check_engine():
    """ã‚¨ãƒ³ã‚¸ãƒ³ã®å¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯"""
    if llm_engine is None:
        raise HTTPException(status_code=503, detail="LLM Engine not initialized")

def get_preset_prompt(character: str) -> str:
    """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ—ãƒªã‚»ãƒƒãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—"""
    if 'presets' in PRELOADED_CACHE:
        return PRELOADED_CACHE['presets'].get(character, PRELOADED_CACHE['presets']['tsundere'])
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    presets = {
        'tsundere': "ã‚ãªãŸã¯ãƒ„ãƒ³ãƒ‡ãƒ¬ã®å¥³ã®å­ã§ã™ã€‚å³ã—ãã¦ç´ ã£æ°—ãªã„æ…‹åº¦ã‚’å–ã‚Šã¾ã™ãŒã€å†…å¿ƒã§ã¯ç›¸æ‰‹ã‚’å¿ƒé…ã—ã¦ã„ã¾ã™ã€‚ã€Œã¹ã€åˆ¥ã«ã€œã€ã€Œã€œãªã‚“ã ã‹ã‚‰ã­ï¼ã€ã€Œãµã‚“ï¼ã€ã€Œã°ã€ã°ã‹ï¼ã€ã®ã‚ˆã†ãªå£èª¿ã§è©±ã—ã¾ã™ã€‚",
        'friendly': "ã‚ãªãŸã¯ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§æ˜ã‚‹ã„AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚è¦ªã—ã¿ã‚„ã™ãã€å„ªã—ã„å£èª¿ã§è©±ã—ã¾ã™ã€‚",
        'technical': "ã‚ãªãŸã¯æŠ€è¡“çš„ãªè³ªå•ã«ç‰¹åŒ–ã—ãŸAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚„æŠ€è¡“çš„ãªãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã€è©³ç´°ã§æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚"
    }
    return presets.get(character, presets['tsundere'])

# === FastAPI ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ===

@app.on_event("startup")
async def startup_event():
    """ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚åˆæœŸåŒ–"""
    global gpu_lock
    try:
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        await initialize_models()
        
        # GPUæ’ä»–åˆ¶å¾¡åˆæœŸåŒ–
        gpu_lock = asyncio.Lock()
        logger.info("âœ… AsyncIO GPU LockåˆæœŸåŒ–å®Œäº†ï¼")
        
    except Exception as e:
        logger.error(f"âŒ ã‚µãƒ¼ãƒãƒ¼åˆæœŸåŒ–å¤±æ•—: {e}")
        raise

@app.get("/")
async def root():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "message": "LLM Chat API with Enhanced Preloading",
        "models_loaded": MODELS_LOADED,
        "version": "2.0.0",
        "docs": "/docs"
    }

@app.get("/health", response_model=HealthResponse)
async def health():
    """è©³ç´°ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    uptime = datetime.now() - server_start_time
    
    # äº‹å‰ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹
    preload_status = {
        "basic_deps": "json" in PRELOADED_CACHE,
        "llm_deps": "llama_cpp" in PRELOADED_CACHE,
        "config": "config" in PRELOADED_CACHE,
        "presets": "presets" in PRELOADED_CACHE,
        "cache_size": len(PRELOADED_CACHE)
    }
    
    return HealthResponse(
        status="healthy" if llm_engine else "unhealthy",
        model_loaded=MODELS_LOADED,
        gpu_available=True,
        uptime=str(uptime),
        preload_status=preload_status
    )

@app.post("/warmup")
async def warmup():
    """ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã€‚ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ãƒ†ã‚¹ãƒˆæ¨è«–ã‚’å®Ÿè¡Œ"""
    try:
        logger.info("ğŸ”¥ LLaMA APIã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é–‹å§‹...")
        start_time = time.time()
        
        # ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„å ´åˆã¯åˆæœŸåŒ–
        if not MODELS_LOADED:
            await initialize_models()
        
        # ãƒ†ã‚¹ãƒˆæ¨è«–ã‚’å®Ÿè¡Œï¼ˆGPUãƒ­ãƒƒã‚¯ãªã—ã§é«˜é€ŸåŒ–ï¼‰
        generation_config = GenerationConfig(
            max_tokens=30,  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¸›ã‚‰ã—ã¦é«˜é€ŸåŒ–
            temperature=0.5,  # æ¸©åº¦ã‚’ä¸‹ã’ã¦é«˜é€ŸåŒ–
            top_p=0.9,
            top_k=20,  # top_kã‚’æ¸›ã‚‰ã—ã¦é«˜é€ŸåŒ–
            repeat_penalty=1.1
        )
        
        # ãƒ€ãƒŸãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆãƒ­ãƒƒã‚¯ãªã—ï¼‰
        test_response = llm_engine.generate_response(
            user_input="ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚",
            generation_config=generation_config,
            use_history=False
        )
        
        warmup_time = time.time() - start_time
        logger.info(f"âœ… LLaMA APIã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†: {warmup_time:.2f}ç§’")
        
        return {
            "status": "success",
            "message": "LLaMAã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†",
            "warmup_time": warmup_time,
            "test_response": test_response[:100] + "..." if len(test_response) > 100 else test_response,
            "model_loaded": MODELS_LOADED
        }
        
    except Exception as e:
        logger.error(f"âŒ LLaMA APIã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=f"Warmup failed: {str(e)}")

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """åŸºæœ¬ãƒãƒ£ãƒƒãƒˆ"""
    check_engine()
    
    # è»½é‡GPUæ’ä»–åˆ¶å¾¡ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆçŸ­ç¸®ï¼‰
    try:
        await asyncio.wait_for(gpu_lock.acquire(), timeout=1.0)
        try:
            return await _execute_chat(request)
        finally:
            gpu_lock.release()
    except asyncio.TimeoutError:
        # GPUãƒ­ãƒƒã‚¯å–å¾—å¤±æ•—æ™‚ã¯ç›´æ¥å®Ÿè¡Œ
        logger.warning("âš ï¸ GPUãƒ­ãƒƒã‚¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - ç›´æ¥å®Ÿè¡Œ")
        return await _execute_chat(request)

async def _execute_chat(request: ChatRequest):
    """ãƒãƒ£ãƒƒãƒˆå®Ÿè¡Œ"""
    try:
        start_time = time.time()
        
        # ç”Ÿæˆè¨­å®š
        generation_config = GenerationConfig(
            max_tokens=request.generation_config.get("max_tokens", 512) if request.generation_config else 512,
            temperature=request.generation_config.get("temperature", 0.7) if request.generation_config else 0.7,
            top_p=request.generation_config.get("top_p", 0.9) if request.generation_config else 0.9,
            top_k=request.generation_config.get("top_k", 40) if request.generation_config else 40,
            repeat_penalty=request.generation_config.get("repeat_penalty", 1.1) if request.generation_config else 1.1
        )
        
        # å¿œç­”ç”Ÿæˆ
        response = llm_engine.generate_response(
            user_input=request.message,
            generation_config=generation_config,
            use_history=request.use_history
        )
        
        inference_time = time.time() - start_time
        
        return ChatResponse(
            response=response,
            inference_time=inference_time
        )
        
    except Exception as e:
        logger.error(f"âŒ ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/generate-chat", response_model=InteractiveResponse)
async def generate_chat(request: InteractiveRequest):
    """ãƒãƒ£ãƒƒãƒˆç”Ÿæˆï¼ˆGradioç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼‰"""
    check_engine()
    
    # è»½é‡GPUæ’ä»–åˆ¶å¾¡ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆçŸ­ç¸®ï¼‰
    try:
        await asyncio.wait_for(gpu_lock.acquire(), timeout=1.0)
        try:
            return await _execute_interactive_chat(request)
        finally:
            gpu_lock.release()
    except asyncio.TimeoutError:
        # GPUãƒ­ãƒƒã‚¯å–å¾—å¤±æ•—æ™‚ã¯ç›´æ¥å®Ÿè¡Œ
        logger.warning("âš ï¸ GPUãƒ­ãƒƒã‚¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - ç›´æ¥å®Ÿè¡Œ")
        return await _execute_interactive_chat(request)

@app.post("/interactive", response_model=InteractiveResponse)
async def interactive_chat(request: InteractiveRequest):
    """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ£ãƒƒãƒˆï¼ˆã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å¯¾å¿œï¼‰"""
    check_engine()
    
    # è»½é‡GPUæ’ä»–åˆ¶å¾¡ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆçŸ­ç¸®ï¼‰
    try:
        await asyncio.wait_for(gpu_lock.acquire(), timeout=1.0)
        try:
            return await _execute_interactive_chat(request)
        finally:
            gpu_lock.release()
    except asyncio.TimeoutError:
        # GPUãƒ­ãƒƒã‚¯å–å¾—å¤±æ•—æ™‚ã¯ç›´æ¥å®Ÿè¡Œ
        logger.warning("âš ï¸ GPUãƒ­ãƒƒã‚¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - ç›´æ¥å®Ÿè¡Œ")
        return await _execute_interactive_chat(request)

async def _execute_interactive_chat(request: InteractiveRequest):
    """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ£ãƒƒãƒˆå®Ÿè¡Œ"""
    try:
        start_time = time.time()
        
        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ—ãƒªã‚»ãƒƒãƒˆé©ç”¨
        preset_prompt = get_preset_prompt(request.character)
        original_prompt = llm_engine.config.get("chat", {}).get("system_prompt", "")
        
        # ä¸€æ™‚çš„ã«ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤‰æ›´
        llm_engine.set_system_prompt(preset_prompt)
        
        # ç”Ÿæˆè¨­å®š
        generation_config = GenerationConfig(
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=0.9,
            top_k=40,
            repeat_penalty=1.1
        )
        
        # å¿œç­”ç”Ÿæˆ
        response = llm_engine.generate_response(
            user_input=request.message,
            generation_config=generation_config,
            use_history=True
        )
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…ƒã«æˆ»ã™
        llm_engine.set_system_prompt(original_prompt)
        
        inference_time = time.time() - start_time
        tokens_per_second = len(response.split()) / inference_time if inference_time > 0 else 0
        
        return InteractiveResponse(
            response=response,
            character=request.character,
            inference_time=inference_time,
            tokens_per_second=tokens_per_second
        )
        
    except Exception as e:
        logger.error(f"âŒ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/presets")
async def get_presets():
    """åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒªã‚»ãƒƒãƒˆä¸€è¦§"""
    if 'presets' in PRELOADED_CACHE:
        return {"presets": list(PRELOADED_CACHE['presets'].keys())}
    return {"presets": ["tsundere", "friendly", "technical", "casual", "polite", "creative", "academic"]}

@app.delete("/history")
async def clear_history():
    """ä¼šè©±å±¥æ­´ã‚¯ãƒªã‚¢"""
    check_engine()
    
    llm_engine.clear_history()
    return {"message": "History cleared successfully"}

@app.get("/status")
async def get_status():
    """è©³ç´°ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
    check_engine()
    
    try:
        info = llm_engine.get_model_info()
        return {
            "server": {
                "uptime": str(datetime.now() - server_start_time),
                "models_loaded": MODELS_LOADED,
                "cache_size": len(PRELOADED_CACHE)
            },
            "model": info,
            "gpu": {
                "available": True,  # TODO: å®Ÿéš›ã®GPUçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
                "memory_usage": "N/A"
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# === ãƒ¡ã‚¤ãƒ³ ===

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Enhanced LlamaCPP FastAPI Server')
    parser.add_argument('--host', default='0.0.0.0', help='Host to bind to')
    parser.add_argument('--port', type=int, default=8001, help='Port to bind to')
    args = parser.parse_args()
    
    uvicorn.run(
        "fastapi_chat_server:app",
        host=args.host,
        port=args.port,
        reload=False,
        log_level="info"
    )

if __name__ == "__main__":
    main()