#!/usr/bin/env python3
"""
Gradio UI Server for LlamaCPP
æ—¢å­˜ã®LLMEngineã‚’ä½¿ç”¨ã—ãŸWebUI
"""

import os
import gradio as gr
import logging
from datetime import datetime
from typing import List, Tuple, Optional
import json

# æ—¢å­˜ã®LLMEngineã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from llm_engine import LLMEngine, ChatMessage, GenerationConfig

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class GradioLLMServer:
    """Gradio UIä»˜ãLLMã‚µãƒ¼ãƒãƒ¼"""
    
    def __init__(self, host: str = "0.0.0.0", port: int = 7860):
        self.host = host
        self.port = port
        self.llm_engine = None
        self.conversation_history = []
        
        # LLMã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        logger.info("Initializing LLM Engine...")
        self.llm_engine = LLMEngine()
        logger.info("LLM Engine initialized successfully!")
    
    def chat_response(self, 
                     message: str, 
                     history: List[dict], 
                     temperature: float = 0.7,
                     max_tokens: int = 512,
                     top_p: float = 0.9) -> Tuple[List[dict], str]:
        """ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’ç”Ÿæˆ"""
        if not message.strip():
            return history, ""
        
        try:
            # ç”Ÿæˆè¨­å®š
            config = GenerationConfig(
                max_tokens=int(max_tokens),
                temperature=float(temperature),
                top_p=float(top_p)
            )
            
            # LLMã‚¨ãƒ³ã‚¸ãƒ³ã§å¿œç­”ç”Ÿæˆ
            start_time = datetime.now()
            response = self.llm_engine.generate_response(message, config)
            inference_time = (datetime.now() - start_time).total_seconds()
            
            # å±¥æ­´ã«è¿½åŠ ï¼ˆOpenAIå½¢å¼ï¼‰
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": response})
            
            logger.info(f"Generated response in {inference_time:.2f}s")
            return history, ""
            
        except Exception as e:
            error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            logger.error(f"Chat error: {e}")
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": error_msg})
            return history, ""
    
    def clear_history(self) -> Tuple[List, str]:
        """ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"""
        if self.llm_engine:
            self.llm_engine.clear_conversation()
        logger.info("Conversation history cleared")
        return [], ""
    
    def change_preset(self, preset_name: str) -> str:
        """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ—ãƒªã‚»ãƒƒãƒˆå¤‰æ›´"""
        try:
            if self.llm_engine:
                self.llm_engine.set_character_preset(preset_name)
                return f"ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’ '{preset_name}' ã«å¤‰æ›´ã—ã¾ã—ãŸ"
        except Exception as e:
            return f"ãƒ—ãƒªã‚»ãƒƒãƒˆå¤‰æ›´ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def get_system_info(self) -> str:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—"""
        try:
            if self.llm_engine:
                info = {
                    "model_loaded": self.llm_engine.model is not None,
                    "gpu_available": self.llm_engine.gpu_available,
                    "model_path": self.llm_engine.config.get("model", {}).get("path", "Unknown"),
                    "conversation_count": len(self.llm_engine.conversation_history)
                }
                return json.dumps(info, indent=2, ensure_ascii=False)
            return "LLM Engine not initialized"
        except Exception as e:
            return f"System info error: {str(e)}"
    
    def create_interface(self) -> gr.Blocks:
        """Gradio UIã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ä½œæˆ"""
        
        with gr.Blocks(
            title="ğŸ¦™ LlamaCPP Gradio Chat",
            theme=gr.themes.Soft(),
            css="""
            .gradio-container {
                max-width: 1200px;
                margin: auto;
            }
            .chat-container {
                height: 500px;
            }
            """
        ) as interface:
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            gr.HTML("""
            <div style="text-align: center; padding: 20px;">
                <h1>ğŸ¦™ LlamaCPP Gradio Chat</h1>
                <p>GPUåŠ é€Ÿãƒ­ãƒ¼ã‚«ãƒ«LLM with ãƒ„ãƒ³ãƒ‡ãƒ¬ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼</p>
            </div>
            """)
            
            with gr.Row():
                with gr.Column(scale=3):
                    # ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆ
                    chatbot = gr.Chatbot(
                        label="ãƒãƒ£ãƒƒãƒˆ",
                        height=500,
                        show_copy_button=True,
                        type="messages"
                    )
                    
                    with gr.Row():
                        msg_input = gr.Textbox(
                            label="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                            placeholder="ãƒ„ãƒ³ãƒ‡ãƒ¬ã¡ã‚ƒã‚“ã¨è©±ã—ã¦ã¿ã¦...",
                            scale=4,
                            submit_btn=True
                        )
                        send_btn = gr.Button("é€ä¿¡", variant="primary", scale=1)
                    
                    # ãƒœã‚¿ãƒ³ç¾¤
                    with gr.Row():
                        clear_btn = gr.Button("å±¥æ­´ã‚¯ãƒªã‚¢", variant="secondary")
                        
                with gr.Column(scale=1):
                    # è¨­å®šãƒ‘ãƒãƒ«
                    gr.HTML("<h3>âš™ï¸ ç”Ÿæˆè¨­å®š</h3>")
                    
                    temperature = gr.Slider(
                        label="Temperature",
                        minimum=0.1,
                        maximum=1.0,
                        value=0.7,
                        step=0.1,
                        info="å‰µé€ æ€§ (ä½=ä¿å®ˆçš„, é«˜=å‰µé€ çš„)"
                    )
                    
                    max_tokens = gr.Slider(
                        label="Max Tokens",
                        minimum=50,
                        maximum=1000,
                        value=512,
                        step=50,
                        info="æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°"
                    )
                    
                    top_p = gr.Slider(
                        label="Top-p",
                        minimum=0.1,
                        maximum=1.0,
                        value=0.9,
                        step=0.1,
                        info="Nucleus sampling"
                    )
                    
                    # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ—ãƒªã‚»ãƒƒãƒˆ
                    gr.HTML("<h3>ğŸ‘¤ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼</h3>")
                    preset_dropdown = gr.Dropdown(
                        choices=["tsundere", "friendly", "technical"],
                        value="tsundere",
                        label="ãƒ—ãƒªã‚»ãƒƒãƒˆ",
                        info="ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®š"
                    )
                    
                    preset_btn = gr.Button("ãƒ—ãƒªã‚»ãƒƒãƒˆé©ç”¨", variant="secondary")
                    preset_status = gr.Textbox(
                        label="ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹",
                        value="Ready",
                        interactive=False
                    )
                    
                    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
                    gr.HTML("<h3>ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±</h3>")
                    info_btn = gr.Button("æƒ…å ±æ›´æ–°")
                    system_info = gr.Textbox(
                        label="ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹",
                        lines=8,
                        interactive=False,
                        value="Click 'æƒ…å ±æ›´æ–°' to load system info"
                    )
            
            # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼
            def submit_message(msg, history, temp, tokens, tp):
                return self.chat_response(msg, history, temp, tokens, tp)
            
            # é€ä¿¡ã‚¤ãƒ™ãƒ³ãƒˆ
            msg_input.submit(
                fn=submit_message,
                inputs=[msg_input, chatbot, temperature, max_tokens, top_p],
                outputs=[chatbot, msg_input]
            )
            
            send_btn.click(
                fn=submit_message,
                inputs=[msg_input, chatbot, temperature, max_tokens, top_p],
                outputs=[chatbot, msg_input]
            )
            
            # ã‚¯ãƒªã‚¢ã‚¤ãƒ™ãƒ³ãƒˆ
            clear_btn.click(
                fn=self.clear_history,
                outputs=[chatbot, msg_input]
            )
            
            # ãƒ—ãƒªã‚»ãƒƒãƒˆå¤‰æ›´
            preset_btn.click(
                fn=self.change_preset,
                inputs=[preset_dropdown],
                outputs=[preset_status]
            )
            
            # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±æ›´æ–°
            info_btn.click(
                fn=self.get_system_info,
                outputs=[system_info]
            )
        
        return interface
    
    def launch(self, share: bool = False, debug: bool = False):
        """Gradioã‚µãƒ¼ãƒãƒ¼èµ·å‹•"""
        interface = self.create_interface()
        
        logger.info(f"Starting Gradio server on {self.host}:{self.port}")
        
        interface.launch(
            server_name=self.host,
            server_port=self.port,
            share=share,
            debug=debug,
            show_error=True,
            quiet=False
        )


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    try:
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šå–å¾—
        host = os.getenv("GRADIO_HOST", "0.0.0.0")
        port = int(os.getenv("GRADIO_PORT", "7860"))
        share = os.getenv("GRADIO_SHARE", "false").lower() == "true"
        debug = os.getenv("GRADIO_DEBUG", "false").lower() == "true"
        
        # Gradioã‚µãƒ¼ãƒãƒ¼èµ·å‹•
        server = GradioLLMServer(host=host, port=port)
        server.launch(share=share, debug=debug)
        
    except KeyboardInterrupt:
        logger.info("Server stopped by user")
    except Exception as e:
        logger.error(f"Server error: {e}")
        raise


if __name__ == "__main__":
    main()