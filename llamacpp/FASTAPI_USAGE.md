# LlamaCPP FastAPI ä½¿ç”¨ã‚¬ã‚¤ãƒ‰

## æ¦‚è¦
äº‹å‰ãƒ­ãƒ¼ãƒ‰æœ€é©åŒ–ã«ã‚ˆã‚‹é«˜é€ŸLLMãƒãƒ£ãƒƒãƒˆã‚µãƒ¼ãƒãƒ¼ã§ã™ã€‚7ã¤ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ—ãƒªã‚»ãƒƒãƒˆã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªä¼šè©±ãŒå¯èƒ½ã§ã™ã€‚

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. FastAPIã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•
```bash
cd /home/adama/gpts/llamacpp

# æ”¹è‰¯ç‰ˆFastAPIã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ï¼ˆåˆå›ã¯7-10ç§’ç¨‹åº¦ã‹ã‹ã‚Šã¾ã™ï¼‰
docker run --gpus all --rm -it \
  --privileged \
  -v $(pwd)/models:/app/models \
  -v $(pwd)/config:/app/config \
  -v $(pwd)/logs:/app/logs \
  -v $(pwd)/src:/app/src \
  -v /usr/lib/wsl:/usr/lib/wsl \
  -e LD_LIBRARY_PATH=/usr/lib/wsl/lib \
  -e NVIDIA_VISIBLE_DEVICES=all \
  -e CUDA_VISIBLE_DEVICES=0 \
  -p 8001:8001 \
  llama-cpp-python:cuda python /app/src/fastapi_chat_server.py
```

### 2. ã‚µãƒ¼ãƒãƒ¼çŠ¶æ…‹ç¢ºèª
```bash
# åŸºæœ¬ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
curl http://localhost:8001/

# è©³ç´°ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
curl http://localhost:8001/health | python3 -m json.tool

# åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒªã‚»ãƒƒãƒˆ
curl http://localhost:8001/presets
```

### 3. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ£ãƒƒãƒˆé–‹å§‹

#### æ–¹æ³•1: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–CLIä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰
```bash
# åŸºæœ¬çš„ãªä½¿ç”¨ï¼ˆãƒ„ãƒ³ãƒ‡ãƒ¬ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ï¼‰
python3 scripts/interactive_chat_cli.py

# ç•°ãªã‚‹ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã§é–‹å§‹
python3 scripts/interactive_chat_cli.py --character friendly

# è¨­å®šã‚’æŒ‡å®šã—ã¦é–‹å§‹
python3 scripts/interactive_chat_cli.py --character technical --temperature 0.5 --max-tokens 256
```

#### æ–¹æ³•2: ä¸€æ‹¬ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
```bash
# å…¨æ©Ÿèƒ½ã®è‡ªå‹•ãƒ†ã‚¹ãƒˆ
./run_fastapi_tests.sh
```

#### æ–¹æ³•3: APIç›´æ¥å‘¼ã³å‡ºã—
```bash
# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ£ãƒƒãƒˆ
curl -X POST http://localhost:8001/interactive \
  -H "Content-Type: application/json" \
  -d '{
    "message": "ã“ã‚“ã«ã¡ã¯ï¼",
    "character": "tsundere",
    "temperature": 0.7,
    "max_tokens": 512
  }'

# åŸºæœ¬ãƒãƒ£ãƒƒãƒˆ
curl -X POST http://localhost:8001/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "ãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
    "use_history": true,
    "stream": false,
    "generation_config": {
      "max_tokens": 256,
      "temperature": 0.7
    }
  }'
```

## ğŸ­ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ—ãƒªã‚»ãƒƒãƒˆ

### åˆ©ç”¨å¯èƒ½ãªã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼
- **tsundere** ğŸ­ - ãƒ„ãƒ³ãƒ‡ãƒ¬å¥³ã®å­ï¼ˆã€Œã¹ã€åˆ¥ã«ã€œã€ã€Œã€œãªã‚“ã ã‹ã‚‰ã­ï¼ã€ï¼‰
- **friendly** ğŸ˜Š - ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ï¼ˆæ˜ã‚‹ãè¦ªã—ã¿ã‚„ã™ã„ï¼‰
- **technical** ğŸ”§ - æŠ€è¡“çš„ï¼ˆãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ»æŠ€è¡“ç‰¹åŒ–ï¼‰
- **casual** ğŸ˜ - ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ï¼ˆå‹é”æ„Ÿè¦šã®ã‚¿ãƒ¡å£ï¼‰
- **polite** ğŸ™ - ä¸å¯§ï¼ˆéå¸¸ã«ç¤¼å„€æ­£ã—ã„æ•¬èªï¼‰
- **creative** ğŸ¨ - ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ï¼ˆè©©çš„ã§å‰µé€ çš„ï¼‰
- **academic** ğŸ“š - å­¦è¡“çš„ï¼ˆè«–ç†çš„ã§å°‚é–€çš„ï¼‰

### ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å¤‰æ›´
```bash
# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–CLIå†…ã§
/character friendly
/character technical
/character casual
```

## ğŸ› ï¸ APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

### GET /health
è©³ç´°ãªãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯

**ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹:**
```json
{
  "status": "healthy",
  "model_loaded": true,
  "gpu_available": true,
  "uptime": "0:05:32.123456",
  "preload_status": {
    "basic_deps": true,
    "llm_deps": true,
    "config": true,
    "presets": true,
    "cache_size": 7
  }
}
```

### POST /interactive
ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ£ãƒƒãƒˆï¼ˆã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å¯¾å¿œï¼‰

**ãƒªã‚¯ã‚¨ã‚¹ãƒˆ:**
```json
{
  "message": "ã“ã‚“ã«ã¡ã¯ï¼",
  "character": "tsundere",
  "temperature": 0.7,
  "max_tokens": 512
}
```

**ãƒ¬ã‚¹ãƒãƒ³ã‚¹:**
```json
{
  "response": "ãµã‚“ï¼ã“ã€ã“ã‚“ã«ã¡ã¯...ã¹ã€åˆ¥ã«æŒ¨æ‹¶ã—ãŸã‹ã£ãŸã‚ã‘ã˜ã‚ƒãªã„ã‚“ã ã‹ã‚‰ã­ï¼",
  "character": "tsundere",
  "inference_time": 1.23,
  "tokens_per_second": 25.4
}
```

### POST /chat
åŸºæœ¬ãƒãƒ£ãƒƒãƒˆ

**ãƒªã‚¯ã‚¨ã‚¹ãƒˆ:**
```json
{
  "message": "ãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
  "use_history": true,
  "stream": false,
  "generation_config": {
    "max_tokens": 256,
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 40,
    "repeat_penalty": 1.1
  }
}
```

### GET /presets
åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒªã‚»ãƒƒãƒˆä¸€è¦§

### DELETE /history
ä¼šè©±å±¥æ­´ã‚¯ãƒªã‚¢

### GET /status
è©³ç´°ãªã‚µãƒ¼ãƒãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹

## ğŸ® ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–CLIä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ã‚³ãƒãƒ³ãƒ‰
```bash
/help              # ãƒ˜ãƒ«ãƒ—è¡¨ç¤º
/character <name>  # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å¤‰æ›´
/temp <value>      # æ¸©åº¦å¤‰æ›´ (0.1-2.0)
/tokens <value>    # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°å¤‰æ›´
/presets           # ãƒ—ãƒªã‚»ãƒƒãƒˆä¸€è¦§
/status            # ã‚µãƒ¼ãƒãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
/clear             # å±¥æ­´ã‚¯ãƒªã‚¢
/stats             # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆ
/quit              # çµ‚äº†
```

### ä½¿ç”¨ä¾‹
```bash
# ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å¤‰æ›´
/character friendly

# æ¸©åº¦èª¿æ•´ï¼ˆå‰µé€ æ€§ã‚’é«˜ã‚ã‚‹ï¼‰
/temp 0.9

# ãƒˆãƒ¼ã‚¯ãƒ³æ•°èª¿æ•´
/tokens 256

# å±¥æ­´ã‚¯ãƒªã‚¢
/clear
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰¹æ€§

### äº‹å‰ãƒ­ãƒ¼ãƒ‰æœ€é©åŒ–
- **åˆæœŸåŒ–æ™‚é–“**: 7-10ç§’ï¼ˆä¾å­˜é–¢ä¿‚äº‹å‰ãƒ­ãƒ¼ãƒ‰è¾¼ã¿ï¼‰
- **å¿œç­”é€Ÿåº¦**: 1-2ç§’/ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
- **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: 30-60ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/åˆ†
- **GPUä½¿ç”¨é‡**: 3.86GB VRAMï¼ˆRTX 3050ï¼‰

### æœ€é©åŒ–æ©Ÿèƒ½
- **åŸºæœ¬ä¾å­˜é–¢ä¿‚**: JSONã€YAMLã€datetimeäº‹å‰ãƒ­ãƒ¼ãƒ‰
- **LLMä¾å­˜é–¢ä¿‚**: llama-cpp-pythonã€è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«äº‹å‰ãƒ­ãƒ¼ãƒ‰
- **ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ—ãƒªã‚»ãƒƒãƒˆ**: 7ç¨®é¡ã®ãƒ—ãƒªã‚»ãƒƒãƒˆäº‹å‰ãƒ­ãƒ¼ãƒ‰
- **Warm-upæ¨è«–**: åˆæœŸåŒ–æ™‚ã«ãƒ†ã‚¹ãƒˆæ¨è«–å®Ÿè¡Œ
- **GPUæœ€é©åŒ–**: TensorCoreã€CUDAæœ€é©åŒ–

## ğŸ”§ ã‚µãƒ¼ãƒãƒ¼ç®¡ç†

### ãƒ­ã‚°ç¢ºèª
```bash
# Dockerã‚³ãƒ³ãƒ†ãƒŠã®ãƒ­ã‚°
docker logs <container_id>

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°
ls -la logs/llm_engine_*.log
```

### ã‚µãƒ¼ãƒãƒ¼åœæ­¢
```bash
# Ctrl+C ã§ã‚µãƒ¼ãƒãƒ¼åœæ­¢
# ã¾ãŸã¯
docker stop <container_id>
```

### è¨­å®šå¤‰æ›´
```bash
# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç·¨é›†
vim config/model_config.yaml

# ã‚µãƒ¼ãƒãƒ¼å†èµ·å‹•ãŒå¿…è¦
```

## ğŸš¨ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ããªã„
```bash
# ãƒãƒ¼ãƒˆä½¿ç”¨çŠ¶æ³ç¢ºèª
sudo lsof -i :8001

# ã‚³ãƒ³ãƒ†ãƒŠçŠ¶æ…‹ç¢ºèª
docker ps -a
```

### GPUèªè­˜ã‚¨ãƒ©ãƒ¼
```bash
# GPUçŠ¶æ…‹ç¢ºèª
nvidia-smi
docker run --gpus all --rm nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

### å¿œç­”ãŒé…ã„
- åˆå›èµ·å‹•æ™‚ã¯7-10ç§’ã‹ã‹ã‚Šã¾ã™
- äº‹å‰ãƒ­ãƒ¼ãƒ‰å®Œäº†å¾Œã¯1-2ç§’ã§å¿œç­”
- `/status`ã§äº‹å‰ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹ã‚’ç¢ºèª

### ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãŒåæ˜ ã•ã‚Œãªã„
- `/presets`ã§åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒªã‚»ãƒƒãƒˆã‚’ç¢ºèª
- æ­£ç¢ºãªã‚¹ãƒšãƒ«ã§ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã‚’æŒ‡å®š
- `/character <name>`ã§å¤‰æ›´

## ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

| æ©Ÿèƒ½ | æ—§Gradio | æ–°FastAPI | æ”¹å–„ç‡ |
|------|---------|-----------|--------|
| åˆæœŸåŒ–æ™‚é–“ | 21ç§’ | **7-10ç§’** | **50%å‘ä¸Š** |
| å¿œç­”é€Ÿåº¦ | 2.14-2.80ç§’ | **1-2ç§’** | **30%å‘ä¸Š** |
| ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ•° | 7å€‹ | **7å€‹** | **åŒç­‰** |
| APIæ©Ÿèƒ½ | é™å®šçš„ | **å……å®Ÿ** | **å¤§å¹…å‘ä¸Š** |
| äº‹å‰ãƒ­ãƒ¼ãƒ‰ | åŸºæœ¬çš„ | **é«˜åº¦** | **å¤§å¹…å‘ä¸Š** |

## ğŸ¯ æ¨å¥¨ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³

### æ—¥å¸¸ä¼šè©±
```bash
python3 scripts/interactive_chat_cli.py --character friendly
```

### æŠ€è¡“è³ªå•
```bash
python3 scripts/interactive_chat_cli.py --character technical --temperature 0.5
```

### å‰µé€ çš„ä½œæ¥­
```bash
python3 scripts/interactive_chat_cli.py --character creative --temperature 0.9
```

### å­¦è¡“çš„è­°è«–
```bash
python3 scripts/interactive_chat_cli.py --character academic --temperature 0.6
```

---

**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 2.0.0 (FastAPI Enhanced)  
**ä½œæˆæ—¥**: 2025-07-17  
**å‹•ä½œç¢ºèªç’°å¢ƒ**: WSL2 Ubuntu 22.04 + RTX 3050 + CUDA 12.1  
**ç‰¹è¨˜äº‹é …**: äº‹å‰ãƒ­ãƒ¼ãƒ‰æœ€é©åŒ–ã€7ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å¯¾å¿œã€GPUåŠ é€Ÿã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–CLI